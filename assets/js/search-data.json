{
  
    
        "post0": {
            "title": "Residual Learning",
            "content": "What is a residual? . Residual is the difference between actual and estimated value. . What is residual learning? . In the context of ensemble learning, a base model is used to fit the residuals to make the ensemble model more accurate. In deep learning, various architectures use a block/layer to fit the residual to improve the performance of the DNN. . How does Gradient Boosting Machines use residuals? . We will try to deconstruct how GBM works using DecisionTrees on a regression task. . from sklearn import datasets from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt . X, y = datasets.make_regression(n_samples=1000, random_state=41) Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.2, random_state=41) . from sklearn.tree import DecisionTreeRegressor . tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg1.fit(Xtr, ytr) y2 = ytr - tree_reg1.predict(Xtr) tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg2.fit(Xtr, y2) y3 = y2 - tree_reg2.predict(Xtr) tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg3.fit(Xtr, y3) y_pred = sum(tree.predict(Xva) for tree in (tree_reg1, tree_reg2, tree_reg3)) . Gradient Boosting . How does residuals play a part in Gradient Boosting Learning? . Train a base learner tree_reg1 to fit data (X) and labels (y) | Train a base learner tree_reg2 that fits on data (X) and residuals between the label and predicted value of base learner tree_reg1. Essentially, we are using a base learner to learn the residuals. | Finally the result of all the base learners are added to make the final prediction. | . The above code is equivalent to calling the GradientBoostingRegressor with 3 base learners. . from sklearn.ensemble import GradientBoostingRegressor gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=41) gbrt.fit(Xtr, ytr) gb_preds = gbrt.predict(Xva) . sum(y_pred - gb_preds) . -4.554578936222242e-12 . Role of residual learning in training deep networks? . Why do we need ResNets? . Research to develop better architectures which perform better has led researchers to go deeper with a notion that to a certain extent going deeper would yield better performance. . But we realized that going deeper brings problems of its own, model become difficult to train. In 2014, VGG had only 19 layers while in 2015 ResNet had 152 layers and a far better performance, one can say at an initial glance that ResNet wins because it has more number of layers. Ofcourse that is the case but it also introduces a trick called &quot;residual learning&quot; that helps achieve this performance. . CNN models have evolved over time from LeNet-5 ( 5 layers ) and AlexNet ( 8 layers ) to VGGNet (16-19) and later GoogleNet ( 22 layers ). According to experimental results of VGGNet, depth of the network plays a crucial role in model&#39;s performance. . Please find below tables extracted from VGG paper that showcases that deeper we go better the effect. . . . Are deeper networks really better? . Later in various experiments it was found out that model performance increases with depth upto a certain extent further which it often decreases. What could be the reasons for that could it be following . Overfitting. | Vanishing/Exploding Gradients. | . Overfitting . In the Resnet paper the authors tried this following experiment . . The y-axis on the left figure represents training error and the y-axis on the right figure represents test error and x-axes on the both the figures represent the number of iterations. . We can see that the 20-layer network trained for a large number of iterations yields in low training error but corresponding test error is relatively large. This is a case of over-fitting ( we are performing better on training compared to test dataset ). . In addition to this the authors also trained a network with 56-layers and found out that error of this network in both training and testing is large compared to the 20-layer network. Thus performance degradation has nothing to do with overfitting. . Vanishing/Exploding Gradients . Vanishing/Exploding gradients make the model difficult to train but there are already some techniques like Batch Normalization to alleviate this problem. . Let&#39;s try to understand the example presented by author in the paper . Suppose we have a following network which can perform good on training and test datasets. . . Then we augment the architecture in the following way to add more layers. The parameters of the first 4 layers are copied from the above network and these parameters remain unchanged during training. . . In theory the performance of the second network should be better than first network since we have more layers which could extract useful features and suppose we find out that the second network performs worse, then one explanation provided by the authors is that since we have copied the parameters of first 4 layers in the second network and if they are enough to meet the performance requirements then the newly added layers are a bit redundant. To maintain the level of performance, the newly added functions has to serve as an identity mapping that is the net effect of the purple layers should be f(x) = x, in this way we would not experience model degradation. . This is what the authors observed that the non-linear expression of the traditional multi-layer network structure has difficulty expressing the identity mapping which leads to model degradation. . How do we tackle model degradation then? . Assuming that a relatively shallow network can already achieve good results, then even if the network is piled up with more layers the effect of the model should not deteriorate. . In reality, however this is the problem, doing nothing happens to be a very challenging task. . Presence of non-linear activation functions makes the input-to-output process almost irreversible. Non-linearity gives the model endless possibilities but it also makes the network forget the original intention. . The quality of not forgetting the original intention/doing nothing is managed by identity mapping. . Residual Block . . In fact, it is difficult for existing neural networks to fit the underlying identity mapping function H(x) = x.But if the network is designed such that H(x) = F(x) + x, then the identity map could be used as part of the network. . The problem can be transformed into learning a residual function F(x) = H(x) - x. As long as F(x)=0, an identity map H(x) = x is formed. The loop in the figure is called a shortcut connection. By jumping before the activation function, the output of the previous layer or layers is added to the output calculated by this layer, and the result of summation is input to the next activation function as the output of this layer. . The idea of the skip connection is to expressed the output as a linear superposition of a nonlinear transformation of the input and the input. There is no new formula, no new theory, but a new expression. . Why is residual learning relatively easier? . Intuitively, residual learning requires less learning, because residuals are generally relatively small and the learning difficulty is less. However, we can analyze this problem from a mathematical point of view. First, the residual unit can be expressed as: . . $F(x, {W_i })$ is the goal of our learning, that is, the residual of the output and input i.e. $y-x$. If we further expand . . $ sigma$ refers to Relu, while $W_1$, $W_2$ refer to two layers of weights. When $F(x, {W_i })$ learns to have a 0 value then $y = x$, this is what we call identity mapping. . Why can&#39;t we have $y=f(x, {W_i })$ instead and no skip connections? . Because $f(x, {W_i })$ has a ReLU activation function in the middle so if x &lt;= 0 then y = 0 which would violate the identity mapping principle. | . Experiments . . Taken directly from Resnet paper . Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left:plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. . Summary . Shortcut connections/residual connections/skip connections/skip connections, etc. are all one thing, there is no new theory, just a new expression. | Problem of model degradation when deepening the network could be somewhat alleviated using residual learning. | . References . Resnet paper | VGG | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/2022/09/19/residual-learning.html",
            "relUrl": "/deeplearning/math/2022/09/19/residual-learning.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "AEDA ( An Easier Data Augmentation Technique for Text Classification )",
            "content": "Paper resources . paper | code | . Objective . This paper proposes a new data augmentation technique for text classification task. | It also compares the performance of this augmentation technique with EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks and concludes that their method is simpler and produces better results. | In this experiment we will try to implement this data augmentation using fastai on a text classification task. | . Why do we need augmentations? . To have better generalizability, we need more and more comprehensive datasets but collection of these datasets and labelling is a laborious task so augmentation becomes an attractive method to introduce more examples for model to consume. | . What are the different kind of augmentations used in NLP? . For improving machine translation task, researchers have tried substituting common words with rare words thus providing more context for rare words. | Some researchers have tried replacing words with their synonyms for tweet classification. | Randomly swap two words in a sentence. | Randomly delete a word in the sentence and many more. | . What is the novel idea presented in the paper? . AEDA method proposes randomly inserting some punctuation marks as an augmentation to introduce noise. The authors report improvement performance in text classification tasks. . Can you share an example of how this augmentation would work? . Original Text: . Appropriate given recent events . . Augmented Text: . Appropriate given ; recent events . Appropriate ? given : recent events . Appropriate given , recent events . How many punctuation marks are inserted? . Between 1 to n/3 where n represents the length of the sentence. | . Why one-third of the sentence length? . The authors mention that they want to increase the complexity of the sentence but doesn&#39;t want to add too many punctuation marks which would interfere with the semantic meaning of the sentence. . At which positions should we insert these punctuation marks? . The authors inserted them at random positions in the sentence. | . What are the different punctuation marks used? . . ; ? : ! , . Why does AEDA work better compared to EDA? . EDA proposes synonym replacement, random replacement, random insertion and random deletion. These modifications could change the semantic meaning of the text. | Whereas AEDA just introduces punctuation marks which would only introduce noise and would not mess the semantic meaning or the word ordering. | . Implementation . We would be using fastai to implement this augmentation. | The authors have released the code as well which we would using. | . Dataset . We will be using this dataset used in this challenge where the goal is to predict the subreddit of a subreddit post based on their title and their description. This is an example of text categorization / text classification task | . Load libraries . import pandas as pd import numpy as np from pathlib import Path from tqdm import tqdm from fastai.text.all import * SEED = 41 . Define paths and constants . BASE_DIR = Path(&#39;~/data/dl_nlp&#39;) RAW_DATA_PATH = BASE_DIR / &#39;data&#39; OUTPUT_DIR = Path(&#39;~/data/dl_nlp/outputs&#39;) PUNCTUATIONS = [&#39;.&#39;, &#39;,&#39;, &#39;!&#39;, &#39;?&#39;, &#39;;&#39;, &#39;:&#39;] PUNC_RATIO = 0.3 . Load dataset . train = pd.read_csv(RAW_DATA_PATH / &#39;train.csv&#39;) train.head() . . Text column represents title as well as description. | Subreddit column represents our label. | . Class Distribution . train.subreddit.value_counts(normalize=True) . . We have multiple categories that our model needs to get right. | Most of the categories have similar percentage of data points in the dataset, with only SubredditSimulator category having less training examples. | . Splitter . splits = RandomSplitter(seed=41)(train) . Create a splitting strategy. | Here we plan to split our training dataframe randomly into training ( 80% ) and validation ( 20% ) datasets. | . Tokenize the training dataset . df_tok, cnt = tokenize_df(train.iloc[splits[0]], text_cols=&#39;text&#39;) . Fast.ai provides a method to tokenize our dataset. | Here we only passing our training examples as the corpus for tokenizer to create vocabulary. | We could pass in different types of tokenizers here but by default it works with WordTokenizer. | . df_tok . . Here we could see that it has split our text string into tokens and created an additional column called text_length describing the length. | It has also added some library specific tokens like xxbos, xxmaj etc. xxbos represents beginning of the sentence token. For more details please refer to fast.ai | . cnt . . Here is a snapshot of the vocabulary constructed by the tokenize_df method. | . Using fast.ai Pipeline to construct Dataset . text_pipe = Pipeline([attrgetter(&#39;text&#39;), Tokenizer.from_df(0), Numericalize(vocab=list(cnt.keys()))]) lbl_pipe = Pipeline([attrgetter(&#39;subreddit&#39;), Categorize()]) lbl_pipe.setup(train.subreddit) dsets = Datasets(train, [text_pipe, lbl_pipe], splits=splits, dl_type=SortedDL) . Here we use Pipeline provided by fast.ai to put together different transforms we want to run on our dataframe. | text_pipe represents the Pipeline that we would like to run on our text column in the dataframe. | lbl_pipe represents the Pipeline that we would like to run on our subreddit column in the dataframe. | Numericalize transform takes in our vocabulary and converts the tokens to ids. | Categorize transforms converts our labels to categories. | Tokenizer.from_df transform tokenizes the text stored in our dataframe. | . AEDA data augmentation as fast.ai transform . np.random.seed(0) PUNCTUATIONS = [&#39;.&#39;, &#39;,&#39;, &#39;!&#39;, &#39;?&#39;, &#39;;&#39;, &#39;:&#39;] PUNC_RATIO = 0.3 class InsertPunctuation(Transform): split_idx = 0 def __init__(self, o2i, punc_ratio=PUNC_RATIO): self.o2i = o2i self.punc_ratio = punc_ratio def encodes(self, words:TensorText): new_line = [] q = random.randint(1, int(self.punc_ratio * len(words) + 1)) qs = random.sample(range(0, len(words)), q) for j, word in enumerate(words): if j in qs: new_line.append(self.o2i[PUNCTUATIONS[random.randint(0, len(PUNCTUATIONS)-1)]]) new_line.append(int(word)) else: new_line.append(int(word)) return TensorText(new_line) . We have taken the implementation from the github shared by the authors and created a fast.ai tranform that would take in the PUNC_RATIO and o2i as parameters and inserts punctuations at random positions in the sentence. | PUNC_RATIO by default takes a value of 0.3 which represents the 1/3rd of the sentence length mentioned in the paper. | o2i is mapping between token to token_id. | . Construct dataloaders . seq_len = 72 dls_kwargs = { &#39;after_item&#39; : InsertPunctuation(dsets.o2i), &#39;before_batch&#39;: Pad_Chunk(seq_len=seq_len) } dls = dsets.dataloaders(bs=32, seq_len=seq_len, **dls_kwargs) . When creating fast.ai dataloders we could perform operations on some of the events emitted. | Here we have made use of two such events, after_item callback is used to run our augmentation and add punctuation marks. | before_batch callback is used to make sure that we have paddded the tokens to make sure they are of same size before we collate them to form a batch. | . dls.show_batch(max_n=3) . . dls.show_batch gives a glimpse of the batch | . Using the classic TextCNN model introduced by Yoon Kim, paper . class TextCNN(Module): def __init__(self, n_embed, embed_dim, num_filters, filter_sizes, num_classes, dropout=0.5, pad_idx=1): store_attr(&#39;n_embed,embed_dim&#39;) self.embed = nn.Embedding(num_embeddings=n_embed, embedding_dim=embed_dim, padding_idx=1 ) self.convs = nn.ModuleList([ nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embed_dim) ) for k in filter_sizes ]) self.dropout = nn.Dropout(dropout) self.relu = nn.ReLU() self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes) def _conv_and_pool(self, x, conv): x = self.relu(conv(x)).squeeze(3) x = F.max_pool1d(x, x.size(2)).squeeze(2) return x def forward(self, x): out = self.embed(x) out = out.unsqueeze(1) out = torch.cat([self._conv_and_pool(out, conv) for conv in self.convs], 1) out = self.dropout(out) out = self.fc(out) return out . vocab = dls.train_ds.vocab num_classes = get_c(dls) model = TextCNN(len(vocab[0]), embed_dim=300, num_filters=100, filter_sizes=[1, 2, 3], num_classes=num_classes, ) . Define learner . learn = Learner(dls, model, metrics=[accuracy, F1Score(average=&#39;weighted&#39;)]) . Using F1 score weighted metric for multi-class classification. | . learn.fit_one_cycle(n_epoch=25, lr_max=3e-4, cbs=EarlyStoppingCallback(patience=3)) . . We are getting a F1( weighted ) score of 0.869 without using any pre-trained embeddings. | . References . AEDA: An Easier Data Augmentation Technique for Text Classification | AEDA code | EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks | Fast.AI | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/nlp/2022/01/31/aeda-text-augmentation.html",
            "relUrl": "/deeplearning/math/fastai/nlp/2022/01/31/aeda-text-augmentation.html",
            "date": " • Jan 31, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Temporal Convolution Networks",
            "content": "Objective . sequence modelling had become synonymous with recurrent networks. | This paper shows that convolutional networks can outperform recurrent networks on some of the tasks. | paper concludes that common association between sequence modelling and recurrent networks should be reconsidered. | . Temporal Convolution Networks? . A new general architecture for convolutional sequence prediction. | This new general architecture is referred to as Temporal Convolutional Networks abbreviated as TCN. | Convolutions in this architecture are causal which means that there is no information leakage. | Architecture can take in a sequence of arbitrary length and map it to an output sequence of the same length, just like RNNs. ( But tcn achieves this function not through seq2seq but simply using convolutional layers. ) | Also this paper highlights how we could combine deep networks ( with residual structures ) and dilated convolutions could be used to build long term dependencies. ( ability of an model to look back in past to make future predictions ) | . What is a sequence modelling task? . Taken directly from paper . Before defining the network structure, we highlight the nature of the sequence modeling task. Suppose that we are given an input sequence $x_0$, . . . , $x_T$ , and wish to predict some corresponding outputs $y_0$, . . . , $y_T$ at each time. The key constraint is that to predict the output $y_t$ for some time t, we are constrained to only use those inputs that have been previously observed:$x_0$, . . . , $x_t$. Formally, a sequence modeling network is any function f : $X_{T +1}$ → $Y_{T +1}$ that produces the mapping . $y_0$, . . . , $y_T$ = f($x_0$, . . . , $x_T$ ) (1) . if it satisfies the causal constraint that $y_t$ depends only on $x_0$, . . . , $x_t$ and not on any “future” inputs $x_{t+1}$, . . . , $x_T$. The goal of learning in the sequence modeling setting is to find a network f that minimizes some expected loss between the actual outputs and the predictions. . L($y_0$, . . . , $y_T$ , f($x_0$, . . . , $x_T$)), where the sequences and outputs are drawn according to some distribution. . What is a 1D convolution? . Before we jump into the paper we must understand what is a 1D convolution since it is used in the causal convolutional layer in TCN . 1D Convolution takes in a 3D tensor as input and outputs a 3D tensor as output. | Shape of the input tensor in TCN would have following dimension ( batch_size, input_length, input_size ) and the output tensor has shape ( batch_size, input_length, output_size ) | Each layer in TCN has same input and output length so only the third dimension would change. | . . Image courtesy . In the above figure we can notice the follwing . to compute a single output we need to look at 3 consecutive values of the input sequence, it is because we are using a kernel of size 3 here. | to maintain that input and output sequences be of the same size we have to pad the input sequence with zeros on both sides. | 1d convolution is a special case of 2d convolution | . . Image courtesy . How is 1d convolution a special case of 2d convolution? . In both time series and NLP, data is laid out in a similar manner, in the figure above we have embedded the words I like this movie very much ! into a 7 x 5 embedding matrix and then we use 1d convolution on this 2D matrix. . 1d convolution is a special case of 2d convolution where kernel size of the 1d convolution is it&#39;s height. The width of the kernel is defined by the embedding size, which is 5 here and it is fixed. So it means that we can only slide vertically and not horizontally which makes it 1D convolution. . What is causal convolution? . Causality means that an element in the output sequence can only depend on elements that precede it in the input sequence. | In order to ensure that an output tensor has the same length as the input tensor, we need to do zero padding. | If we only pad the left side of the input tensor with zeros, then causal convolution is guaranteed. | $x^{&#39;}_4$ in the figure below is generated by combining $x_2$, $x_3$, $x_4$ which ensures no leakage of information. | . . This operation generates $x^{&#39;}_5$ and $x^{&#39;}_6$ which are extraneous and should be removed before passing the output to the next layer. We have to take care of it in the implementation. . How many zeros would be required to make sure that the output would be of same length as input? (kernel_size - 1) . How it all fits together? . TCN has two basic principles: . input and output length of the sequences remain same. | there can be no leakage from the past. | . To achieve the first point TCN makes use of 1D FCN ( Fully Convolutional Network ) and to achieve the second point TCN makes use of causal convolutions. . Disadvantages of the above architecture . To model long term dependencies, we need a very deep network or very large convolution kernels, neither of which turned out to be particularly feasible in the experiments. | . Dilated Convolutions . A desirable quality of a the model is that the value of a particular entry in the output depends on all previous entries in the input. . | This is achieved when the size of the receptive field is equal to the length of the input. . | We could expand our receptive field when we stack multiple layers together. In the figure below we can see that by stacking two layers with kernel_size 3, we get a receptive field size of 5. . | . . In general, the receptive field r of a 1D convolutional network with n layers and kernel_size k is . $r = 1 + n * ( k - 1 )$ . To know how many layers are needed for full coverage, we can set the receptive field size to input_length l and solve for the number of layers n (non-integer values need to be rounded): . $ lceil frac{(l-1)}{(k-1)} rceil$ . This means that, with a fixed kernel_size, the number of layers required for complete coverage would be linear in input length. This will cause the network to become very deep and very fast, resulting in models with a large number of parameters that take longer to train. . How could we solve this issue? . One way to increase the size of the receptive field while keeping the number of layers relatively small is to introduce the concept of dilation. . Dilation in the context of convolutional layers refers to the distance between elements of the input sequence that are used to compute one entry of the output sequence. Therefore, a traditional convolutional layer can be viewed as a layer dilated by 1, because the input elements involved in calculating output value are adjacent. . The image below shows an example of how the receptive field grows when we introduce dilation. The right side image uses a dilation rate r 1 in the first layer with kernel_size 3 which is how a traditional conv layer would work although in the next layer we use r=2 which makes sure that we combine input elements that are 2 elements apart when producing output for the next layer and so on. . . To overcome the problem of number of layers required for covering the entire input length we must progressively increase the dilation rate over multiple layers. . This problem can be solved by exponentially increasing the value of d as we move up in the layer. To do this, we choose a constant dilation_base integer b that will allow us to calculate the dilation d for a particular layer based on the number of layers i under it, i.e. $d = b^i$. . The figure below shows a network with an input_length of 10, a kernel_size of 3, and a dilation_base of 2, which would result in a complete coverage of the 3 dilated convolutional layers. . . Here we can see that the all input values are used to produce the last value in the output layer. With the above mentioned setup we could have an input of length 15 while maintaining the full coverage. . How did we calculate that the receptive width is 15? . When a layer is added to the architecture the receptive field is increased by $d*(k-1)$ | So if we have n layers with kernel_size k and dilation base rate as b then receptive width is calculated as | . $w=1+(k-1) frac{b^n-1}{b-1}$ . but depending on values of b and k the architecture could have many holes in it. . What does that mean? . . Here we can see not all inputs are used to compute the last value of the output, even though w is greater than the input size. To fix this we would have to either increase the kernel size or decrease the dilation rate from 3 to 2. In general we must ensure that kernel_size is atleast equal to dilation rate to avoid such cases. . How many layers would be required for full coverage? . Given a kernel size k, a dilation base b where k ≥ b, and an input length l, in order to achieve full coverage following condition must be satisfied . $1+(k-1) frac{b^n-1}{b-1} geq l$, then . $n= lceil log_b( frac{(l-1)*(b-1)}{k-1}+1) rceil$ . Now number of layers is lograthmic in input layer length l which is what we wanted. This is a significant improvement that can be achieved without sacrificing receptive field coverage. . Now, the only thing that needs to be specified is the number of zero-padded items required for each layer. Assuming that the dilation expansion base is b, the kernel size is k, and there are i layers below the current layer, the number p of zero-padding items required by the current layer is calculated as follows: . $p=b^i*(k-1)$ . Temporal Residual Block . Now let&#39;s discuss the basic building blocks of TCN network. . . Residual links have proven to be an effective way to train deep networks, which allow the network to pass information in a cross-layer manner. | This paper constructs a residual block to replace one layer of convolution. As shown in the figure above, a residual block contains two layers of convolution and nonlinear mapping, and WeightNorm and Dropout are added to each layer to regularize the network. | . . Each hidden layer has the same length as the input layer, and is padded with zeros to ensure subsequent layers have the same length. . | For the output at time t, the causal convolution (convolution with causal constraints) uses the input at time t and the previous layer at an earlier time (see the blue line connection at the bottom of the figure above). . | Causal convolution is not a new idea, but the paper incorporates very deep networks to allow for long-term efficient histories. . | . Residual Link . . Residual blocks (originally from ResNet) have repeatedly shown to benefit very deep networks. | Since the receptive field of a TCN depends on the network depth n as well as the convolution kernel size k and dilation factor d, it becomes very important to stabilize deeper and larger TCNs. | Predictions may depend on long historical values and high-dimensional input sequences. e.g An input sequence of size $2^{12}$ may require a network of up to 12 layers. | In standard ResNet, the input is directly added to the output of the residual function, while in TCN the input and output can have different widths. To account for the difference in input-output width, an additional 1x1 convolution is used to ensure that element-wise addition receives tensors of the same shape. | . . Conclusion . The innovation in the TCN model is to sort out how to use causal and dilated convolutions to solve the sequence modelling task. | Causal and Dilated convolutions have already been proposed earlier but this paper highlights how they could be combined together for sequence modelling tasks | . Advantages . Parallelism. When given a sentence, TCN can process the sentence in parallel without the need for sequential processing like RNN. . | Flexible receptive field. The size of the receptive field of TCN is determined by the number of layers, the size of the convolution kernel, and the expansion coefficient. It can be flexibly customized according to different characteristics of different tasks. . | Stable gradient. RNN often has the problems of vanishing gradients and gradient explosion, which are mainly caused by sharing parameters in different time periods. Like traditional convolutional neural networks, TCN does not have the problem of gradient disappearance and explosion. . | Lower memory requirements. When RNN is used, it needs to save the information of each step, which will occupy a lot of memory. The convolution kernel of TCN is shared in one layer, and hence lower memory usage. . | . Disadvantages . TCN may not be so adaptable in transfer learning. This is because the amount of historical information required for model predictions may be different in different domains. Therefore, when migrating a model from a problem that requires less memory information to a problem that requires longer memory, TCN may perform poorly because its receptive field is not large enough. . | The TCN described in the paper is also a one-way structure. In tasks such as speech recognition and speech synthesis, the pure one-way structure is quite useful. However, most of the texts use a bidirectional structure. Of course, it is easy to expand the TCN into a bidirectional structure. Instead of using causal convolution, the traditional convolution structure can be used. . | TCN is a variant of convolutional neural network after all. Although the receptive field can be expanded by using dilated convolution, it is still limited. Compared with Transformer, it is still poor in capturing relevant information of any length. The application of TCN to text remains to be tested. . | . Tips for implementation . Next we would highlight things to keep in mind if you plan to implement the paper . After the convolution, the size of the output data after the convolution is greater than the size of the input data | This is caused owing to padding both sides, so we chomp off extra padded 0s from right side to get the desired data values. | . We have taken this (https://github.com/locuslab/TCN/blob/2221de3323/TCN/tcn.py) implementation of TCN and implemented in fast.ai and tsai to demonstrate how TCN could be used for sequence modelling. . How to prepare dataset? . from tsai.all import * computer_setup() . . We are going to select appliances energy dataset recently released by Monash, UEA &amp; UCR Time Series Extrinsic Regression Repository (2020) . dsid = &#39;AppliancesEnergy&#39; X, y, splits = get_regression_data(dsid, split_data=False) X.shape, y.shape, y[:10] . . check_data(X, y, splits) . . tfms = [None, [TSRegression()]] batch_tfms = TSStandardize(by_sample=True, by_var=True) dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=128) dls.one_batch() . . dls.show_batch() . . Model implementation . from fastai.torch_basics import * from fastai.tabular.core import * from torch.nn.utils import weight_norm . class Chomp1d(Module): def __init__(self, chomp_size): store_attr() def forward(self, x): return x[:, :, :-self.chomp_size].contiguous() def get_conv_block(n_inputs, n_outputs, kernel_size, stride, padding, dilation, dropout): conv = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation )) chomp = Chomp1d(padding) relu = nn.ReLU() drop = nn.Dropout(dropout) return nn.Sequential(*(conv, chomp, relu, drop )) class TemporalBlock(Module): def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.5): store_attr() self.in_conv_blk = get_conv_block(n_inputs, n_outputs, kernel_size, stride, padding, dilation, dropout ) self.out_conv_blk = get_conv_block(n_outputs, n_outputs, kernel_size, stride, padding, dilation, dropout ) self.net = nn.Sequential(*(self.in_conv_blk, self.out_conv_blk)) self.downsample_conv = nn.Conv1d(n_inputs, n_outputs, kernel_size=1) if n_inputs != n_outputs else None self.relu = nn.ReLU() self.init_weights() def init_weights(self): # 0 index represents the convolutional layer self.in_conv_blk[0].weight.data.normal_(0, 0.01) self.out_conv_blk[0].weight.data.normal_(0, 0.01) if self.downsample_conv is not None: self.downsample_conv.weight.data.normal_(0, 0.01) def forward(self, x): out = self.net(x) res = x if self.downsample_conv is None else self.downsample_conv(x) return self.relu(out + res) class TemporalConvNet(Module): def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2): layers = [] num_levels = len(num_channels) for i in range(num_levels): dilation_size = 2 ** i in_channels = num_inputs if i == 0 else num_channels[i-1] out_channels = num_channels[i] layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=(kernel_size-1) * dilation_size, dropout=dropout ) ] self.network = nn.Sequential(*layers) def forward(self, x): return self.network(x) class TCN(Module): def __init__(self, input_size, output_size, num_channels, kernel_size, dropout): self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout) self.linear = nn.Linear(num_channels[-1], output_size) self.init_weights() def init_weights(self): self.linear.weight.data.normal_(0, 0.01) def forward(self, x): y1 = self.tcn(x) return self.linear(y1[:, :, -1]) . model = TCN(input_size=24, output_size=1, num_channels=[24, 32, 64], kernel_size=2, dropout=0.2 ) learn = Learner(dls, model, metrics=[mae, rmse], cbs=ShowGraph()) learn.lr_find() . . model = TCN(input_size=24, output_size=1, num_channels=[24, 32, 64], kernel_size=2, dropout=0.2 ) learn = Learner(dls, model, metrics=[mae, rmse], cbs=ShowGraph()) learn.fit_one_cycle(100, 8e-4) . . References . An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling | Fast.AI | tsai | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/tsai/sequencemodelling/2021/12/22/temporal-convolutional-networks.html",
            "relUrl": "/deeplearning/math/fastai/tsai/sequencemodelling/2021/12/22/temporal-convolutional-networks.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Do we need downsampling?",
            "content": "Objective . Deep CNN architecutures are made up of many components like conv layer, activation function, pooling layers, batch-norm layer etc.All of them are designed for specific reasons and to better understand the effect of these components it is important to play with them. For example, we include layers like pooling, strided convolution etc to reduce the size of the input. . If we look at the architecture of VGG below, we see that lot of max_pooling layers are used. The idea is to increase the receptive field and decrease the number of parameters by reducing the size of the input. But then a question arises, do we really need to downsample? Luckily I came across this repository where this specific question is addressed and the author has tried to replace downsampling layers with dilated convolutions or large kernels with appropriate padding to address this issue. The purpose of this post is to implement and validate these ideas by performing the above mentioned experiments on CIFAR-10 dataset using fastai. The intention is to show how easy it is for us to experiment using fastai. . . Libraries . let&#39;s start by installing fastai2 and keras . !pip install fastai2 keras &gt; /dev/null . from fastai2.vision.all import * from keras.datasets import cifar10 . Dataset . We are going to train our network on CIFAR-10 dataset. CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. . (x_train,y_train),(x_test,y_test) = cifar10.load_data() . Datasets API . This is where fastai comes in with its flexible api to form a dataset that is easily consumable by the models. We are passing a list of pairs of (image, label) to the pipeline. No data augmentation is applied. To validate our ideas we will keep a separate holdout set. If you want to understand more about Datasets api please read official docs . items = np.array(list(zip(x_train, y_train.ravel()))) # 80-20 percent split splits = RandomSplitter(seed=42)(items) tfms = [[lambda x: x[0], PILImage.create], [lambda x: x[1], Categorize()]] dsets = Datasets(items, tfms, splits=splits) dls = dsets.dataloaders(bs=64, after_item=[ToTensor(), IntToFloatTensor()]) . dls.show_batch(figsize=(4, 4)) . . VGG ( 4 layer ) network . Let&#39;s try to train a 4 layer VGG based network with downsampling and see the performance on CIFAR-10. . class VGG_4(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG_4, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=16, out_channels=24, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=24, out_channels=32, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=48, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . learn = Learner(dls, VGG_4(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . Let&#39;s train it for 30 epochs . learn.fit_one_cycle(30, 1e-2) . . learner.summary() . . VGG with dilated convolution . Idea is to progressively increase the dilation from 1, 2, 4, 8 to increase the receptive field of the network. . class VGG4_Dilation(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG4_Dilation, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3), dilation=1), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.Conv2d(in_channels=16, out_channels=24, padding=(1, 1), kernel_size=(3, 3), dilation=2), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.Conv2d(in_channels=24, out_channels=32, padding=(1, 1), kernel_size=(3, 3), dilation=4), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(in_channels=32, out_channels=48, padding=(1, 1), kernel_size=(3, 3), dilation=8), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . In the head of the model instead of using a fully connected layer we are using AdaptiveAvgPool2d with 1x1 convolution layer, it is because researchers have observed that using AdaptiveAvgPool2d with 1x1 convolution layer decreases the total number of parameters without taking a hit on the performance. . learn = Learner(dls, VGG4_Dilation(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . learn.fit_one_cycle(30, 1e-2) . . learn.summary() . . Note: There is no change in number of trainable in params in vanilla vgg 4 layer model with dowsampling and vgg 4 layer model with dilation. . VGG with large kernels . We plan to progressively increase the size of the kernels from 3 to 9. Increasing the kernel size would enable us to increase the receptive field of the network but we have to make sure that we use adequate padding so as to not shrink our input. . class VGG4_large_filter(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG4_large_filter, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.Conv2d(in_channels=16, out_channels=24, padding=(2, 2), kernel_size=(5, 5)), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.Conv2d(in_channels=24, out_channels=32, padding=(3, 3), kernel_size=(7, 7)), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(in_channels=32, out_channels=48, padding=(4, 4), kernel_size=(9, 9)), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . learn = Learner(dls, VGG4_large_filter(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . learn.fit_one_cycle(30, 5e-3) . . learn.summary() . . Even though there is an improvement in terms of loss but number of trainable parameters have jumped from 25,474 to 172, 930 . Conclusion . If we look at the performance of 4 layer VGG network with downsampling and compare it with dilated and large kernel size, we observe that their is an increase in the performance. | Using a large kernel will improve performance but at the cost of increased number of trainable parameters. | Using dilation would improve performance without increasing the number of trainable parameters | . Next steps . To make it generalizable, we would have to test this idea on other architectures e.g. (resnet18) and see if it increases performance or not. | Also it would be interesting to see what kind of features will our models learn if we use dilation. | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/fastai/cnn/2020/05/02/Do-we-need-downsampling.html",
            "relUrl": "/deeplearning/fastai/cnn/2020/05/02/Do-we-need-downsampling.html",
            "date": " • May 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Machine Learning Engineer with 6+ years of experience in machine learning, deep learning and MLOPs. Presently working in Gojek on building a Data Science Platform on top of Kubeflow to enable DS teams to take models to production faster. | Working on a real-time feature store to manage data pipelines and features required for running real-time ML models in production. | Prior to moving into building the platform worked as data scientist in user-growth team in Gopay ( financial arm of Gojek ) to build models for estimating Customer Lifetime Value to help business target and cater to user needs in a better way. We have been working on a model that has helped organisation champion organic growth and other growth efforts. This model has helped organisation bring down cost spent on promotions by 22% without taking hit on growth efforts. | Worked as data scientist in Gopay to build models for fraud and account takeover detection ( ATO ). This model predicts whether an incoming request is an ATO or not. This model has been running in real-time setting and has resulted in 33% reduction in tickets. | Before joining Gojek, worked in DeltaX and worked on Data Driven Attribution where we created data-driven models by borrowing ideas from Game Theory like Ordered Shapley Value to attribute conversions tracked by our system which served as building block for another model that takes into account converting and non-converting user journeys to calculate the importance of a campaign for the advertiser. It helped advertisers get better understanding of their audiences. | Also worked on budget distribution problem where we helped develop an algorithm that would distribute budgets among competing ad-groups running on Facebook while optimizing the business objective of the advertiser. This is a big step up from traditional approaches which mostly look at single indicator and it helped advertisers plan better. | . | Mentor at Scaler academy where mentored lots of Machine Learning enthusiasts on how to take their career forward and setup Machine Learning infrastructure, models and practices in their respective organizations. | Open Source contributor at numb3r33 | Interested in competitive machine learning Winner of Deep Learning Hackathon organized by HackerEarth (https://www.hackerearth.com/challenges/competitive/deep-learning-beginner-challenge/leaderboard/). | Ranked in Top-100 data scientists on Analytics Vidhya platform which has around 3500 competitors at the time of writing. | . | Interested in blogging about machine learning, deep learning and mathematics, you can read my posts here | . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://numb3r33.github.io/experiments/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://numb3r33.github.io/experiments/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}