{
  
    
        "post0": {
            "title": "Residual Learning",
            "content": "What is a residual? . Residuals are the difference between actual and estimated value. . What is residual learning? . In the context of ensemble learning, a base model is used to fit the residuals to make the ensemble model more accurate. In deep learning, various architectures use a block/layer to fit the residual to improve the performance of the DNN. . How does Gradient Boosting Machines use residuals? . from sklearn import datasets from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt . X, y = datasets.make_regression(n_samples=1000, random_state=41) Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.2, random_state=41) . from sklearn.tree import DecisionTreeRegressor . tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg1.fit(Xtr, ytr) y2 = ytr - tree_reg1.predict(Xtr) tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg2.fit(Xtr, y2) y3 = y2 - tree_reg2.predict(Xtr) tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg3.fit(Xtr, y3) y_pred = sum(tree.predict(Xva) for tree in (tree_reg1, tree_reg2, tree_reg3)) . Gradient Boosting . How does residuals play a part in Gradient Boosting Learning? . Train a base learner tree_reg1 to fit data (X) and labels (y) | Train a base learner tree_reg2 that fits on data (X) and residuals between the label and predicted value of base learner tree_reg1. Essentially, we are using a base learner to learn the residuals. | Finally the result of all the base learners are added to make the final prediction. | . The above code is equivalent to calling the GradientBoostingRegressor with 3 base learners. . from sklearn.ensemble import GradientBoostingRegressor gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=41) gbrt.fit(Xtr, ytr) gb_preds = gbrt.predict(Xva) . sum(y_pred - gb_preds) . Role of residual learning in training deep networks? . Example: Compare two networks trained to fit &quot;pi&quot; using with and without residual block . from fastai.data.all import * from fastai.vision.all import * . bs = 1 items = [(1., np.pi)] . items . [(1.0, 3.141592653589793)] . class f(ItemTransform): def encodes(self, x): return x[0] class s(ItemTransform): def encodes(self, x): return x[1] dsets = Datasets(items, tfms=[[f],[s]]) dls = dsets.dataloaders(bs=1) dls.one_batch() # dls = DataLoader(items, bs=1) # dls.one_batch() . (tensor([1.], device=&#39;cuda:0&#39;, dtype=torch.float64), tensor([3.1416], device=&#39;cuda:0&#39;, dtype=torch.float64)) . class WithoutResBlock(Module): def __init__(self, n): store_attr(&#39;n&#39;) self.lin = nn.Linear(1, 1) def forward(self, x): out = self.lin(x) for i in range(self. n): out = self.lin(out) return out . class DummyResBlock(Module): def __init__(self, n): store_attr(&#39;n&#39;) self.lin = nn.Linear(1, 1) def forward(self, x): out = self.lin(x) for i in range(self.n): t = self.lin(out) out = t + out return out . x, y = dls.one_batch() m = DummyResBlock(n=2).cuda().double() m(x) . tensor([1.6790], device=&#39;cuda:0&#39;, dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;) . m = DummyResBlock(n=2).cuda().double() learn = Learner(dls, m, loss_func=mse) . learn.fit(n_epoch=500) . epoch train_loss valid_loss time . 0 | 8.327976 | None | 00:00 | . 1 | 8.324323 | None | 00:00 | . 2 | 8.320645 | None | 00:00 | . 3 | 8.316943 | None | 00:00 | . 4 | 8.313217 | None | 00:00 | . 5 | 8.309466 | None | 00:00 | . 6 | 8.305691 | None | 00:00 | . 7 | 8.301890 | None | 00:00 | . 8 | 8.298066 | None | 00:00 | . 9 | 8.294218 | None | 00:00 | . 10 | 8.290344 | None | 00:00 | . 11 | 8.286447 | None | 00:00 | . 12 | 8.282524 | None | 00:00 | . 13 | 8.278578 | None | 00:00 | . 14 | 8.274607 | None | 00:01 | . 15 | 8.270611 | None | 00:00 | . 16 | 8.266590 | None | 00:00 | . 17 | 8.262545 | None | 00:00 | . 18 | 8.258476 | None | 00:00 | . 19 | 8.254381 | None | 00:00 | . 20 | 8.250263 | None | 00:00 | . 21 | 8.246120 | None | 00:00 | . 22 | 8.241953 | None | 00:00 | . 23 | 8.237761 | None | 00:00 | . 24 | 8.233544 | None | 00:00 | . 25 | 8.229304 | None | 00:00 | . 26 | 8.225039 | None | 00:00 | . 27 | 8.220749 | None | 00:00 | . 28 | 8.216435 | None | 00:00 | . 29 | 8.212096 | None | 00:00 | . 30 | 8.207733 | None | 00:00 | . 31 | 8.203345 | None | 00:00 | . 32 | 8.198934 | None | 00:00 | . 33 | 8.194498 | None | 00:00 | . 34 | 8.190037 | None | 00:00 | . 35 | 8.185553 | None | 00:00 | . 36 | 8.181044 | None | 00:00 | . 37 | 8.176510 | None | 00:00 | . 38 | 8.171951 | None | 00:00 | . 39 | 8.167370 | None | 00:00 | . 40 | 8.162764 | None | 00:00 | . 41 | 8.158134 | None | 00:00 | . 42 | 8.153479 | None | 00:00 | . 43 | 8.148800 | None | 00:00 | . 44 | 8.144097 | None | 00:00 | . 45 | 8.139371 | None | 00:00 | . 46 | 8.134620 | None | 00:00 | . 47 | 8.129845 | None | 00:00 | . 48 | 8.125046 | None | 00:00 | . 49 | 8.120223 | None | 00:00 | . 50 | 8.115376 | None | 00:00 | . 51 | 8.110506 | None | 00:00 | . 52 | 8.105612 | None | 00:00 | . 53 | 8.100694 | None | 00:00 | . 54 | 8.095752 | None | 00:00 | . 55 | 8.090785 | None | 00:00 | . 56 | 8.085796 | None | 00:00 | . 57 | 8.080784 | None | 00:00 | . 58 | 8.075747 | None | 00:00 | . 59 | 8.070686 | None | 00:00 | . 60 | 8.065603 | None | 00:00 | . 61 | 8.060496 | None | 00:00 | . 62 | 8.055365 | None | 00:00 | . 63 | 8.050211 | None | 00:00 | . 64 | 8.045033 | None | 00:00 | . 65 | 8.039834 | None | 00:00 | . 66 | 8.034610 | None | 00:00 | . 67 | 8.029363 | None | 00:00 | . 68 | 8.024092 | None | 00:00 | . 69 | 8.018799 | None | 00:00 | . 70 | 8.013482 | None | 00:00 | . 71 | 8.008143 | None | 00:00 | . 72 | 8.002781 | None | 00:00 | . 73 | 7.997396 | None | 00:00 | . 74 | 7.991988 | None | 00:00 | . 75 | 7.986557 | None | 00:00 | . 76 | 7.981103 | None | 00:00 | . 77 | 7.975626 | None | 00:00 | . 78 | 7.970128 | None | 00:00 | . 79 | 7.964606 | None | 00:00 | . 80 | 7.959062 | None | 00:00 | . 81 | 7.953495 | None | 00:00 | . 82 | 7.947906 | None | 00:00 | . 83 | 7.942294 | None | 00:00 | . 84 | 7.936660 | None | 00:00 | . 85 | 7.931005 | None | 00:00 | . 86 | 7.925326 | None | 00:00 | . 87 | 7.919625 | None | 00:00 | . 88 | 7.913902 | None | 00:00 | . 89 | 7.908157 | None | 00:00 | . 90 | 7.902390 | None | 00:00 | . 91 | 7.896601 | None | 00:00 | . 92 | 7.890790 | None | 00:00 | . 93 | 7.884958 | None | 00:00 | . 94 | 7.879103 | None | 00:00 | . 95 | 7.873227 | None | 00:00 | . 96 | 7.867328 | None | 00:00 | . 97 | 7.861408 | None | 00:00 | . 98 | 7.855467 | None | 00:00 | . 99 | 7.849504 | None | 00:00 | . 100 | 7.843520 | None | 00:00 | . 101 | 7.837513 | None | 00:00 | . 102 | 7.831487 | None | 00:00 | . 103 | 7.825438 | None | 00:00 | . 104 | 7.819368 | None | 00:00 | . 105 | 7.813277 | None | 00:00 | . 106 | 7.807165 | None | 00:00 | . 107 | 7.801031 | None | 00:00 | . 108 | 7.794876 | None | 00:00 | . 109 | 7.788700 | None | 00:00 | . 110 | 7.782504 | None | 00:00 | . 111 | 7.776286 | None | 00:00 | . 112 | 7.770047 | None | 00:00 | . 113 | 7.763788 | None | 00:00 | . 114 | 7.757508 | None | 00:00 | . 115 | 7.751206 | None | 00:00 | . 116 | 7.744885 | None | 00:00 | . 117 | 7.738543 | None | 00:00 | . 118 | 7.732180 | None | 00:00 | . 119 | 7.725796 | None | 00:00 | . 120 | 7.719392 | None | 00:00 | . 121 | 7.712967 | None | 00:00 | . 122 | 7.706522 | None | 00:00 | . 123 | 7.700058 | None | 00:00 | . 124 | 7.693572 | None | 00:00 | . 125 | 7.687066 | None | 00:00 | . 126 | 7.680540 | None | 00:00 | . 127 | 7.673994 | None | 00:00 | . 128 | 7.667427 | None | 00:00 | . 129 | 7.660840 | None | 00:00 | . 130 | 7.654234 | None | 00:00 | . 131 | 7.647607 | None | 00:00 | . 132 | 7.640960 | None | 00:00 | . 133 | 7.634293 | None | 00:00 | . 134 | 7.627607 | None | 00:00 | . 135 | 7.620900 | None | 00:00 | . 136 | 7.614173 | None | 00:00 | . 137 | 7.607427 | None | 00:00 | . 138 | 7.600661 | None | 00:00 | . 139 | 7.593875 | None | 00:00 | . 140 | 7.587069 | None | 00:00 | . 141 | 7.580244 | None | 00:00 | . 142 | 7.573400 | None | 00:00 | . 143 | 7.566535 | None | 00:00 | . 144 | 7.559651 | None | 00:00 | . 145 | 7.552747 | None | 00:00 | . 146 | 7.545824 | None | 00:00 | . 147 | 7.538881 | None | 00:00 | . 148 | 7.531919 | None | 00:00 | . 149 | 7.524938 | None | 00:00 | . 150 | 7.517936 | None | 00:00 | . 151 | 7.510916 | None | 00:00 | . 152 | 7.503876 | None | 00:00 | . 153 | 7.496817 | None | 00:00 | . 154 | 7.489738 | None | 00:00 | . 155 | 7.482641 | None | 00:00 | . 156 | 7.475523 | None | 00:00 | . 157 | 7.468388 | None | 00:00 | . 158 | 7.461232 | None | 00:00 | . 159 | 7.454057 | None | 00:00 | . 160 | 7.446863 | None | 00:00 | . 161 | 7.439650 | None | 00:00 | . 162 | 7.432417 | None | 00:00 | . 163 | 7.425166 | None | 00:00 | . 164 | 7.417895 | None | 00:00 | . 165 | 7.410606 | None | 00:00 | . 166 | 7.403297 | None | 00:00 | . 167 | 7.395969 | None | 00:00 | . 168 | 7.388622 | None | 00:00 | . 169 | 7.381257 | None | 00:00 | . 170 | 7.373871 | None | 00:00 | . 171 | 7.366467 | None | 00:00 | . 172 | 7.359044 | None | 00:00 | . 173 | 7.351602 | None | 00:00 | . 174 | 7.344141 | None | 00:00 | . 175 | 7.336661 | None | 00:00 | . 176 | 7.329161 | None | 00:00 | . 177 | 7.321643 | None | 00:00 | . 178 | 7.314106 | None | 00:00 | . 179 | 7.306550 | None | 00:00 | . 180 | 7.298974 | None | 00:00 | . 181 | 7.291380 | None | 00:00 | . 182 | 7.283767 | None | 00:00 | . 183 | 7.276135 | None | 00:00 | . 184 | 7.268484 | None | 00:00 | . 185 | 7.260813 | None | 00:00 | . 186 | 7.253125 | None | 00:00 | . 187 | 7.245417 | None | 00:00 | . 188 | 7.237689 | None | 00:00 | . 189 | 7.229943 | None | 00:00 | . 190 | 7.222178 | None | 00:00 | . 191 | 7.214394 | None | 00:00 | . 192 | 7.206591 | None | 00:00 | . 193 | 7.198768 | None | 00:00 | . 194 | 7.190928 | None | 00:00 | . 195 | 7.183067 | None | 00:00 | . 196 | 7.175188 | None | 00:00 | . 197 | 7.167290 | None | 00:00 | . 198 | 7.159372 | None | 00:00 | . 199 | 7.151436 | None | 00:00 | . 200 | 7.143480 | None | 00:00 | . 201 | 7.135506 | None | 00:00 | . 202 | 7.127512 | None | 00:00 | . 203 | 7.119499 | None | 00:00 | . 204 | 7.111467 | None | 00:00 | . 205 | 7.103416 | None | 00:00 | . 206 | 7.095346 | None | 00:00 | . 207 | 7.087256 | None | 00:00 | . 208 | 7.079149 | None | 00:00 | . 209 | 7.071021 | None | 00:00 | . 210 | 7.062873 | None | 00:00 | . 211 | 7.054708 | None | 00:00 | . 212 | 7.046522 | None | 00:00 | . 213 | 7.038317 | None | 00:00 | . 214 | 7.030094 | None | 00:00 | . 215 | 7.021850 | None | 00:00 | . 216 | 7.013587 | None | 00:00 | . 217 | 7.005306 | None | 00:00 | . 218 | 6.997005 | None | 00:00 | . 219 | 6.988685 | None | 00:00 | . 220 | 6.980345 | None | 00:00 | . 221 | 6.971985 | None | 00:00 | . 222 | 6.963606 | None | 00:00 | . 223 | 6.955208 | None | 00:00 | . 224 | 6.946790 | None | 00:00 | . 225 | 6.938353 | None | 00:00 | . 226 | 6.929896 | None | 00:00 | . 227 | 6.921420 | None | 00:00 | . 228 | 6.912924 | None | 00:00 | . 229 | 6.904409 | None | 00:00 | . 230 | 6.895874 | None | 00:00 | . 231 | 6.887319 | None | 00:00 | . 232 | 6.878744 | None | 00:00 | . 233 | 6.870150 | None | 00:00 | . 234 | 6.861537 | None | 00:00 | . 235 | 6.852903 | None | 00:00 | . 236 | 6.844250 | None | 00:00 | . 237 | 6.835577 | None | 00:00 | . 238 | 6.826884 | None | 00:00 | . 239 | 6.818171 | None | 00:00 | . 240 | 6.809439 | None | 00:00 | . 241 | 6.800686 | None | 00:00 | . 242 | 6.791914 | None | 00:00 | . 243 | 6.783121 | None | 00:00 | . 244 | 6.774309 | None | 00:00 | . 245 | 6.765476 | None | 00:00 | . 246 | 6.756624 | None | 00:00 | . 247 | 6.747752 | None | 00:00 | . 248 | 6.738859 | None | 00:00 | . 249 | 6.729946 | None | 00:00 | . 250 | 6.721014 | None | 00:00 | . 251 | 6.712060 | None | 00:00 | . 252 | 6.703087 | None | 00:00 | . 253 | 6.694094 | None | 00:00 | . 254 | 6.685081 | None | 00:00 | . 255 | 6.676046 | None | 00:00 | . 256 | 6.666992 | None | 00:00 | . 257 | 6.657917 | None | 00:00 | . 258 | 6.648822 | None | 00:00 | . 259 | 6.639707 | None | 00:00 | . 260 | 6.630571 | None | 00:00 | . 261 | 6.621415 | None | 00:00 | . 262 | 6.612239 | None | 00:00 | . 263 | 6.603041 | None | 00:00 | . 264 | 6.593823 | None | 00:00 | . 265 | 6.584585 | None | 00:00 | . 266 | 6.575326 | None | 00:00 | . 267 | 6.566047 | None | 00:00 | . 268 | 6.556747 | None | 00:00 | . 269 | 6.547426 | None | 00:00 | . 270 | 6.538085 | None | 00:00 | . 271 | 6.528723 | None | 00:00 | . 272 | 6.519340 | None | 00:00 | . 273 | 6.509937 | None | 00:00 | . 274 | 6.500512 | None | 00:00 | . 275 | 6.491067 | None | 00:00 | . 276 | 6.481601 | None | 00:00 | . 277 | 6.472115 | None | 00:00 | . 278 | 6.462607 | None | 00:00 | . 279 | 6.453079 | None | 00:00 | . 280 | 6.443530 | None | 00:00 | . 281 | 6.433959 | None | 00:00 | . 282 | 6.424368 | None | 00:00 | . 283 | 6.414756 | None | 00:00 | . 284 | 6.405123 | None | 00:00 | . 285 | 6.395469 | None | 00:00 | . 286 | 6.385793 | None | 00:00 | . 287 | 6.376097 | None | 00:00 | . 288 | 6.366380 | None | 00:00 | . 289 | 6.356641 | None | 00:00 | . 290 | 6.346881 | None | 00:00 | . 291 | 6.337101 | None | 00:00 | . 292 | 6.327300 | None | 00:00 | . 293 | 6.317476 | None | 00:00 | . 294 | 6.307632 | None | 00:00 | . 295 | 6.297767 | None | 00:00 | . 296 | 6.287880 | None | 00:00 | . 297 | 6.277973 | None | 00:00 | . 298 | 6.268044 | None | 00:00 | . 299 | 6.258093 | None | 00:00 | . 300 | 6.248122 | None | 00:00 | . 301 | 6.238130 | None | 00:00 | . 302 | 6.228116 | None | 00:00 | . 303 | 6.218080 | None | 00:00 | . 304 | 6.208024 | None | 00:00 | . 305 | 6.197946 | None | 00:00 | . 306 | 6.187847 | None | 00:00 | . 307 | 6.177726 | None | 00:00 | . 308 | 6.167585 | None | 00:00 | . 309 | 6.157422 | None | 00:00 | . 310 | 6.147237 | None | 00:00 | . 311 | 6.137031 | None | 00:00 | . 312 | 6.126804 | None | 00:00 | . 313 | 6.116556 | None | 00:00 | . 314 | 6.106285 | None | 00:00 | . 315 | 6.095994 | None | 00:00 | . 316 | 6.085681 | None | 00:00 | . 317 | 6.075347 | None | 00:00 | . 318 | 6.064992 | None | 00:00 | . 319 | 6.054615 | None | 00:00 | . 320 | 6.044217 | None | 00:00 | . 321 | 6.033797 | None | 00:00 | . 322 | 6.023356 | None | 00:00 | . 323 | 6.012894 | None | 00:00 | . 324 | 6.002409 | None | 00:00 | . 325 | 5.991905 | None | 00:00 | . 326 | 5.981378 | None | 00:00 | . 327 | 5.970830 | None | 00:00 | . 328 | 5.960261 | None | 00:00 | . 329 | 5.949670 | None | 00:00 | . 330 | 5.939059 | None | 00:00 | . 331 | 5.928426 | None | 00:00 | . 332 | 5.917772 | None | 00:00 | . 333 | 5.907096 | None | 00:00 | . 334 | 5.896399 | None | 00:00 | . 335 | 5.885681 | None | 00:00 | . 336 | 5.874941 | None | 00:00 | . 337 | 5.864180 | None | 00:00 | . 338 | 5.853397 | None | 00:00 | . 339 | 5.842594 | None | 00:00 | . 340 | 5.831769 | None | 00:00 | . 341 | 5.820924 | None | 00:00 | . 342 | 5.810057 | None | 00:00 | . 343 | 5.799169 | None | 00:00 | . 344 | 5.788259 | None | 00:00 | . 345 | 5.777328 | None | 00:00 | . 346 | 5.766377 | None | 00:00 | . 347 | 5.755405 | None | 00:00 | . 348 | 5.744411 | None | 00:00 | . 349 | 5.733397 | None | 00:00 | . 350 | 5.722361 | None | 00:00 | . 351 | 5.711304 | None | 00:00 | . 352 | 5.700227 | None | 00:00 | . 353 | 5.689128 | None | 00:00 | . 354 | 5.678009 | None | 00:00 | . 355 | 5.666869 | None | 00:00 | . 356 | 5.655708 | None | 00:00 | . 357 | 5.644526 | None | 00:00 | . 358 | 5.633324 | None | 00:00 | . 359 | 5.622100 | None | 00:00 | . 360 | 5.610856 | None | 00:00 | . 361 | 5.599592 | None | 00:00 | . 362 | 5.588306 | None | 00:00 | . 363 | 5.577000 | None | 00:00 | . 364 | 5.565674 | None | 00:00 | . 365 | 5.554327 | None | 00:00 | . 366 | 5.542960 | None | 00:00 | . 367 | 5.531572 | None | 00:00 | . 368 | 5.520164 | None | 00:00 | . 369 | 5.508735 | None | 00:00 | . 370 | 5.497286 | None | 00:00 | . 371 | 5.485817 | None | 00:00 | . 372 | 5.474328 | None | 00:00 | . 373 | 5.462819 | None | 00:00 | . 374 | 5.451289 | None | 00:00 | . 375 | 5.439739 | None | 00:00 | . 376 | 5.428169 | None | 00:00 | . 377 | 5.416580 | None | 00:00 | . 378 | 5.404970 | None | 00:00 | . 379 | 5.393341 | None | 00:00 | . 380 | 5.381692 | None | 00:00 | . 381 | 5.370023 | None | 00:00 | . 382 | 5.358335 | None | 00:00 | . 383 | 5.346626 | None | 00:00 | . 384 | 5.334898 | None | 00:00 | . 385 | 5.323151 | None | 00:00 | . 386 | 5.311385 | None | 00:00 | . 387 | 5.299598 | None | 00:00 | . 388 | 5.287793 | None | 00:00 | . 389 | 5.275968 | None | 00:00 | . 390 | 5.264124 | None | 00:00 | . 391 | 5.252261 | None | 00:00 | . 392 | 5.240379 | None | 00:00 | . 393 | 5.228478 | None | 00:00 | . 394 | 5.216558 | None | 00:00 | . 395 | 5.204619 | None | 00:00 | . 396 | 5.192661 | None | 00:00 | . 397 | 5.180685 | None | 00:00 | . 398 | 5.168690 | None | 00:00 | . 399 | 5.156676 | None | 00:00 | . 400 | 5.144644 | None | 00:00 | . 401 | 5.132593 | None | 00:00 | . 402 | 5.120524 | None | 00:00 | . 403 | 5.108437 | None | 00:00 | . 404 | 5.096331 | None | 00:00 | . 405 | 5.084208 | None | 00:00 | . 406 | 5.072066 | None | 00:00 | . 407 | 5.059906 | None | 00:00 | . 408 | 5.047729 | None | 00:00 | . 409 | 5.035534 | None | 00:00 | . 410 | 5.023322 | None | 00:00 | . 411 | 5.011091 | None | 00:00 | . 412 | 4.998843 | None | 00:00 | . 413 | 4.986577 | None | 00:00 | . 414 | 4.974294 | None | 00:00 | . 415 | 4.961994 | None | 00:00 | . 416 | 4.949677 | None | 00:00 | . 417 | 4.937343 | None | 00:00 | . 418 | 4.924991 | None | 00:00 | . 419 | 4.912623 | None | 00:00 | . 420 | 4.900238 | None | 00:00 | . 421 | 4.887836 | None | 00:00 | . 422 | 4.875417 | None | 00:00 | . 423 | 4.862982 | None | 00:00 | . 424 | 4.850531 | None | 00:00 | . 425 | 4.838063 | None | 00:00 | . 426 | 4.825579 | None | 00:00 | . 427 | 4.813079 | None | 00:00 | . 428 | 4.800563 | None | 00:00 | . 429 | 4.788031 | None | 00:00 | . 430 | 4.775484 | None | 00:00 | . 431 | 4.762920 | None | 00:00 | . 432 | 4.750341 | None | 00:00 | . 433 | 4.737746 | None | 00:00 | . 434 | 4.725136 | None | 00:00 | . 435 | 4.712511 | None | 00:00 | . 436 | 4.699871 | None | 00:00 | . 437 | 4.687216 | None | 00:00 | . 438 | 4.674546 | None | 00:00 | . 439 | 4.661861 | None | 00:00 | . 440 | 4.649161 | None | 00:00 | . 441 | 4.636446 | None | 00:00 | . 442 | 4.623718 | None | 00:00 | . 443 | 4.610975 | None | 00:00 | . 444 | 4.598217 | None | 00:00 | . 445 | 4.585445 | None | 00:00 | . 446 | 4.572660 | None | 00:00 | . 447 | 4.559861 | None | 00:00 | . 448 | 4.547048 | None | 00:00 | . 449 | 4.534221 | None | 00:00 | . 450 | 4.521380 | None | 00:00 | . 451 | 4.508527 | None | 00:00 | . 452 | 4.495660 | None | 00:00 | . 453 | 4.482780 | None | 00:00 | . 454 | 4.469886 | None | 00:00 | . 455 | 4.456981 | None | 00:00 | . 456 | 4.444062 | None | 00:00 | . 457 | 4.431131 | None | 00:00 | . 458 | 4.418187 | None | 00:00 | . 459 | 4.405231 | None | 00:00 | . 460 | 4.392263 | None | 00:00 | . 461 | 4.379282 | None | 00:00 | . 462 | 4.366290 | None | 00:00 | . 463 | 4.353286 | None | 00:00 | . 464 | 4.340271 | None | 00:00 | . 465 | 4.327243 | None | 00:00 | . 466 | 4.314205 | None | 00:00 | . 467 | 4.301155 | None | 00:00 | . 468 | 4.288094 | None | 00:00 | . 469 | 4.275022 | None | 00:00 | . 470 | 4.261940 | None | 00:00 | . 471 | 4.248846 | None | 00:00 | . 472 | 4.235742 | None | 00:00 | . 473 | 4.222628 | None | 00:00 | . 474 | 4.209503 | None | 00:00 | . 475 | 4.196369 | None | 00:00 | . 476 | 4.183224 | None | 00:00 | . 477 | 4.170070 | None | 00:00 | . 478 | 4.156906 | None | 00:00 | . 479 | 4.143733 | None | 00:00 | . 480 | 4.130549 | None | 00:00 | . 481 | 4.117358 | None | 00:00 | . 482 | 4.104157 | None | 00:00 | . 483 | 4.090947 | None | 00:00 | . 484 | 4.077729 | None | 00:00 | . 485 | 4.064502 | None | 00:00 | . 486 | 4.051266 | None | 00:00 | . 487 | 4.038023 | None | 00:00 | . 488 | 4.024771 | None | 00:00 | . 489 | 4.011510 | None | 00:00 | . 490 | 3.998243 | None | 00:00 | . 491 | 3.984968 | None | 00:00 | . 492 | 3.971685 | None | 00:00 | . 493 | 3.958396 | None | 00:00 | . 494 | 3.945099 | None | 00:00 | . 495 | 3.931795 | None | 00:00 | . 496 | 3.918484 | None | 00:00 | . 497 | 3.905167 | None | 00:00 | . 498 | 3.891843 | None | 00:00 | . 499 | 3.878513 | None | 00:00 | . wores = WithoutResBlock(n=2).cuda().double() learn = Learner(dls, wores, loss_func=mse) . learn.fit(n_epoch=500) . epoch train_loss valid_loss time . 0 | 10.716390 | None | 00:00 | . 1 | 10.710617 | None | 00:00 | . 2 | 10.704815 | None | 00:00 | . 3 | 10.698986 | None | 00:00 | . 4 | 10.693130 | None | 00:00 | . 5 | 10.687246 | None | 00:00 | . 6 | 10.681334 | None | 00:00 | . 7 | 10.675397 | None | 00:00 | . 8 | 10.669433 | None | 00:00 | . 9 | 10.663443 | None | 00:00 | . 10 | 10.657427 | None | 00:00 | . 11 | 10.651385 | None | 00:00 | . 12 | 10.645318 | None | 00:00 | . 13 | 10.639226 | None | 00:00 | . 14 | 10.633109 | None | 00:00 | . 15 | 10.626966 | None | 00:00 | . 16 | 10.620800 | None | 00:00 | . 17 | 10.614610 | None | 00:00 | . 18 | 10.608397 | None | 00:00 | . 19 | 10.602159 | None | 00:00 | . 20 | 10.595899 | None | 00:00 | . 21 | 10.589617 | None | 00:00 | . 22 | 10.583310 | None | 00:00 | . 23 | 10.576982 | None | 00:00 | . 24 | 10.570632 | None | 00:00 | . 25 | 10.564260 | None | 00:00 | . 26 | 10.557868 | None | 00:00 | . 27 | 10.551454 | None | 00:00 | . 28 | 10.545019 | None | 00:00 | . 29 | 10.538563 | None | 00:00 | . 30 | 10.532085 | None | 00:00 | . 31 | 10.525589 | None | 00:00 | . 32 | 10.519073 | None | 00:00 | . 33 | 10.512537 | None | 00:00 | . 34 | 10.505980 | None | 00:00 | . 35 | 10.499407 | None | 00:00 | . 36 | 10.492815 | None | 00:00 | . 37 | 10.486202 | None | 00:00 | . 38 | 10.479572 | None | 00:00 | . 39 | 10.472925 | None | 00:00 | . 40 | 10.466260 | None | 00:00 | . 41 | 10.459577 | None | 00:00 | . 42 | 10.452876 | None | 00:00 | . 43 | 10.446158 | None | 00:00 | . 44 | 10.439425 | None | 00:00 | . 45 | 10.432674 | None | 00:00 | . 46 | 10.425908 | None | 00:00 | . 47 | 10.419126 | None | 00:00 | . 48 | 10.412327 | None | 00:00 | . 49 | 10.405513 | None | 00:00 | . 50 | 10.398685 | None | 00:00 | . 51 | 10.391841 | None | 00:00 | . 52 | 10.384982 | None | 00:00 | . 53 | 10.378109 | None | 00:00 | . 54 | 10.371222 | None | 00:00 | . 55 | 10.364320 | None | 00:00 | . 56 | 10.357405 | None | 00:00 | . 57 | 10.350475 | None | 00:00 | . 58 | 10.343534 | None | 00:00 | . 59 | 10.336578 | None | 00:00 | . 60 | 10.329610 | None | 00:00 | . 61 | 10.322630 | None | 00:00 | . 62 | 10.315636 | None | 00:00 | . 63 | 10.308631 | None | 00:00 | . 64 | 10.301614 | None | 00:00 | . 65 | 10.294585 | None | 00:00 | . 66 | 10.287544 | None | 00:00 | . 67 | 10.280492 | None | 00:00 | . 68 | 10.273429 | None | 00:00 | . 69 | 10.266356 | None | 00:01 | . 70 | 10.259271 | None | 00:00 | . 71 | 10.252176 | None | 00:00 | . 72 | 10.245071 | None | 00:00 | . 73 | 10.237956 | None | 00:01 | . 74 | 10.230832 | None | 00:00 | . 75 | 10.223698 | None | 00:00 | . 76 | 10.216555 | None | 00:00 | . 77 | 10.209401 | None | 00:00 | . 78 | 10.202238 | None | 00:00 | . 79 | 10.195068 | None | 00:00 | . 80 | 10.187889 | None | 00:00 | . 81 | 10.180701 | None | 00:00 | . 82 | 10.173507 | None | 00:00 | . 83 | 10.166304 | None | 00:00 | . 84 | 10.159092 | None | 00:00 | . 85 | 10.151874 | None | 00:00 | . 86 | 10.144648 | None | 00:00 | . 87 | 10.137414 | None | 00:00 | . 88 | 10.130174 | None | 00:00 | . 89 | 10.122928 | None | 00:00 | . 90 | 10.115674 | None | 00:01 | . 91 | 10.108414 | None | 00:00 | . 92 | 10.101149 | None | 00:00 | . 93 | 10.093877 | None | 00:00 | . 94 | 10.086599 | None | 00:00 | . 95 | 10.079317 | None | 00:00 | . 96 | 10.072029 | None | 00:00 | . 97 | 10.064734 | None | 00:00 | . 98 | 10.057436 | None | 00:00 | . 99 | 10.050132 | None | 00:00 | . 100 | 10.042823 | None | 00:00 | . 101 | 10.035511 | None | 00:00 | . 102 | 10.028194 | None | 00:00 | . 103 | 10.020873 | None | 00:00 | . 104 | 10.013547 | None | 00:00 | . 105 | 10.006219 | None | 00:00 | . 106 | 9.998885 | None | 00:00 | . 107 | 9.991549 | None | 00:00 | . 108 | 9.984210 | None | 00:00 | . 109 | 9.976868 | None | 00:00 | . 110 | 9.969522 | None | 00:00 | . 111 | 9.962173 | None | 00:00 | . 112 | 9.954823 | None | 00:00 | . 113 | 9.947470 | None | 00:00 | . 114 | 9.940114 | None | 00:00 | . 115 | 9.932755 | None | 00:00 | . 116 | 9.925395 | None | 00:00 | . 117 | 9.918033 | None | 00:00 | . 118 | 9.910669 | None | 00:00 | . 119 | 9.903304 | None | 00:00 | . 120 | 9.895938 | None | 00:00 | . 121 | 9.888569 | None | 00:00 | . 122 | 9.881199 | None | 00:00 | . 123 | 9.873829 | None | 00:00 | . 124 | 9.866458 | None | 00:00 | . 125 | 9.859087 | None | 00:01 | . 126 | 9.851714 | None | 00:00 | . 127 | 9.844341 | None | 00:00 | . 128 | 9.836967 | None | 00:00 | . 129 | 9.829594 | None | 00:00 | . 130 | 9.822221 | None | 00:00 | . 131 | 9.814847 | None | 00:00 | . 132 | 9.807474 | None | 00:00 | . 133 | 9.800101 | None | 00:00 | . 134 | 9.792729 | None | 00:00 | . 135 | 9.785357 | None | 00:00 | . 136 | 9.777987 | None | 00:00 | . 137 | 9.770616 | None | 00:00 | . 138 | 9.763247 | None | 00:00 | . 139 | 9.755878 | None | 00:00 | . 140 | 9.748511 | None | 00:00 | . 141 | 9.741145 | None | 00:00 | . 142 | 9.733780 | None | 00:00 | . 143 | 9.726418 | None | 00:00 | . 144 | 9.719056 | None | 00:00 | . 145 | 9.711697 | None | 00:00 | . 146 | 9.704339 | None | 00:00 | . 147 | 9.696983 | None | 00:00 | . 148 | 9.689629 | None | 00:00 | . 149 | 9.682277 | None | 00:00 | . 150 | 9.674928 | None | 00:00 | . 151 | 9.667580 | None | 00:00 | . 152 | 9.660234 | None | 00:00 | . 153 | 9.652893 | None | 00:01 | . 154 | 9.645552 | None | 00:00 | . 155 | 9.638215 | None | 00:00 | . 156 | 9.630879 | None | 00:00 | . 157 | 9.623549 | None | 00:00 | . 158 | 9.616220 | None | 00:00 | . 159 | 9.608893 | None | 00:00 | . 160 | 9.601570 | None | 00:00 | . 161 | 9.594252 | None | 00:00 | . 162 | 9.586935 | None | 00:00 | . 163 | 9.579622 | None | 00:00 | . 164 | 9.572312 | None | 00:00 | . 165 | 9.565006 | None | 00:00 | . 166 | 9.557703 | None | 00:00 | . 167 | 9.550404 | None | 00:00 | . 168 | 9.543109 | None | 00:00 | . 169 | 9.535817 | None | 00:00 | . 170 | 9.528530 | None | 00:00 | . 171 | 9.521246 | None | 00:00 | . 172 | 9.513966 | None | 00:00 | . 173 | 9.506689 | None | 00:00 | . 174 | 9.499417 | None | 00:00 | . 175 | 9.492149 | None | 00:00 | . 176 | 9.484885 | None | 00:00 | . 177 | 9.477626 | None | 00:00 | . 178 | 9.470370 | None | 00:01 | . 179 | 9.463119 | None | 00:00 | . 180 | 9.455873 | None | 00:00 | . 181 | 9.448630 | None | 00:00 | . 182 | 9.441393 | None | 00:01 | . 183 | 9.434161 | None | 00:01 | . 184 | 9.426932 | None | 00:00 | . 185 | 9.419709 | None | 00:00 | . 186 | 9.412491 | None | 00:00 | . 187 | 9.405276 | None | 00:00 | . 188 | 9.398066 | None | 00:00 | . 189 | 9.390862 | None | 00:01 | . 190 | 9.383661 | None | 00:00 | . 191 | 9.376467 | None | 00:00 | . 192 | 9.369276 | None | 00:00 | . 193 | 9.362091 | None | 00:00 | . 194 | 9.354911 | None | 00:00 | . 195 | 9.347735 | None | 00:00 | . 196 | 9.340566 | None | 00:00 | . 197 | 9.333401 | None | 00:00 | . 198 | 9.326241 | None | 00:00 | . 199 | 9.319087 | None | 00:00 | . 200 | 9.311937 | None | 00:00 | . 201 | 9.304792 | None | 00:00 | . 202 | 9.297653 | None | 00:00 | . 203 | 9.290519 | None | 00:00 | . 204 | 9.283391 | None | 00:00 | . 205 | 9.276267 | None | 00:00 | . 206 | 9.269149 | None | 00:00 | . 207 | 9.262035 | None | 00:00 | . 208 | 9.254929 | None | 00:00 | . 209 | 9.247827 | None | 00:00 | . 210 | 9.240729 | None | 00:00 | . 211 | 9.233638 | None | 00:00 | . 212 | 9.226552 | None | 00:00 | . 213 | 9.219471 | None | 00:00 | . 214 | 9.212396 | None | 00:00 | . 215 | 9.205326 | None | 00:00 | . 216 | 9.198261 | None | 00:00 | . 217 | 9.191202 | None | 00:00 | . 218 | 9.184149 | None | 00:00 | . 219 | 9.177101 | None | 00:00 | . 220 | 9.170058 | None | 00:00 | . 221 | 9.163021 | None | 00:00 | . 222 | 9.155989 | None | 00:00 | . 223 | 9.148962 | None | 00:00 | . 224 | 9.141942 | None | 00:00 | . 225 | 9.134928 | None | 00:00 | . 226 | 9.127917 | None | 00:00 | . 227 | 9.120914 | None | 00:00 | . 228 | 9.113914 | None | 00:00 | . 229 | 9.106922 | None | 00:00 | . 230 | 9.099935 | None | 00:00 | . 231 | 9.092952 | None | 00:00 | . 232 | 9.085975 | None | 00:00 | . 233 | 9.079004 | None | 00:00 | . 234 | 9.072039 | None | 00:00 | . 235 | 9.065079 | None | 00:00 | . 236 | 9.058124 | None | 00:00 | . 237 | 9.051175 | None | 00:00 | . 238 | 9.044230 | None | 00:00 | . 239 | 9.037292 | None | 00:00 | . 240 | 9.030360 | None | 00:00 | . 241 | 9.023432 | None | 00:00 | . 242 | 9.016510 | None | 00:00 | . 243 | 9.009593 | None | 00:00 | . 244 | 9.002682 | None | 00:00 | . 245 | 8.995776 | None | 00:00 | . 246 | 8.988874 | None | 00:00 | . 247 | 8.981980 | None | 00:00 | . 248 | 8.975090 | None | 00:00 | . 249 | 8.968205 | None | 00:00 | . 250 | 8.961326 | None | 00:00 | . 251 | 8.954452 | None | 00:00 | . 252 | 8.947582 | None | 00:00 | . 253 | 8.940720 | None | 00:00 | . 254 | 8.933861 | None | 00:00 | . 255 | 8.927008 | None | 00:00 | . 256 | 8.920159 | None | 00:00 | . 257 | 8.913317 | None | 00:00 | . 258 | 8.906479 | None | 00:00 | . 259 | 8.899647 | None | 00:00 | . 260 | 8.892819 | None | 00:00 | . 261 | 8.885997 | None | 00:00 | . 262 | 8.879180 | None | 00:00 | . 263 | 8.872368 | None | 00:00 | . 264 | 8.865561 | None | 00:00 | . 265 | 8.858759 | None | 00:00 | . 266 | 8.851961 | None | 00:00 | . 267 | 8.845169 | None | 00:00 | . 268 | 8.838382 | None | 00:01 | . 269 | 8.831599 | None | 00:00 | . 270 | 8.824821 | None | 00:00 | . 271 | 8.818048 | None | 00:00 | . 272 | 8.811280 | None | 00:00 | . 273 | 8.804518 | None | 00:01 | . 274 | 8.797759 | None | 00:00 | . 275 | 8.791005 | None | 00:00 | . 276 | 8.784256 | None | 00:00 | . 277 | 8.777513 | None | 00:00 | . 278 | 8.770773 | None | 00:00 | . 279 | 8.764038 | None | 00:00 | . 280 | 8.757307 | None | 00:00 | . 281 | 8.750581 | None | 00:00 | . 282 | 8.743860 | None | 00:00 | . 283 | 8.737144 | None | 00:00 | . 284 | 8.730431 | None | 00:00 | . 285 | 8.723722 | None | 00:00 | . 286 | 8.717020 | None | 00:00 | . 287 | 8.710320 | None | 00:00 | . 288 | 8.703627 | None | 00:00 | . 289 | 8.696936 | None | 00:00 | . 290 | 8.690249 | None | 00:00 | . 291 | 8.683568 | None | 00:00 | . 292 | 8.676890 | None | 00:00 | . 293 | 8.670217 | None | 00:00 | . 294 | 8.663547 | None | 00:00 | . 295 | 8.656882 | None | 00:00 | . 296 | 8.650222 | None | 00:00 | . 297 | 8.643564 | None | 00:00 | . 298 | 8.636912 | None | 00:00 | . 299 | 8.630262 | None | 00:00 | . 300 | 8.623617 | None | 00:00 | . 301 | 8.616976 | None | 00:00 | . 302 | 8.610338 | None | 00:00 | . 303 | 8.603703 | None | 00:00 | . 304 | 8.597075 | None | 00:00 | . 305 | 8.590448 | None | 00:00 | . 306 | 8.583826 | None | 00:00 | . 307 | 8.577208 | None | 00:00 | . 308 | 8.570592 | None | 00:00 | . 309 | 8.563980 | None | 00:00 | . 310 | 8.557372 | None | 00:00 | . 311 | 8.550768 | None | 00:00 | . 312 | 8.544168 | None | 00:00 | . 313 | 8.537570 | None | 00:00 | . 314 | 8.530975 | None | 00:00 | . 315 | 8.524384 | None | 00:00 | . 316 | 8.517797 | None | 00:00 | . 317 | 8.511214 | None | 00:00 | . 318 | 8.504633 | None | 00:00 | . 319 | 8.498055 | None | 00:00 | . 320 | 8.491481 | None | 00:00 | . 321 | 8.484909 | None | 00:00 | . 322 | 8.478341 | None | 00:00 | . 323 | 8.471775 | None | 00:00 | . 324 | 8.465213 | None | 00:00 | . 325 | 8.458653 | None | 00:00 | . 326 | 8.452097 | None | 00:00 | . 327 | 8.445543 | None | 00:00 | . 328 | 8.438993 | None | 00:00 | . 329 | 8.432446 | None | 00:00 | . 330 | 8.425900 | None | 00:00 | . 331 | 8.419358 | None | 00:00 | . 332 | 8.412819 | None | 00:00 | . 333 | 8.406281 | None | 00:00 | . 334 | 8.399748 | None | 00:00 | . 335 | 8.393216 | None | 00:00 | . 336 | 8.386685 | None | 00:00 | . 337 | 8.380159 | None | 00:00 | . 338 | 8.373635 | None | 00:00 | . 339 | 8.367112 | None | 00:00 | . 340 | 8.360593 | None | 00:00 | . 341 | 8.354074 | None | 00:00 | . 342 | 8.347560 | None | 00:00 | . 343 | 8.341046 | None | 00:00 | . 344 | 8.334536 | None | 00:00 | . 345 | 8.328026 | None | 00:00 | . 346 | 8.321519 | None | 00:00 | . 347 | 8.315015 | None | 00:00 | . 348 | 8.308511 | None | 00:00 | . 349 | 8.302010 | None | 00:00 | . 350 | 8.295511 | None | 00:00 | . 351 | 8.289015 | None | 00:00 | . 352 | 8.282518 | None | 00:00 | . 353 | 8.276025 | None | 00:00 | . 354 | 8.269533 | None | 00:00 | . 355 | 8.263043 | None | 00:00 | . 356 | 8.256555 | None | 00:00 | . 357 | 8.250067 | None | 00:00 | . 358 | 8.243582 | None | 00:00 | . 359 | 8.237098 | None | 00:00 | . 360 | 8.230615 | None | 00:00 | . 361 | 8.224133 | None | 00:00 | . 362 | 8.217654 | None | 00:00 | . 363 | 8.211176 | None | 00:00 | . 364 | 8.204698 | None | 00:00 | . 365 | 8.198221 | None | 00:00 | . 366 | 8.191747 | None | 00:00 | . 367 | 8.185272 | None | 00:00 | . 368 | 8.178799 | None | 00:00 | . 369 | 8.172327 | None | 00:00 | . 370 | 8.165857 | None | 00:00 | . 371 | 8.159388 | None | 00:00 | . 372 | 8.152918 | None | 00:00 | . 373 | 8.146449 | None | 00:00 | . 374 | 8.139981 | None | 00:00 | . 375 | 8.133514 | None | 00:00 | . 376 | 8.127048 | None | 00:01 | . 377 | 8.120584 | None | 00:00 | . 378 | 8.114120 | None | 00:00 | . 379 | 8.107655 | None | 00:00 | . 380 | 8.101191 | None | 00:00 | . 381 | 8.094728 | None | 00:00 | . 382 | 8.088265 | None | 00:00 | . 383 | 8.081802 | None | 00:01 | . 384 | 8.075340 | None | 00:01 | . 385 | 8.068878 | None | 00:00 | . 386 | 8.062416 | None | 00:00 | . 387 | 8.055954 | None | 00:00 | . 388 | 8.049493 | None | 00:00 | . 389 | 8.043031 | None | 00:00 | . 390 | 8.036570 | None | 00:00 | . 391 | 8.030107 | None | 00:00 | . 392 | 8.023646 | None | 00:00 | . 393 | 8.017185 | None | 00:00 | . 394 | 8.010722 | None | 00:00 | . 395 | 8.004261 | None | 00:00 | . 396 | 7.997798 | None | 00:00 | . 397 | 7.991335 | None | 00:00 | . 398 | 7.984872 | None | 00:00 | . 399 | 7.978409 | None | 00:00 | . 400 | 7.971944 | None | 00:00 | . 401 | 7.965479 | None | 00:00 | . 402 | 7.959015 | None | 00:00 | . 403 | 7.952549 | None | 00:00 | . 404 | 7.946082 | None | 00:00 | . 405 | 7.939614 | None | 00:00 | . 406 | 7.933146 | None | 00:00 | . 407 | 7.926677 | None | 00:00 | . 408 | 7.920208 | None | 00:00 | . 409 | 7.913736 | None | 00:00 | . 410 | 7.907265 | None | 00:00 | . 411 | 7.900792 | None | 00:00 | . 412 | 7.894318 | None | 00:00 | . 413 | 7.887843 | None | 00:00 | . 414 | 7.881367 | None | 00:00 | . 415 | 7.874889 | None | 00:00 | . 416 | 7.868410 | None | 00:00 | . 417 | 7.861930 | None | 00:00 | . 418 | 7.855448 | None | 00:00 | . 419 | 7.848966 | None | 00:00 | . 420 | 7.842480 | None | 00:00 | . 421 | 7.835995 | None | 00:00 | . 422 | 7.829507 | None | 00:00 | . 423 | 7.823017 | None | 00:00 | . 424 | 7.816526 | None | 00:00 | . 425 | 7.810033 | None | 00:00 | . 426 | 7.803538 | None | 00:00 | . 427 | 7.797041 | None | 00:00 | . 428 | 7.790543 | None | 00:00 | . 429 | 7.784042 | None | 00:00 | . 430 | 7.777539 | None | 00:00 | . 431 | 7.771034 | None | 00:01 | . 432 | 7.764527 | None | 00:00 | . 433 | 7.758018 | None | 00:00 | . 434 | 7.751507 | None | 00:00 | . 435 | 7.744993 | None | 00:00 | . 436 | 7.738477 | None | 00:00 | . 437 | 7.731957 | None | 00:00 | . 438 | 7.725437 | None | 00:00 | . 439 | 7.718913 | None | 00:00 | . 440 | 7.712387 | None | 00:00 | . 441 | 7.705857 | None | 00:00 | . 442 | 7.699326 | None | 00:00 | . 443 | 7.692792 | None | 00:00 | . 444 | 7.686255 | None | 00:00 | . 445 | 7.679714 | None | 00:00 | . 446 | 7.673171 | None | 00:00 | . 447 | 7.666625 | None | 00:00 | . 448 | 7.660076 | None | 00:00 | . 449 | 7.653524 | None | 00:00 | . 450 | 7.646968 | None | 00:00 | . 451 | 7.640410 | None | 00:00 | . 452 | 7.633849 | None | 00:00 | . 453 | 7.627283 | None | 00:00 | . 454 | 7.620715 | None | 00:00 | . 455 | 7.614143 | None | 00:00 | . 456 | 7.607568 | None | 00:00 | . 457 | 7.600989 | None | 00:00 | . 458 | 7.594406 | None | 00:00 | . 459 | 7.587820 | None | 00:00 | . 460 | 7.581230 | None | 00:00 | . 461 | 7.574636 | None | 00:00 | . 462 | 7.568038 | None | 00:00 | . 463 | 7.561437 | None | 00:00 | . 464 | 7.554831 | None | 00:00 | . 465 | 7.548222 | None | 00:00 | . 466 | 7.541608 | None | 00:01 | . 467 | 7.534990 | None | 00:00 | . 468 | 7.528368 | None | 00:00 | . 469 | 7.521742 | None | 00:00 | . 470 | 7.515112 | None | 00:01 | . 471 | 7.508477 | None | 00:01 | . 472 | 7.501838 | None | 00:00 | . 473 | 7.495194 | None | 00:00 | . 474 | 7.488545 | None | 00:02 | . 475 | 7.481893 | None | 00:00 | . 476 | 7.475235 | None | 00:00 | . 477 | 7.468572 | None | 00:00 | . 478 | 7.461905 | None | 00:00 | . 479 | 7.455233 | None | 00:00 | . 480 | 7.448556 | None | 00:00 | . 481 | 7.441875 | None | 00:00 | . 482 | 7.435188 | None | 00:00 | . 483 | 7.428496 | None | 00:00 | . 484 | 7.421799 | None | 00:00 | . 485 | 7.415096 | None | 00:00 | . 486 | 7.408389 | None | 00:01 | . 487 | 7.401675 | None | 00:01 | . 488 | 7.394957 | None | 00:00 | . 489 | 7.388233 | None | 00:00 | . 490 | 7.381504 | None | 00:00 | . 491 | 7.374768 | None | 00:00 | . 492 | 7.368027 | None | 00:00 | . 493 | 7.361281 | None | 00:00 | . 494 | 7.354528 | None | 00:00 | . 495 | 7.347770 | None | 00:00 | . 496 | 7.341006 | None | 00:00 | . 497 | 7.334236 | None | 00:00 | . 498 | 7.327460 | None | 00:00 | . 499 | 7.320678 | None | 00:00 | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/2022/08/28/residual-learning.html",
            "relUrl": "/deeplearning/math/fastai/2022/08/28/residual-learning.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "AEDA ( An Easier Data Augmentation Technique for Text Classification )",
            "content": "Paper resources . paper | code | . Objective . This paper proposes a new data augmentation technique for text classification task. | It also compares the performance of this augmentation technique with EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks and concludes that their method is simpler and produces better results. | In this experiment we will try to implement this data augmentation using fastai on a text classification task. | . Why do we need augmentations? . To have better generalizability, we need more and more comprehensive datasets but collection of these datasets and labelling is a laborious task so augmentation becomes an attractive method to introduce more examples for model to consume. | . What are the different kind of augmentations used in NLP? . For improving machine translation task, researchers have tried substituting common words with rare words thus providing more context for rare words. | Some researchers have tried replacing words with their synonyms for tweet classification. | Randomly swap two words in a sentence. | Randomly delete a word in the sentence and many more. | . What is the novel idea presented in the paper? . AEDA method proposes randomly inserting some punctuation marks as an augmentation to introduce noise. The authors report improvement performance in text classification tasks. . Can you share an example of how this augmentation would work? . Original Text: . Appropriate given recent events . . Augmented Text: . Appropriate given ; recent events . Appropriate ? given : recent events . Appropriate given , recent events . How many punctuation marks are inserted? . Between 1 to n/3 where n represents the length of the sentence. | . Why one-third of the sentence length? . The authors mention that they want to increase the complexity of the sentence but doesn&#39;t want to add too many punctuation marks which would interfere with the semantic meaning of the sentence. . At which positions should we insert these punctuation marks? . The authors inserted them at random positions in the sentence. | . What are the different punctuation marks used? . . ; ? : ! , . Why does AEDA work better compared to EDA? . EDA proposes synonym replacement, random replacement, random insertion and random deletion. These modifications could change the semantic meaning of the text. | Whereas AEDA just introduces punctuation marks which would only introduce noise and would not mess the semantic meaning or the word ordering. | . Implementation . We would be using fastai to implement this augmentation. | The authors have released the code as well which we would using. | . Dataset . We will be using this dataset used in this challenge where the goal is to predict the subreddit of a subreddit post based on their title and their description. This is an example of text categorization / text classification task | . Load libraries . import pandas as pd import numpy as np from pathlib import Path from tqdm import tqdm from fastai.text.all import * SEED = 41 . Define paths and constants . BASE_DIR = Path(&#39;~/data/dl_nlp&#39;) RAW_DATA_PATH = BASE_DIR / &#39;data&#39; OUTPUT_DIR = Path(&#39;~/data/dl_nlp/outputs&#39;) PUNCTUATIONS = [&#39;.&#39;, &#39;,&#39;, &#39;!&#39;, &#39;?&#39;, &#39;;&#39;, &#39;:&#39;] PUNC_RATIO = 0.3 . Load dataset . train = pd.read_csv(RAW_DATA_PATH / &#39;train.csv&#39;) train.head() . . Text column represents title as well as description. | Subreddit column represents our label. | . Class Distribution . train.subreddit.value_counts(normalize=True) . . We have multiple categories that our model needs to get right. | Most of the categories have similar percentage of data points in the dataset, with only SubredditSimulator category having less training examples. | . Splitter . splits = RandomSplitter(seed=41)(train) . Create a splitting strategy. | Here we plan to split our training dataframe randomly into training ( 80% ) and validation ( 20% ) datasets. | . Tokenize the training dataset . df_tok, cnt = tokenize_df(train.iloc[splits[0]], text_cols=&#39;text&#39;) . Fast.ai provides a method to tokenize our dataset. | Here we only passing our training examples as the corpus for tokenizer to create vocabulary. | We could pass in different types of tokenizers here but by default it works with WordTokenizer. | . df_tok . . Here we could see that it has split our text string into tokens and created an additional column called text_length describing the length. | It has also added some library specific tokens like xxbos, xxmaj etc. xxbos represents beginning of the sentence token. For more details please refer to fast.ai | . cnt . . Here is a snapshot of the vocabulary constructed by the tokenize_df method. | . Using fast.ai Pipeline to construct Dataset . text_pipe = Pipeline([attrgetter(&#39;text&#39;), Tokenizer.from_df(0), Numericalize(vocab=list(cnt.keys()))]) lbl_pipe = Pipeline([attrgetter(&#39;subreddit&#39;), Categorize()]) lbl_pipe.setup(train.subreddit) dsets = Datasets(train, [text_pipe, lbl_pipe], splits=splits, dl_type=SortedDL) . Here we use Pipeline provided by fast.ai to put together different transforms we want to run on our dataframe. | text_pipe represents the Pipeline that we would like to run on our text column in the dataframe. | lbl_pipe represents the Pipeline that we would like to run on our subreddit column in the dataframe. | Numericalize transform takes in our vocabulary and converts the tokens to ids. | Categorize transforms converts our labels to categories. | Tokenizer.from_df transform tokenizes the text stored in our dataframe. | . AEDA data augmentation as fast.ai transform . np.random.seed(0) PUNCTUATIONS = [&#39;.&#39;, &#39;,&#39;, &#39;!&#39;, &#39;?&#39;, &#39;;&#39;, &#39;:&#39;] PUNC_RATIO = 0.3 class InsertPunctuation(Transform): split_idx = 0 def __init__(self, o2i, punc_ratio=PUNC_RATIO): self.o2i = o2i self.punc_ratio = punc_ratio def encodes(self, words:TensorText): new_line = [] q = random.randint(1, int(self.punc_ratio * len(words) + 1)) qs = random.sample(range(0, len(words)), q) for j, word in enumerate(words): if j in qs: new_line.append(self.o2i[PUNCTUATIONS[random.randint(0, len(PUNCTUATIONS)-1)]]) new_line.append(int(word)) else: new_line.append(int(word)) return TensorText(new_line) . We have taken the implementation from the github shared by the authors and created a fast.ai tranform that would take in the PUNC_RATIO and o2i as parameters and inserts punctuations at random positions in the sentence. | PUNC_RATIO by default takes a value of 0.3 which represents the 1/3rd of the sentence length mentioned in the paper. | o2i is mapping between token to token_id. | . Construct dataloaders . seq_len = 72 dls_kwargs = { &#39;after_item&#39; : InsertPunctuation(dsets.o2i), &#39;before_batch&#39;: Pad_Chunk(seq_len=seq_len) } dls = dsets.dataloaders(bs=32, seq_len=seq_len, **dls_kwargs) . When creating fast.ai dataloders we could perform operations on some of the events emitted. | Here we have made use of two such events, after_item callback is used to run our augmentation and add punctuation marks. | before_batch callback is used to make sure that we have paddded the tokens to make sure they are of same size before we collate them to form a batch. | . dls.show_batch(max_n=3) . . dls.show_batch gives a glimpse of the batch | . Using the classic TextCNN model introduced by Yoon Kim, paper . class TextCNN(Module): def __init__(self, n_embed, embed_dim, num_filters, filter_sizes, num_classes, dropout=0.5, pad_idx=1): store_attr(&#39;n_embed,embed_dim&#39;) self.embed = nn.Embedding(num_embeddings=n_embed, embedding_dim=embed_dim, padding_idx=1 ) self.convs = nn.ModuleList([ nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embed_dim) ) for k in filter_sizes ]) self.dropout = nn.Dropout(dropout) self.relu = nn.ReLU() self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes) def _conv_and_pool(self, x, conv): x = self.relu(conv(x)).squeeze(3) x = F.max_pool1d(x, x.size(2)).squeeze(2) return x def forward(self, x): out = self.embed(x) out = out.unsqueeze(1) out = torch.cat([self._conv_and_pool(out, conv) for conv in self.convs], 1) out = self.dropout(out) out = self.fc(out) return out . vocab = dls.train_ds.vocab num_classes = get_c(dls) model = TextCNN(len(vocab[0]), embed_dim=300, num_filters=100, filter_sizes=[1, 2, 3], num_classes=num_classes, ) . Define learner . learn = Learner(dls, model, metrics=[accuracy, F1Score(average=&#39;weighted&#39;)]) . Using F1 score weighted metric for multi-class classification. | . learn.fit_one_cycle(n_epoch=25, lr_max=3e-4, cbs=EarlyStoppingCallback(patience=3)) . . We are getting a F1( weighted ) score of 0.869 without using any pre-trained embeddings. | . References . AEDA: An Easier Data Augmentation Technique for Text Classification | AEDA code | EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks | Fast.AI | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/nlp/2022/01/31/aeda-text-augmentation.html",
            "relUrl": "/deeplearning/math/fastai/nlp/2022/01/31/aeda-text-augmentation.html",
            "date": " • Jan 31, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Temporal Convolution Networks",
            "content": "Objective . sequence modelling had become synonymous with recurrent networks. | This paper shows that convolutional networks can outperform recurrent networks on some of the tasks. | paper concludes that common association between sequence modelling and recurrent networks should be reconsidered. | . Temporal Convolution Networks? . A new general architecture for convolutional sequence prediction. | This new general architecture is referred to as Temporal Convolutional Networks abbreviated as TCN. | Convolutions in this architecture are causal which means that there is no information leakage. | Architecture can take in a sequence of arbitrary length and map it to an output sequence of the same length, just like RNNs. ( But tcn achieves this function not through seq2seq but simply using convolutional layers. ) | Also this paper highlights how we could combine deep networks ( with residual structures ) and dilated convolutions could be used to build long term dependencies. ( ability of an model to look back in past to make future predictions ) | . What is a sequence modelling task? . Taken directly from paper . Before defining the network structure, we highlight the nature of the sequence modeling task. Suppose that we are given an input sequence $x_0$, . . . , $x_T$ , and wish to predict some corresponding outputs $y_0$, . . . , $y_T$ at each time. The key constraint is that to predict the output $y_t$ for some time t, we are constrained to only use those inputs that have been previously observed:$x_0$, . . . , $x_t$. Formally, a sequence modeling network is any function f : $X_{T +1}$ → $Y_{T +1}$ that produces the mapping . $y_0$, . . . , $y_T$ = f($x_0$, . . . , $x_T$ ) (1) . if it satisfies the causal constraint that $y_t$ depends only on $x_0$, . . . , $x_t$ and not on any “future” inputs $x_{t+1}$, . . . , $x_T$. The goal of learning in the sequence modeling setting is to find a network f that minimizes some expected loss between the actual outputs and the predictions. . L($y_0$, . . . , $y_T$ , f($x_0$, . . . , $x_T$)), where the sequences and outputs are drawn according to some distribution. . What is a 1D convolution? . Before we jump into the paper we must understand what is a 1D convolution since it is used in the causal convolutional layer in TCN . 1D Convolution takes in a 3D tensor as input and outputs a 3D tensor as output. | Shape of the input tensor in TCN would have following dimension ( batch_size, input_length, input_size ) and the output tensor has shape ( batch_size, input_length, output_size ) | Each layer in TCN has same input and output length so only the third dimension would change. | . . Image courtesy . In the above figure we can notice the follwing . to compute a single output we need to look at 3 consecutive values of the input sequence, it is because we are using a kernel of size 3 here. | to maintain that input and output sequences be of the same size we have to pad the input sequence with zeros on both sides. | 1d convolution is a special case of 2d convolution | . . Image courtesy . How is 1d convolution a special case of 2d convolution? . In both time series and NLP, data is laid out in a similar manner, in the figure above we have embedded the words I like this movie very much ! into a 7 x 5 embedding matrix and then we use 1d convolution on this 2D matrix. . 1d convolution is a special case of 2d convolution where kernel size of the 1d convolution is it&#39;s height. The width of the kernel is defined by the embedding size, which is 5 here and it is fixed. So it means that we can only slide vertically and not horizontally which makes it 1D convolution. . What is causal convolution? . Causality means that an element in the output sequence can only depend on elements that precede it in the input sequence. | In order to ensure that an output tensor has the same length as the input tensor, we need to do zero padding. | If we only pad the left side of the input tensor with zeros, then causal convolution is guaranteed. | $x^{&#39;}_4$ in the figure below is generated by combining $x_2$, $x_3$, $x_4$ which ensures no leakage of information. | . . This operation generates $x^{&#39;}_5$ and $x^{&#39;}_6$ which are extraneous and should be removed before passing the output to the next layer. We have to take care of it in the implementation. . How many zeros would be required to make sure that the output would be of same length as input? (kernel_size - 1) . How it all fits together? . TCN has two basic principles: . input and output length of the sequences remain same. | there can be no leakage from the past. | . To achieve the first point TCN makes use of 1D FCN ( Fully Convolutional Network ) and to achieve the second point TCN makes use of causal convolutions. . Disadvantages of the above architecture . To model long term dependencies, we need a very deep network or very large convolution kernels, neither of which turned out to be particularly feasible in the experiments. | . Dilated Convolutions . A desirable quality of a the model is that the value of a particular entry in the output depends on all previous entries in the input. . | This is achieved when the size of the receptive field is equal to the length of the input. . | We could expand our receptive field when we stack multiple layers together. In the figure below we can see that by stacking two layers with kernel_size 3, we get a receptive field size of 5. . | . . In general, the receptive field r of a 1D convolutional network with n layers and kernel_size k is . $r = 1 + n * ( k - 1 )$ . To know how many layers are needed for full coverage, we can set the receptive field size to input_length l and solve for the number of layers n (non-integer values need to be rounded): . $ lceil frac{(l-1)}{(k-1)} rceil$ . This means that, with a fixed kernel_size, the number of layers required for complete coverage would be linear in input length. This will cause the network to become very deep and very fast, resulting in models with a large number of parameters that take longer to train. . How could we solve this issue? . One way to increase the size of the receptive field while keeping the number of layers relatively small is to introduce the concept of dilation. . Dilation in the context of convolutional layers refers to the distance between elements of the input sequence that are used to compute one entry of the output sequence. Therefore, a traditional convolutional layer can be viewed as a layer dilated by 1, because the input elements involved in calculating output value are adjacent. . The image below shows an example of how the receptive field grows when we introduce dilation. The right side image uses a dilation rate r 1 in the first layer with kernel_size 3 which is how a traditional conv layer would work although in the next layer we use r=2 which makes sure that we combine input elements that are 2 elements apart when producing output for the next layer and so on. . . To overcome the problem of number of layers required for covering the entire input length we must progressively increase the dilation rate over multiple layers. . This problem can be solved by exponentially increasing the value of d as we move up in the layer. To do this, we choose a constant dilation_base integer b that will allow us to calculate the dilation d for a particular layer based on the number of layers i under it, i.e. $d = b^i$. . The figure below shows a network with an input_length of 10, a kernel_size of 3, and a dilation_base of 2, which would result in a complete coverage of the 3 dilated convolutional layers. . . Here we can see that the all input values are used to produce the last value in the output layer. With the above mentioned setup we could have an input of length 15 while maintaining the full coverage. . How did we calculate that the receptive width is 15? . When a layer is added to the architecture the receptive field is increased by $d*(k-1)$ | So if we have n layers with kernel_size k and dilation base rate as b then receptive width is calculated as | . $w=1+(k-1) frac{b^n-1}{b-1}$ . but depending on values of b and k the architecture could have many holes in it. . What does that mean? . . Here we can see not all inputs are used to compute the last value of the output, even though w is greater than the input size. To fix this we would have to either increase the kernel size or decrease the dilation rate from 3 to 2. In general we must ensure that kernel_size is atleast equal to dilation rate to avoid such cases. . How many layers would be required for full coverage? . Given a kernel size k, a dilation base b where k ≥ b, and an input length l, in order to achieve full coverage following condition must be satisfied . $1+(k-1) frac{b^n-1}{b-1} geq l$, then . $n= lceil log_b( frac{(l-1)*(b-1)}{k-1}+1) rceil$ . Now number of layers is lograthmic in input layer length l which is what we wanted. This is a significant improvement that can be achieved without sacrificing receptive field coverage. . Now, the only thing that needs to be specified is the number of zero-padded items required for each layer. Assuming that the dilation expansion base is b, the kernel size is k, and there are i layers below the current layer, the number p of zero-padding items required by the current layer is calculated as follows: . $p=b^i*(k-1)$ . Temporal Residual Block . Now let&#39;s discuss the basic building blocks of TCN network. . . Residual links have proven to be an effective way to train deep networks, which allow the network to pass information in a cross-layer manner. | This paper constructs a residual block to replace one layer of convolution. As shown in the figure above, a residual block contains two layers of convolution and nonlinear mapping, and WeightNorm and Dropout are added to each layer to regularize the network. | . . Each hidden layer has the same length as the input layer, and is padded with zeros to ensure subsequent layers have the same length. . | For the output at time t, the causal convolution (convolution with causal constraints) uses the input at time t and the previous layer at an earlier time (see the blue line connection at the bottom of the figure above). . | Causal convolution is not a new idea, but the paper incorporates very deep networks to allow for long-term efficient histories. . | . Residual Link . . Residual blocks (originally from ResNet) have repeatedly shown to benefit very deep networks. | Since the receptive field of a TCN depends on the network depth n as well as the convolution kernel size k and dilation factor d, it becomes very important to stabilize deeper and larger TCNs. | Predictions may depend on long historical values and high-dimensional input sequences. e.g An input sequence of size $2^{12}$ may require a network of up to 12 layers. | In standard ResNet, the input is directly added to the output of the residual function, while in TCN the input and output can have different widths. To account for the difference in input-output width, an additional 1x1 convolution is used to ensure that element-wise addition receives tensors of the same shape. | . . Conclusion . The innovation in the TCN model is to sort out how to use causal and dilated convolutions to solve the sequence modelling task. | Causal and Dilated convolutions have already been proposed earlier but this paper highlights how they could be combined together for sequence modelling tasks | . Advantages . Parallelism. When given a sentence, TCN can process the sentence in parallel without the need for sequential processing like RNN. . | Flexible receptive field. The size of the receptive field of TCN is determined by the number of layers, the size of the convolution kernel, and the expansion coefficient. It can be flexibly customized according to different characteristics of different tasks. . | Stable gradient. RNN often has the problems of vanishing gradients and gradient explosion, which are mainly caused by sharing parameters in different time periods. Like traditional convolutional neural networks, TCN does not have the problem of gradient disappearance and explosion. . | Lower memory requirements. When RNN is used, it needs to save the information of each step, which will occupy a lot of memory. The convolution kernel of TCN is shared in one layer, and hence lower memory usage. . | . Disadvantages . TCN may not be so adaptable in transfer learning. This is because the amount of historical information required for model predictions may be different in different domains. Therefore, when migrating a model from a problem that requires less memory information to a problem that requires longer memory, TCN may perform poorly because its receptive field is not large enough. . | The TCN described in the paper is also a one-way structure. In tasks such as speech recognition and speech synthesis, the pure one-way structure is quite useful. However, most of the texts use a bidirectional structure. Of course, it is easy to expand the TCN into a bidirectional structure. Instead of using causal convolution, the traditional convolution structure can be used. . | TCN is a variant of convolutional neural network after all. Although the receptive field can be expanded by using dilated convolution, it is still limited. Compared with Transformer, it is still poor in capturing relevant information of any length. The application of TCN to text remains to be tested. . | . Tips for implementation . Next we would highlight things to keep in mind if you plan to implement the paper . After the convolution, the size of the output data after the convolution is greater than the size of the input data | This is caused owing to padding both sides, so we chomp off extra padded 0s from right side to get the desired data values. | . We have taken this (https://github.com/locuslab/TCN/blob/2221de3323/TCN/tcn.py) implementation of TCN and implemented in fast.ai and tsai to demonstrate how TCN could be used for sequence modelling. . How to prepare dataset? . from tsai.all import * computer_setup() . . We are going to select appliances energy dataset recently released by Monash, UEA &amp; UCR Time Series Extrinsic Regression Repository (2020) . dsid = &#39;AppliancesEnergy&#39; X, y, splits = get_regression_data(dsid, split_data=False) X.shape, y.shape, y[:10] . . check_data(X, y, splits) . . tfms = [None, [TSRegression()]] batch_tfms = TSStandardize(by_sample=True, by_var=True) dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=128) dls.one_batch() . . dls.show_batch() . . Model implementation . from fastai.torch_basics import * from fastai.tabular.core import * from torch.nn.utils import weight_norm . class Chomp1d(Module): def __init__(self, chomp_size): store_attr() def forward(self, x): return x[:, :, :-self.chomp_size].contiguous() def get_conv_block(n_inputs, n_outputs, kernel_size, stride, padding, dilation, dropout): conv = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation )) chomp = Chomp1d(padding) relu = nn.ReLU() drop = nn.Dropout(dropout) return nn.Sequential(*(conv, chomp, relu, drop )) class TemporalBlock(Module): def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.5): store_attr() self.in_conv_blk = get_conv_block(n_inputs, n_outputs, kernel_size, stride, padding, dilation, dropout ) self.out_conv_blk = get_conv_block(n_outputs, n_outputs, kernel_size, stride, padding, dilation, dropout ) self.net = nn.Sequential(*(self.in_conv_blk, self.out_conv_blk)) self.downsample_conv = nn.Conv1d(n_inputs, n_outputs, kernel_size=1) if n_inputs != n_outputs else None self.relu = nn.ReLU() self.init_weights() def init_weights(self): # 0 index represents the convolutional layer self.in_conv_blk[0].weight.data.normal_(0, 0.01) self.out_conv_blk[0].weight.data.normal_(0, 0.01) if self.downsample_conv is not None: self.downsample_conv.weight.data.normal_(0, 0.01) def forward(self, x): out = self.net(x) res = x if self.downsample_conv is None else self.downsample_conv(x) return self.relu(out + res) class TemporalConvNet(Module): def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2): layers = [] num_levels = len(num_channels) for i in range(num_levels): dilation_size = 2 ** i in_channels = num_inputs if i == 0 else num_channels[i-1] out_channels = num_channels[i] layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=(kernel_size-1) * dilation_size, dropout=dropout ) ] self.network = nn.Sequential(*layers) def forward(self, x): return self.network(x) class TCN(Module): def __init__(self, input_size, output_size, num_channels, kernel_size, dropout): self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout) self.linear = nn.Linear(num_channels[-1], output_size) self.init_weights() def init_weights(self): self.linear.weight.data.normal_(0, 0.01) def forward(self, x): y1 = self.tcn(x) return self.linear(y1[:, :, -1]) . model = TCN(input_size=24, output_size=1, num_channels=[24, 32, 64], kernel_size=2, dropout=0.2 ) learn = Learner(dls, model, metrics=[mae, rmse], cbs=ShowGraph()) learn.lr_find() . . model = TCN(input_size=24, output_size=1, num_channels=[24, 32, 64], kernel_size=2, dropout=0.2 ) learn = Learner(dls, model, metrics=[mae, rmse], cbs=ShowGraph()) learn.fit_one_cycle(100, 8e-4) . . References . An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling | Fast.AI | tsai | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/tsai/sequencemodelling/2021/12/22/temporal-convolutional-networks.html",
            "relUrl": "/deeplearning/math/fastai/tsai/sequencemodelling/2021/12/22/temporal-convolutional-networks.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Do we need downsampling?",
            "content": "Objective . Deep CNN architecutures are made up of many components like conv layer, activation function, pooling layers, batch-norm layer etc.All of them are designed for specific reasons and to better understand the effect of these components it is important to play with them. For example, we include layers like pooling, strided convolution etc to reduce the size of the input. . If we look at the architecture of VGG below, we see that lot of max_pooling layers are used. The idea is to increase the receptive field and decrease the number of parameters by reducing the size of the input. But then a question arises, do we really need to downsample? Luckily I came across this repository where this specific question is addressed and the author has tried to replace downsampling layers with dilated convolutions or large kernels with appropriate padding to address this issue. The purpose of this post is to implement and validate these ideas by performing the above mentioned experiments on CIFAR-10 dataset using fastai. The intention is to show how easy it is for us to experiment using fastai. . . Libraries . let&#39;s start by installing fastai2 and keras . !pip install fastai2 keras &gt; /dev/null . from fastai2.vision.all import * from keras.datasets import cifar10 . Dataset . We are going to train our network on CIFAR-10 dataset. CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. . (x_train,y_train),(x_test,y_test) = cifar10.load_data() . Datasets API . This is where fastai comes in with its flexible api to form a dataset that is easily consumable by the models. We are passing a list of pairs of (image, label) to the pipeline. No data augmentation is applied. To validate our ideas we will keep a separate holdout set. If you want to understand more about Datasets api please read official docs . items = np.array(list(zip(x_train, y_train.ravel()))) # 80-20 percent split splits = RandomSplitter(seed=42)(items) tfms = [[lambda x: x[0], PILImage.create], [lambda x: x[1], Categorize()]] dsets = Datasets(items, tfms, splits=splits) dls = dsets.dataloaders(bs=64, after_item=[ToTensor(), IntToFloatTensor()]) . dls.show_batch(figsize=(4, 4)) . . VGG ( 4 layer ) network . Let&#39;s try to train a 4 layer VGG based network with downsampling and see the performance on CIFAR-10. . class VGG_4(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG_4, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=16, out_channels=24, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=24, out_channels=32, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=48, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . learn = Learner(dls, VGG_4(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . Let&#39;s train it for 30 epochs . learn.fit_one_cycle(30, 1e-2) . . learner.summary() . . VGG with dilated convolution . Idea is to progressively increase the dilation from 1, 2, 4, 8 to increase the receptive field of the network. . class VGG4_Dilation(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG4_Dilation, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3), dilation=1), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.Conv2d(in_channels=16, out_channels=24, padding=(1, 1), kernel_size=(3, 3), dilation=2), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.Conv2d(in_channels=24, out_channels=32, padding=(1, 1), kernel_size=(3, 3), dilation=4), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(in_channels=32, out_channels=48, padding=(1, 1), kernel_size=(3, 3), dilation=8), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . In the head of the model instead of using a fully connected layer we are using AdaptiveAvgPool2d with 1x1 convolution layer, it is because researchers have observed that using AdaptiveAvgPool2d with 1x1 convolution layer decreases the total number of parameters without taking a hit on the performance. . learn = Learner(dls, VGG4_Dilation(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . learn.fit_one_cycle(30, 1e-2) . . learn.summary() . . Note: There is no change in number of trainable in params in vanilla vgg 4 layer model with dowsampling and vgg 4 layer model with dilation. . VGG with large kernels . We plan to progressively increase the size of the kernels from 3 to 9. Increasing the kernel size would enable us to increase the receptive field of the network but we have to make sure that we use adequate padding so as to not shrink our input. . class VGG4_large_filter(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG4_large_filter, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.Conv2d(in_channels=16, out_channels=24, padding=(2, 2), kernel_size=(5, 5)), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.Conv2d(in_channels=24, out_channels=32, padding=(3, 3), kernel_size=(7, 7)), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(in_channels=32, out_channels=48, padding=(4, 4), kernel_size=(9, 9)), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . learn = Learner(dls, VGG4_large_filter(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . learn.fit_one_cycle(30, 5e-3) . . learn.summary() . . Even though there is an improvement in terms of loss but number of trainable parameters have jumped from 25,474 to 172, 930 . Conclusion . If we look at the performance of 4 layer VGG network with downsampling and compare it with dilated and large kernel size, we observe that their is an increase in the performance. | Using a large kernel will improve performance but at the cost of increased number of trainable parameters. | Using dilation would improve performance without increasing the number of trainable parameters | . Next steps . To make it generalizable, we would have to test this idea on other architectures e.g. (resnet18) and see if it increases performance or not. | Also it would be interesting to see what kind of features will our models learn if we use dilation. | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/fastai/cnn/2020/05/02/Do-we-need-downsampling.html",
            "relUrl": "/deeplearning/fastai/cnn/2020/05/02/Do-we-need-downsampling.html",
            "date": " • May 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://numb3r33.github.io/experiments/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://numb3r33.github.io/experiments/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}