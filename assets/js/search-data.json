{
  
    
        "post0": {
            "title": "Understanding and Preventing Collapse in Self-Supervised Learning A Deep Dive into BYOL",
            "content": "Introduction . Why do machine learning models typically need labeled data to learn? The core challenge in self-supervised learning is to extract meaningful representations without explicit labels. Instead of relying on labeled datasets, can we learn representations simply by understanding relationships between examples? . Imagine teaching someone to distinguish between cats and dogs, but instead of providing explicit labels, you only tell them whether two animals belong to the same or different categories. Could they eventually learn the distinction? This thought experiment highlights a key insight: understanding relationships can sometimes be sufficient for learning meaningful representations. . BYOL (Bootstrap Your Own Latent) is a self-supervised learning framework that leverages this idea. Unlike earlier contrastive methods that rely on negative pairs, BYOL innovates by learning without explicit dissimilarity constraints. How does it avoid collapse while training with only positive pairs? Let’s dive into the details. . . 1. Traditional Supervised vs. Self-Supervised Learning . 1.1 Supervised Learning (ImageNet Pre-training) . Requires millions of labeled images | Each image is categorized into one of 1000 classes | Learns features through explicit supervision (e.g., &quot;this is a dog&quot;) | . 1.2 Self-Supervised Learning (BYOL Approach) . Uses unlabeled images | No need for human categorization | Learns features by understanding relationships between images | . Thought Experiment: Imagine you have: . 1 million labeled images (supervised approach) | 100 million unlabeled images (self-supervised approach) | Could self-supervised learning from a larger unlabeled dataset provide better or different kinds of features compared to supervised learning from fewer labeled examples? . . 2. How BYOL Works: Learning Without Negative Pairs . Unlike earlier contrastive methods (e.g., SimCLR), BYOL does not require negative pairs. Instead, it relies on two key mechanisms: . Data Augmentation to Create Positive Pairs: Given an image, two augmented views are created (e.g., cropping, color jittering, rotation). | The model learns to predict one view from the other. | . | Asymmetric Network Design: An online network and a target network work together. | The online network learns directly through gradient updates, while the target network updates slowly via an exponential moving average (EMA). | . | 2.1 Architectural Components of BYOL . Online Network: Encoder: $f_ theta$ | Projector: $g_ theta$ | Predictor: $q_ theta$ | . | Target Network: Encoder: $f_ xi$ | Projector: $g_ xi$ (EMA of online parameters) | . | 2.2 Loss Function . The BYOL loss is: $$ L_{ text{BYOL}} = mathbb{E} left[ |q_ theta(g_ theta(f_ theta(x))) - g_ xi(f_ xi(x^+)) |^2 right] $$ Unlike contrastive loss, it does not rely on negative pairs. . . 3. Avoiding Collapse: The Role of Asymmetry . 3.1 Representation Collapse in SSL . Collapse occurs when a model maps all inputs to the same representation, leading to trivial solutions: . Constant Representations: . ∃𝑐∈ℝ𝑑 s.t. 𝔼𝑥[|𝑓𝜃(𝑥)−𝑐|2]≈0 . | Dimensionality Collapse: Representations lie in a low-dimensional subspace. . | 3.2 How BYOL Avoids Collapse . BYOL prevents collapse through two mechanisms: . Predictor Asymmetry: . The predictor 𝑞𝜃 ensures that the representations do not collapse. | The loss function minimizes: . min𝜃‖𝑞𝜃(𝑧𝜃)−𝑧′𝜉‖2 . | . | EMA Dynamics: The target network updates via: 𝜉←𝜏𝜉+(1−𝜏)𝜃 | This creates a slowly evolving target that prevents trivial solutions. | . | . 4. Mathematical Insights . 4.1 Conditional Variance Perspective . BYOL minimizes: $$ mathbb{E}[ text{Var}(z&#39;_ xi | z_ theta)] $$ Using the law of total variance: $$ text{Var}(z&#39;_ xi) = mathbb{E}[ text{Var}(z&#39;_ xi|z_ theta)] + text{Var}( mathbb{E}[z&#39;_ xi|z_ theta]) $$ BYOL prevents collapse by balancing these terms. . 4.2 Stability Analysis . BYOL’s predictor introduces an implicit regularization: $$ min_ theta |q_ theta(z_ theta) - z&#39;_ xi |^2 quad text{s.t. } z_ theta = f_ theta(x), z&#39;_ xi = text{EMA}(f_ theta(x)) $$ . . 5. Empirical Insights and Practical Considerations . 5.1 Detecting Collapse . Feature Variance: $$ text{Var}(z_ theta) = frac{1}{d} sum_{i=1}^d text{Var}(z_ theta^{(i)}) $$ | Eigenvalue Spectrum: Collapse occurs when: $$ frac{ lambda_{ max}(C)}{ lambda_{ min}(C)} &gt; 10^3 $$ | 5.2 Recommended Hyperparameters . EMA Momentum: Warm up $ tau$ from 0.996 to 0.999 over 1000 steps. | Predictor Learning Rate: Should be 10× encoder LR. | Augmentations: Strong augmentations remain crucial. | . . 6. Key Takeaways . BYOL learns representations without negative pairs by leveraging moving target networks. | Architectural asymmetry is crucial in preventing collapse. | The predictor network acts as an implicit constraint, ensuring diverse representations. | The exponential moving average (EMA) smooths updates, maintaining stable learning dynamics. | . 7. Practical Implementation . Let&#39;s use fast.ai to implement the paper from scratch using a manageble dataset ( CIFAR10 ) and resnet18 for faster iterations. We can later scale it to larger datasets and models. . import torch import torch.nn as nn import torch.nn.functional as F from copy import deepcopy from fastai.vision.all import * import albumentations as A from albumentations.pytorch import ToTensorV2 class BYOL(nn.Module): def __init__(self, encoder=&#39;resnet18&#39;, projection_size=128, projection_hidden=256, prediction_hidden=64): super().__init__() # 1. Online encoder - initialize the model first encoder_model = getattr(models, encoder)(weights=None) self.online_encoder = nn.Sequential( *list(encoder_model.children())[:-1], # Remove final FC layer nn.AdaptiveAvgPool2d((1, 1)), # Add adaptive pooling nn.Flatten() # Add flatten layer ) # Get encoder output dimension self.online_encoder.eval() with torch.no_grad(): enc_out = self.online_encoder(torch.randn(4, 3, 32, 32)) enc_dim = enc_out.shape[1] self.enc_dim = enc_dim self.online_encoder.train() # 2. Projector self.online_projector = nn.Sequential( nn.Linear(enc_dim, projection_hidden), nn.BatchNorm1d(projection_hidden), nn.ReLU(inplace=True), nn.Linear(projection_hidden, projection_size) ) # 3. Predictor self.predictor = nn.Sequential( nn.Linear(projection_size, prediction_hidden), nn.BatchNorm1d(prediction_hidden), nn.ReLU(inplace=True), nn.Linear(prediction_hidden, projection_size) ) # 4. Target network - no gradients needed self.target_encoder = None self.target_projector = None self.init_target_network() def init_target_network(self): &quot;&quot;&quot;Initialize target network with online network weights&quot;&quot;&quot; self.target_encoder = deepcopy(self.online_encoder) self.target_projector = deepcopy(self.online_projector) for param in self.target_encoder.parameters(): param.requires_grad = False for param in self.target_projector.parameters(): param.requires_grad = False def forward(self, x): &quot;&quot;&quot; x is expected to be a tuple of (view1, view2) &quot;&quot;&quot; if isinstance(x, (tuple, list)): x1, x2 = x else: x1, x2 = x, x # For validation # Online network forward passes z1 = self.online_projector(self.online_encoder(x1)) z2 = self.online_projector(self.online_encoder(x2)) # Get predictions p1 = self.predictor(z1) p1 = F.normalize(p1, dim=-1) p2 = self.predictor(z2) p2 = F.normalize(p2, dim=-1) # Target network forward passes with torch.no_grad(): t1 = self.target_projector(self.target_encoder(x1)) t2 = self.target_projector(self.target_encoder(x2)) t1 = F.normalize(t1, dim=-1) t2 = F.normalize(t2, dim=-1) return p1, p2, t1.detach(), t2.detach() @torch.no_grad() def update_target_network(self, tau): &quot;&quot;&quot; Update target network parameters using exponential moving average tau: EMA decay rate (0,1) &quot;&quot;&quot; for online_p, target_p in zip(self.online_encoder.parameters(), self.target_encoder.parameters()): target_p.data = tau * target_p.data + (1 - tau) * online_p.data for online_p, target_p in zip(self.online_projector.parameters(), self.target_projector.parameters()): target_p.data = tau * target_p.data + (1 - tau) * online_p.data class BYOLLoss(nn.Module): def forward(self, preds, *args): &quot;&quot;&quot; Compute BYOL&#39;s loss function. preds: tuple of (p1, p2, t1, t2) from BYOL model &quot;&quot;&quot; p1, p2, t1, t2 = preds # Symmetric loss loss = ( 2 - 2 * F.cosine_similarity(p1, t2.detach(), dim=-1).mean() + 2 - 2 * F.cosine_similarity(p2, t1.detach(), dim=-1).mean() ) / 2 return loss class BYOLMetrics(Callback): def __init__(self, τ_base=0.996): super().__init__() self.τ_base = τ_base self.feat_var = None # Store feat_var as instance variable self.output_similarity = None # Track output similarity def before_train(self): self.total_steps = len(self.learn.dls.train) * self.learn.n_epoch self.current_step = 0 def after_batch(self): # Update target network τ = 1 - (1 - self.τ_base) * (math.cos(math.pi * self.current_step / self.total_steps) + 1) / 2 self.learn.model.update_target_network(τ) # Compute feature variance (collapse detection) if not self.training: return with torch.no_grad(): # Handle tuple input - take first view x = self.xb[0][0] if isinstance(self.xb[0], tuple) else self.xb[0] z = self.learn.model.online_projector( self.learn.model.online_encoder(x) ) t = self.learn.model.target_projector( self.learn.model.target_encoder(x) ) # Compute cosine similarity self.output_similarity = F.cosine_similarity( z, t, dim=1 ).mean().item() self.feat_var = z.var(dim=0).mean().item() # Check for collapse if self.feat_var &lt; 1e-4: raise CancelFitException(&quot;Training collapsed!&quot;) self.current_step += 1 def after_epoch(self): &quot;&quot;&quot;Log feature variance at the end of each epoch&quot;&quot;&quot; if self.feat_var is not None: print(f&quot;Feature variance: {self.feat_var:.6f}&quot;) print(f&quot;Output similarity: {self.output_similarity:.6f}&quot;) def get_augmentations(): &quot;&quot;&quot;Get BYOL&#39;s augmentation pipeline&quot;&quot;&quot; return A.Compose([ A.RandomResizedCrop(32, 32), A.HorizontalFlip(p=0.5), A.ColorJitter( brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8 ), A.ToGray(p=0.2), A.GaussianBlur(p=0.1), A.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) ]) class TwoCropsTransform: def __init__(self, aug): self.aug1 = aug self.aug2 = deepcopy(aug) def __call__(self, x): # Convert PILImage/Tensor to numpy array if isinstance(x, torch.Tensor): img = x.numpy() elif isinstance(x, PILImage): img = np.array(x) else: img = x if img.ndim == 2: # Handle grayscale img = np.repeat(img[..., None], 3, axis=-1) # Create two views view1 = self.aug1(image=img)[&#39;image&#39;] view2 = self.aug2(image=img)[&#39;image&#39;] # Convert to NCHW format view1 = torch.from_numpy(view1).float().permute(2, 0, 1) # HWC -&gt; CHW view2 = torch.from_numpy(view2).float().permute(2, 0, 1) # HWC -&gt; CHW return ( TensorImage(view1), TensorImage(view2) ) def get_data(batch_size=256): &quot;&quot;&quot;Setup CIFAR-10 with dual augmentations&quot;&quot;&quot; path = untar_data(URLs.CIFAR) dblock = DataBlock( blocks=(ImageBlock, ImageBlock), # One block for each view get_items=get_image_files, splitter=RandomSplitter(), item_tfms=TwoCropsTransform(get_augmentations()) ) return dblock.dataloaders( path, bs=batch_size, num_workers=4 ) class FeatureVarianceMetric(Metric): def __init__(self): self.reset() def reset(self): self.total = 0 self.count = 0 def accumulate(self, learn): x = learn.xb[0][0] if isinstance(learn.xb[0], tuple) else learn.xb[0] with torch.no_grad(): z = learn.model.online_projector(learn.model.online_encoder(x)) var = z.var(dim=0).mean().item() self.total += var self.count += 1 @property def value(self): return self.total/self.count if self.count != 0 else None dls = get_data() model = BYOL() def test_target_network_initialization(): model = BYOL() for t, o in zip(model.target_encoder.parameters(), model.online_encoder.parameters()): assert torch.allclose(t, o) assert not t.requires_grad def test_ema_update(): model = BYOL() initial_weight = model.target_encoder[0].weight.clone() model.online_encoder[0].weight.data += 0.1 model.update_target_network(0.9) new_weight = model.target_encoder[0].weight expected = 0.9 * initial_weight + 0.1 * model.online_encoder[0].weight assert torch.allclose(new_weight, expected, atol=1e-6) def test_normalization(): model = BYOL() x = torch.randn(4, 3, 32, 32) p1, p2, t1, t2 = model((x, x)) assert torch.allclose(p1.norm(dim=-1), torch.ones(4), atol=1e-4) # After loss norm test_target_network_initialization() test_ema_update() test_normalization() learn = Learner( dls, model, loss_func=BYOLLoss(), metrics=[FeatureVarianceMetric()], cbs=[ BYOLMetrics(), GradientClip(1.0), MixedPrecision() ] ) learn.fit_one_cycle( 10, # epochs lr_max=2e-3 ) . . 7.1 Validation . Linear Evaluation . linear evaluation involves freezing the encoder and training a linear classifier on top using labeled data. This is a standard way to assess the quality of learned representations. . class LinearEval(nn.Module): def __init__(self, encoder, num_classes=14): super().__init__() self.encoder = encoder self.classifier = nn.Linear(learn.enc_dim, num_classes) # Freeze encoder for param in self.encoder.parameters(): param.requires_grad = False def forward(self, x): with torch.no_grad(): # Critical for correct evaluation features = self.encoder(x) return self.classifier(features) def get_linear_data(): &quot;&quot;&quot;Modified DataBlock for labeled Pets&quot;&quot;&quot; path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) pat = r&#39;^(.*)_ d+.jpg$&#39; # extract pet breed from filename, e.g. &quot;Abyssinian_39.jpg&quot; → &quot;Abyssinian&quot; return DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=RegexLabeller(pat), item_tfms=Resize(32), batch_tfms=aug_transforms() # Standard supervised augs ).dataloaders(path/&#39;images&#39;, bs=32, num_workers=4) linear_dls = get_linear_data() encoder = deepcopy(learn.model.online_encoder) linear_model = LinearEval(encoder) learn_linear = Learner( linear_dls, linear_model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=MixedPrecision() ) def test_encoder_frozen(): for param in learn_linear.model.encoder.parameters(): assert not param.requires_grad def test_feature_consistency(): x = torch.randn(4, 3, 32, 32).cuda() with torch.no_grad(): f1 = learn.model.online_encoder(x) f2 = learn.model.online_encoder(x) assert torch.allclose(f1, f2, atol=1e-6) # Deterministic features test_encoder_frozen(), test_feature_consistency() learn_linear.fit_one_cycle( 10, lr_max=3e-4 ) . . 8. Conclusion . BYOL showcases that self-supervised learning can be effective without contrastive learning. By leveraging moving target networks, asymmetric predictors, and implicit variance regularization, BYOL remains a foundational approach for learning powerful image representations. .",
            "url": "https://numb3r33.github.io/experiments/self-supervised-learning/math/deeplearning/fast.ai/2025/02/04/byol.html",
            "relUrl": "/self-supervised-learning/math/deeplearning/fast.ai/2025/02/04/byol.html",
            "date": " • Feb 4, 2025"
        }
        
    
  
    
        ,"post1": {
            "title": "Gradient Clipping and Adaptive Learning Rates",
            "content": "Introduction . Gradient clipping is a common technique to make model training more stable. It&#39;s operation could be expressed as . . The clipping threshold τ is often set to 1. Is this just a default value, or is there something deeper? | . The above formula ensures that if your gradient&#39;s magnitude exceeds τ, it gets scaled down while maintaining the direction. . But what exactly is ‖g‖, it is defined as &quot;global gradient norm&quot; in the literature | . Let&#39;s try to understand it with a simple example, imagine a tiny neural network . layer1 = Linear(2, 2) # 2x2 weight matrix + 2 biases = 6 parameters layer2 = Linear(2, 1) # 2x1 weight matrix + 1 bias = 3 parameters . When computing gradients, each parameter gets its own gradient. The global gradient norm is calculated by: . Flattening all gradients into one long vector | Computing the L2 norm ( Euclidean Norm ) of this vector | It is like measuring the &quot;overall strength&quot; of all gradients combined. . But why τ=1 works so well. What&#39;s the mathematical intuition? | . The key insight comes from looking at how model updates affect the loss function. For SGD, we can write . . For vanilla SGD $$u_t = g_t$$ . We can rewrite the above equation as . . The key insight is that change in loss is proportional to the square of the gradient norm. For stable training, we typically want |ΔL| &lt; η ( change in loss to be less than the learning rate ). . Therefore: . |-η‖gt‖²| &lt; η . η‖gt‖² &lt; η . ‖gt‖² &lt; 1 . ‖gt‖ &lt; 1 . Let&#39;s try to verify this claim by running some experiments on MNIST dataset using fast.ai . from __future__ import annotations from fastai.torch_basics import * from fastai.vision.all import * . import pandas as pd import matplotlib.pyplot as plt from torch.utils.data import DataLoader from functools import partial . class GradientTracker(Callback): &quot;&quot;&quot;Callback to track gradient norms during training&quot;&quot;&quot; def __init__(self): self.grad_norms = [] self.loss_changes = [] self.last_loss = None def before_batch(self): # Store the loss value before the batch if self.learn.loss_func is not None: self.last_loss = self.learn.loss.item() if self.learn.loss is not None else None def after_backward(self): # Calculate global gradient norm total_norm = 0. for p in self.learn.model.parameters(): if p.grad is not None: param_norm = p.grad.data.norm(2) total_norm += param_norm.item() ** 2 total_norm = total_norm ** 0.5 self.grad_norms.append(total_norm) # Calculate change in loss if self.last_loss is not None and self.learn.loss is not None: loss_change = abs(self.learn.loss.item() - self.last_loss) self.loss_changes.append(loss_change) def plot_stats(self): &quot;&quot;&quot;Plot gradient norms and loss changes&quot;&quot;&quot; fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10)) # Plot gradient norms ax1.plot(self.grad_norms) ax1.axhline(y=1.0, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;τ=1.0&#39;) ax1.set_title(&#39;Gradient Norms During Training&#39;) ax1.set_xlabel(&#39;Batch&#39;) ax1.set_ylabel(&#39;Global Gradient Norm&#39;) ax1.legend() # Plot loss changes ax2.plot(self.loss_changes) ax2.axhline(y=self.learn.lr, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=f&#39;η={self.learn.lr}&#39;) ax2.set_title(&#39;Absolute Loss Changes During Training&#39;) ax2.set_xlabel(&#39;Batch&#39;) ax2.set_ylabel(&#39;|ΔL|&#39;) ax2.legend() plt.tight_layout() return fig . def create_experiment(clip=True): path = untar_data(URLs.MNIST) dls = ImageDataLoaders.from_folder(path, valid_pct=0.2, batch_size=128, item_tfms=Resize(28), n_inp=1 ) model = nn.Sequential( nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(32 * 7 * 7, 10) ) # Create learner with gradient tracking learn = Learner(dls, model, metrics=accuracy) grad_tracker = GradientTracker() learn.add_cb(grad_tracker) # Add gradient clipping if specified if clip: learn.add_cb(GradientClip(max_norm=1.0)) # Train for a few epochs learn.fit_one_cycle(5, 1e-2) return learn, grad_tracker . learn_clip, tracker_clip = create_experiment(clip=True) learn_no_clip, tracker_no_clip = create_experiment(clip=False) . . fig_clip = tracker_clip.plot_stats() plt.suptitle(&#39;With Gradient Clipping (τ=1.0)&#39;) . . fig_no_clip = tracker_no_clip.plot_stats() plt.suptitle(&#39;Without Gradient Clipping&#39;) . . comparison = pd.DataFrame({ &#39;Metric&#39;: [&#39;Mean Gradient Norm&#39;, &#39;Max Gradient Norm&#39;, &#39;Mean |ΔL|&#39;, &#39;Max |ΔL|&#39;], &#39;With Clipping&#39;: [ np.mean(tracker_clip.grad_norms), np.max(tracker_clip.grad_norms), np.mean(tracker_clip.loss_changes), np.max(tracker_clip.loss_changes) ], &#39;Without Clipping&#39;: [ np.mean(tracker_no_clip.grad_norms), np.max(tracker_no_clip.grad_norms), np.mean(tracker_no_clip.loss_changes), np.max(tracker_no_clip.loss_changes) ] }) print(&quot; nComparison Statistics:&quot;) print(comparison.to_string(index=False)) . . What the above experiment suggests that . Early Training ( 0-500 batches ): . Global Gradient Norm naturally start large ( &gt; 1) | Without clipping, they reached 8.0 | With clipping, they were capped at ~4 | . | Middle/Late Training ( 500+ batches ) . Both experiments naturally converged to gradient norms &lt; 1 | Mean gradient norms were similar ( 0.48 v 0.49 ) | . | This confirms that ||g|| &lt; 1 is indeed the natural state of a well trained model. . We also know from literature that if the gradient modulus is significanly greather than 1 in the early state, then usual strategy is warmup ir we can consider a more general strategy: set another threshold $ tau$ . . For optimizers like Adam . . How does $u_t$ becomes $sign(g_t)$ and $sign(g_t)$ . $g_t$ becomes $ |g_t |_1$? . $u_t$ = $sign(g_t)$ because Adam&#39;s updates tend to normalize gradient magnitudes while preserving direction. . And when we multiply a number by it&#39;s sign, you get its absolute value. . This explains why Adam typically needs smaller learnings rates than SGD as L1 norm is usually larger than L2 norm. . The above formula could also be written as . . Since $sign(g_t)$ is a vector of either +1 or -1 thereforme $||sign(g_t)||^2$ = N so therefore $||sign(g_t)||$ = $ sqrt(N)$ . and . a . b = ||a|| . ||b||. cos(a, b) . therefore . $sign(g_t) . gt$ = $ sqrt(N) . ||g_t|| . cos(sign(g_t), g_t)$. $cos(sign(g_t, g_t))$ is roughly constant across model scales so if we want to maintain $ Delta L$ $ eta$ should be inversely proportional to $ sqrt(N)$, which means if the number of model parameter increases by 4 times, learning rate can be halved. . Takeaways . τ=1 isn&#39;t arbitrary - it aligns with the natural scale of gradients during stable training. | The relationship |ΔL| &lt; η mathematically explains why gradients naturally stay below 1 during stable training. | Different optimizers (SGD vs Adam) have different relationships between gradients and loss changes, which affects their learning rate requirements. | When scaling to larger models, learning rates should scale with 1/√N (where N is parameter count) to maintain consistent training dynamics. | . This understanding helps us move from blindly using default values to making principled decisions about optimization hyperparameters in deep learning! . References . https://kexue.fm/archives/10657 | https://papers.cool/arxiv/2310.07831 | .",
            "url": "https://numb3r33.github.io/experiments/optimization/math/deeplearning/fast.ai/2025/01/12/gradient-clipping.html",
            "relUrl": "/optimization/math/deeplearning/fast.ai/2025/01/12/gradient-clipping.html",
            "date": " • Jan 12, 2025"
        }
        
    
  
    
        ,"post2": {
            "title": "Deep-Contextualized Embeddings ( ELMO )",
            "content": "In this post, I will explore the key ideas presented in the ELMo paper and discuss what it teaches us as a research community. The focus will be on understanding the big concepts and implementing them using fast.ai. To keep it practical, we will work on a toy dataset and prioritize implementation over reproducing the original paper’s results. . Paper details . Contextualized Word Embeddings . Limitations of Word2Vec paper . Why Settle for One Embedding When Words Have Multiple Meanings? . Consider the word stick in these sentences: . Let’s stick to the plan. | He picked up a stick. | . The meaning of stick changes based on its context. So, why not embed words in a way that captures this contextual nuance? . ELMo (Embeddings from Language Models) solves this problem by creating contextualized word embeddings. Instead of static embeddings, ELMo uses multiple layers to generate embeddings based on surrounding context. . The architecture consists of the following key components (as illustrated in the figure): . Embedding Layer | Two-layer BiLSTM | Top Layer: Contextualized Word Representation | . Character Embedding Layer . The Character Embedding Layer in ELMo represents words using character-level features. This layer helps capture morphological details such as prefixes and suffixes, which are especially useful for handling out-of-vocabulary words and subword structures. . Key components: . Character Embeddings: Each word is broken into characters, and an embedding is assigned to each character. | CNN for Feature Extraction: A Convolutional Neural Network (CNN) is applied to extract character-level features from the input word. | Multiple Convolutional Kernels: The CNN uses multiple kernels (filters), each corresponding to different window sizes. This enables the model to learn features at varying character-level contexts. | Feature Vector: The output is a fixed-size character-level feature vector that encodes morphological information for the word. | . Bi-LSTM Layer . The Bi-LSTM Layer in ELMo is designed to capture rich semantic information by processing text bidirectionally: . It scans the input left-to-right and right-to-left simultaneously. The input passes through the following sequence of layers: . Character-based CNN Encoder: Extracts character-level features. | Two Bi-LSTM Layers: Captures context-dependent word representations by combining information from both directions. | . This bidirectional approach ensures that the embeddings are deeply contextualized, incorporating information from the entire sentence. . Character CNN Encoder . The Character CNN Encoder processes the input at the character level to generate dense representations for words. Below is the step-by-step breakdown: . Input Shape: | . [batch, max_sentence_len, max_word_len] . Reshape: The input is reshaped to: | . [batch * max_sentence_len, max_word_len] . Character Embedding: Pass the reshaped input through an embedding layer to get: | . [batch * max_sentence_len, max_word_len, embedding_dim] . embedding_dim is a predefined hyperparameter, e.g., 16. The embedding weights are learned during training. This step maps the one-hot encoded input of size 50 to a dense representation of size 16. . Transpose: Transpose the output to prepare it for convolution: | . [batch * max_sentence_len, embedding_dim, max_word_len] . Convolutional Layers: Pass the transposed output through multiple convolutional layers with varying kernel sizes and output channels: | . [batch * max_sentence_len, out_channels, new_h] . Max Pooling: Apply max pooling along the h dimension to obtain: | . [batch * max_sentence_len, out_channels] . Concatenation: Concatenate the outputs of all convolutional layers along the out_channels dimension to form: | . [batch * max_sentence_len, n_filters] n_filters represents the total number of output channels summed across all kernels. . Highway Layers: Pass the concatenated output through two highway layers, defined as: | . y = g * x + (1 - g) * f(A(x)) where g = Sigmoid(B(x)) . This operation preserves the output shape. . Linear Projection: Apply a linear projection to map the output to the desired dimension: | . [batch * max_sentence_len, output_dim] . Final Reshape: Reshape the output back to: | . [batch, max_sentence_len, output_dim] . . Bi-LSTM . Char CNN&#39;s output passes through forward layer [ batch, max_sentence_len, hidden_size ] and backward layer [ batch, max_sentence_len, hidden_size ] | Concat forward and backward data in the hidden_size dim to get [b, max_sentence_len, 2 * hidden_size] | . from fastai.torch_basics import * from fastai.text.all import * . from datasets import load_dataset dsd = load_dataset(&#39;karpathy/tiny_shakespeare&#39;, trust_remote_code=True) train, valid, test = dsd[&#39;train&#39;][0][&#39;text&#39;], dsd[&#39;validation&#39;][0][&#39;text&#39;], dsd[&#39;test&#39;][0][&#39;text&#39;] . We would use the tiny-shakespeare dataset which contains 40,000 lines of Shakespeare from a variety of Shakespeare&#39;s plays. . @delegates() class ELMODataLoader(LMDataLoader): &quot;A `DataLoader` suitable for Continuous Bag of Words (CBOW)&quot; def __init__(self, dataset, context_size=2, bs=64, num_workers=0, **kwargs): self.context_size = context_size super().__init__(dataset, bs=bs, num_workers=num_workers, **kwargs) def create_item(self, idx): if idx is None: idx = 0 if is_listy(self.dataset[idx][0]): tokens, char_tokens = self.dataset[idx][0][0], self.dataset[idx][0][1] else: tokens, char_tokens = self.dataset[idx][0], self.dataset[idx][1] if len(tokens) &lt; 2 * self.context_size + 1: raise IndexError(&quot;Sentence too short for the given context size&quot;) context = torch.hstack((tokens[:self.context_size], tokens[self.context_size+1:])) target = tokens[self.context_size] return (TensorText(context), char_tokens), TensorCategory(target) @delegates(TfmdDL.new) def new(self, dataset=None, context_size=None, **kwargs): context_size = self.context_size if context_size is None else context_size return super().new(dataset=dataset, context_size=context_size, **kwargs) . Extending LMDataLoader for CBOW-style Data . To prepare data in the Continuous Bag of Words (CBOW) style, we modify the LMDataLoader to return (context, target word) pairs. Here, the context is a group of surrounding words used to predict the target word. . Example: Given the sequence: . x1, x2, y, x3, x4 . If the context size is 2, the model will use: . Context: x1, x2, x3, and x4 Target: y . texts = list(filter(lambda x: len(x.split()) &gt; 7, train[:100000].split(&#39; n&#39;))) n_trn = len(texts) val_txts = list(filter(lambda x: len(x.split()) &gt; 7, valid.split(&#39; n&#39;))) texts.extend(val_txts) # Create a DataFrame df = pd.DataFrame({&#39;texts&#39;: texts}) splits = L([np.arange(n_trn), np.arange(n_trn, n_trn+len(val_txts))]) df.shape . Only consider those lines which have atleast 7 words in the training and validation dataset. . class NGramsTokenizer(): def __init__(self, n=1): self.n = n def __call__(self, sent): words = sent.split() ngrams = [[&#39;&#39;.join(word[i:i+self.n]) for i in range(len(word)-self.n+1)] for word in words] return ngrams class NumericalizeChars(Numericalize): def encodes(self, x): return [[self.o2i[l_] for l_ in o_ ] for o_ in x] class ElmoTransform(Transform): def __init__(self, word_vocab, char_vocab=None): self.num_word = Numericalize(min_freq=2) self.num_char = NumericalizeChars(min_freq=2) self.num_word.setup(word_vocab) self.num_char.setup(char_vocab) def encodes(self, x): tok_word = Tokenizer.from_df(&#39;texts&#39;)(x) tok_char = NGramsTokenizer()(x.lower()) numer_words = self.num_word(tok_word) numer_chars = self.num_char(tok_char) return (numer_words,numer_chars) word_vocab = L(x for x in df.texts.str.split()) char_vocab = L(x for x in df.texts.map(set)) pipe = TfmdLists(df, [attrgetter(&#39;texts&#39;), ElmoTransform(word_vocab=word_vocab, char_vocab=char_vocab)]) pipe[0] . Define a custom transform to numericalize both word and character tokens. . class ElmoPadChunk(Transform): def __init__(self, pad_word_idx=1, pad_char_idx=0, pad_first=False, seq_len=72, word_len=32, decode=True, **kwargs): store_attr(&#39;pad_word_idx, pad_char_idx, pad_first, seq_len,seq_len,word_len&#39;) def _get_max_word_len(self, batch): return max([len(s[0][0]) for s in batch]) def _get_max_char_len(self, batch): return max([len(x) for s in batch for x in s[0][1]]) def encodes(self, batch): X, lbl = zip(*batch) xw, xc = zip(*X) xw = [(x, ) for x in xw] self.max_word_len = self._get_max_word_len(batch) self.max_char_len = self._get_max_char_len(batch) xc = [ f[:self.max_word_len] if len(f) &gt; self.max_word_len else f + [[self.pad_word_idx]]*(self.max_word_len - len(f)) for f in xc ] xc = [[w + [self.pad_char_idx]*(self.max_char_len - len(w)) for w in f] for f in xc] pw = pad_input_chunk(xw, pad_idx=self.pad_word_idx, pad_first=self.pad_first, seq_len=self.max_word_len ) pwc = default_collate([x[0] for x in pw]) pc = TensorText(xc) pc = pc.transpose(1, 2) lbls = default_collate(lbl, ) return pc, lbls . vocab_tfm = ElmoTransform(word_vocab=word_vocab, char_vocab=char_vocab) vw = vocab_tfm.num_word.vocab vc = vocab_tfm.num_char.vocab tfms = [attrgetter(&#39;texts&#39;), vocab_tfm] dsets = Datasets(df, [tfms], splits=splits, dl_type=ELMODataLoader) bs,cs = 16,1 dl_kwargs = { &#39;before_batch&#39;: ElmoPadChunk(pad_word_idx=1, pad_char_idx=1), &#39;create_batch&#39;: fa_convert } dls = dsets.dataloaders(bs=bs, context_size=cs, **dl_kwargs) . class CharacterLayer(Module): def __init__(self, input_dim, embedding_dim, output_dim, filters_list, kernel_size_list, highway_num): store_attr() self.embedding = nn.Embedding(input_dim, embedding_dim) self.convs = nn.ModuleList([ nn.Conv1d(embedding_dim, f, k, padding=&#39;same&#39;) for f, k in zip(filters_list, kernel_size_list) ]) self.dim = sum(filters_list) self.highways = nn.ModuleList([nn.Linear(self.dim, self.dim*2) for _ in range(highway_num)]) self.projection = nn.Linear(self.dim, output_dim) def forward(self, x): # x: (batch_size, seq_len, token_len) bs, seq_len, token_len = x.shape # Flatten (batch_size, seq_len) for embedding x = x.reshape(bs*seq_len, token_len) x = self.embedding(x) # (bs*seq_len, token_len, embedding_dim) x = x.permute(0, 2, 1) # (bs*seq_len, embedding_dim, token_len) # Convolutions + max pooling outs = [] for conv in self.convs: c_out = conv(x) # (bs*seq_len, f, token_len) c_out = torch.max(c_out, dim=2)[0] # max pool along token_len c_out = F.relu(c_out) outs.append(c_out) x = torch.cat(outs, dim=-1) # (bs*seq_len, sum(filters_list)) # Highway layers for hw in self.highways: hw_out = hw(x) # (bs*seq_len, 2*self.dim) a = F.relu(hw_out[:, :self.dim]) g = torch.sigmoid(hw_out[:, self.dim:]) x = a*g + x*(1-g) # Projection x = self.projection(x) # (bs*seq_len, output_dim) x = x.reshape(bs, seq_len, -1) # (bs, seq_len, output_dim) return x . class Elmo(Module): def __init__(self, lstm_num): store_attr() self.embedding = CharacterLayer( input_dim=len(vc), embedding_dim=16, output_dim=50, filters_list=[12,24], kernel_size_list=[2,3], highway_num=2 ) self.forward_lstm = nn.ModuleList() self.backward_lstm = nn.ModuleList() for _ in range(lstm_num): self.forward_lstm.append(nn.LSTM(50, 50, batch_first=True)) self.backward_lstm.append(nn.LSTM(50, 50, batch_first=True)) # Append one more forward and backward LSTM as in the original code self.forward_lstm.append(nn.LSTM(50, 50, batch_first=True)) self.backward_lstm.append(nn.LSTM(50, 50, batch_first=True)) self.forward_projection = nn.Linear(50, len(vw)) self.backward_projection = nn.Linear(50, len(vw)) def _run_lstm(self, lstm, x, backwards=False): # If backwards=True, we simulate go_backwards by flipping the sequence dimension if backwards: x = torch.flip(x, dims=[1]) x, _ = lstm(x) if backwards: x = torch.flip(x, dims=[1]) return x def forward(self, inputs): # inputs: (batch, seq_len, token_len) x = self.embedding(inputs) # (batch, seq_len, 50) # First forward and backward LSTM pass outside the loop, as in original code outputs1 = self._run_lstm(self.forward_lstm[0], x, backwards=False) outputs2 = self._run_lstm(self.backward_lstm[0], x, backwards=True) # Apply all forward LSTMs (including the first one again) for lstm in self.forward_lstm: outputs1 = self._run_lstm(lstm, outputs1, backwards=False) # Apply all backward LSTMs (including the first one again) for lstm in self.backward_lstm: outputs2 = self._run_lstm(lstm, outputs2, backwards=True) outputs1 = self.forward_projection(outputs1) # (batch, seq_len, vocab_size) outputs2 = self.forward_projection(outputs2) # Original code uses forward_projection again # Concatenate along seq_len dimension (dim=1 since batch_first=True) out = torch.cat([outputs1, outputs2], dim=1) return out.mean(dim=1) . m = Elmo(lstm_num=2).cuda() . learner = Learner(dls, m, loss_func=CrossEntropyLossFlat(), metrics=accuracy) . References . https://speakerdeck.com/i_mishramayank/deep-contextualized-word-embeddings | https://arxiv.org/abs/1802.05365 | .",
            "url": "https://numb3r33.github.io/experiments/optimization/math/deeplearning/fast.ai/2024/12/17/elmo.html",
            "relUrl": "/optimization/math/deeplearning/fast.ai/2024/12/17/elmo.html",
            "date": " • Dec 17, 2024"
        }
        
    
  
    
        ,"post3": {
            "title": "Tight-fisted Optimizer ( Tiger )",
            "content": "This post will try to breakdown the process of how to implement any optimizer ( in this case tiger ) in fast.ai. . Primer on how to implement an optimizer in fast.ai . Setup . import matplotlib as mpl mpl.rcParams[&#39;image.cmap&#39;] = &#39;gray&#39; from __future__ import annotations from fastai.torch_basics import * from fastai.vision.all import * from torcheval.metrics import MulticlassAccuracy from torch import tensor . from datasets import load_dataset . dsd = load_dataset(&#39;fashion_mnist&#39;, trust_remote_code=True) . xl,yl = &#39;image&#39;,&#39;label&#39; name = &quot;fashion_mnist&quot; dsd = load_dataset(name) . class FashionMnistTransform(Transform): def __init__(self, xl): self.xl = xl def encodes(self, o): return TensorImage(o[self.xl]) class Lblr(Transform): def __init__(self, yl): self.yl = yl def encodes(self, o): return o[self.yl] . tfms = [[FashionMnistTransform(xl), IntToFloatTensor()], [Lblr(yl), Categorize()]] dsets = Datasets(dsd[&#39;train&#39;], tfms, splits=RandomSplitter(seed=42)(dsd[&#39;train&#39;])) . dls = dsets.dataloaders(bs=64) . x, y = dls.one_batch() x.shape, y.shape . dls.show_batch() . def conv(ni, nf, ks=3, stride=2, act=True): res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res class BasicModel(Module): def __init__(self): self.m = nn.Sequential(conv(1 ,8), conv(8 ,16), conv(16,32), conv(32,64), conv(64,10, act=False), nn.Flatten() ) def forward(self, x): x = x.unsqueeze(1) return self.m(x) . bm = BasicModel() o = bm(x) . learn = Learner(dls, bm, metrics=accuracy) . learn.fit_one_cycle(3) . . Delving deep into the Optimizers in fast.ai . fast.ai provides an optimizer class which takes in params and a list of callbacks. . class Optimizer(_BaseOptimizer): &quot;Base optimizer class for the fastai library, updating `params` with `cbs`&quot; _keep_on_clear = [&#39;force_train&#39;, &#39;do_wd&#39;] def __init__(self, params:Tensor|Iterable, # Model parameters cbs:callable|MutableSequence, # `Optimizer` step callbacks **defaults # Hyper parameters default values ): . params will be used to create the param_groups of the optimizer.cbs is a list of functions that will be composed when applying the step. For instance, you can compose a function making the SGD step, with another one applying weight decay. . Additionally, each cb can have a defaults attribute that contains hyper-parameters and their default value. Those are all gathered at initialization, and new values can be passed to override those defaults with the defaults kwargs. The steppers will be called by Optimizer.step (which is the standard PyTorch name), and gradients can be cleared with Optimizer.zero_grad (also a standard PyTorch name). . Once the defaults have all been pulled off, they are copied as many times as there are param_groups and stored in hypers. To apply different hyper-parameters to different groups (differential learning rates, or no weight decay for certain layers for instance), you will need to adjust those values after the init. . Example of how to implement SGD with momentum. . begin{aligned} text{Update Step:} quad &amp; mathbf{v}_{t+1} = mu mathbf{v}_t - eta left( nabla_{ mathbf{w}} mathcal{L}( mathbf{w}_t) + lambda mathbf{w}_t right) &amp; mathbf{w}_{t+1} = mathbf{w}_t + mathbf{v}_{t+1} end{aligned} def weight_decay(p, lr, wd, do_wd=True, **kwargs): &quot;Weight decay as decaying `p` with `lr*wd`&quot; if do_wd and wd!=0: p.data.mul_(1 - lr*wd) weight_decay.defaults = dict(wd=0.) def average_grad(p, mom, dampening=False, grad_avg=None, **kwargs): &quot;Keeps track of the avg grads of `p` in `state` with `mom`.&quot; if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data) damp = 1-mom if dampening else 1. grad_avg.mul_(mom).add_(p.grad.data, alpha=damp) return {&#39;grad_avg&#39;: grad_avg} average_grad.defaults = dict(mom=0.9) def momentum_step(p, lr, grad_avg, **kwargs): &quot;Step for SGD with momentum with `lr`&quot; p.data.add_(grad_avg, alpha=-lr) def SGD( params:Tensor|Iterable, # Model parameters lr:float|slice, # Default learning rate mom:float=0., # Gradient moving average (β1) coefficient wd:Real=0., # Optional weight decay (true or L2) decouple_wd:bool=True # Apply true weight decay or L2 regularization (SGD) ) -&gt; Optimizer: &quot;A SGD `Optimizer`&quot; cbs = [weight_decay] if decouple_wd else [l2_reg] if mom != 0: cbs.append(average_grad) cbs.append(sgd_step if mom==0 else momentum_step) return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd) . Here are the equations inolved for tight-fisted optimizer . begin{align*} m_t &amp;= beta m_{t-1} + (1 - beta) g_t theta_t &amp;= theta_{t-1} - eta_t left[ operatorname{sign}(m_t) + lambda_t theta_{t-1} right] end{align*} def tiger_step(p, lr, grad_avg, wd, **kwargs): p.data.add_(grad_avg.sign(), alpha=-lr) return p def Tiger( params:Tensor|Iterable, # Model parameters lr:float|slice, # Default learning rate mom:float=0.945, # Gradient moving average (β) coefficient wd:Real=0.01, # Optional weight decay (true or L2) decouple_wd:bool=True # Apply weight decay ) -&gt; Optimizer: &quot;A Tight-fisted ( Tiger ) `Optimizer`&quot; cbs = [weight_decay] if decouple_wd else [l2_reg] cbs += [partial(average_grad, dampening=True), tiger_step] return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd) . learn_tiger = Learner(dls, bm, metrics=accuracy, opt_func=Tiger) . learn_tiger.lr_find() . . learn_tiger.fit_one_cycle(3, 3e-3) . .",
            "url": "https://numb3r33.github.io/experiments/optimization/math/deeplearning/2024/11/19/tight-optimizer.html",
            "relUrl": "/optimization/math/deeplearning/2024/11/19/tight-optimizer.html",
            "date": " • Nov 19, 2024"
        }
        
    
  
    
        ,"post4": {
            "title": "im2col",
            "content": "What is convolution? . . Convolutional is a mathematical operation which in the context of image processing is used to transform an image which could lead to blurring, sharpening, edge detection or more. It can take input from two-dimensional matrices to four-dimensional matrices and the convolutional filters which also range from two-dimensional matrices to four-dimensional matrices. The above equation gives a unified view of this, where x is the input and k is the convolutional kernel giving us a feature map called a. . . In the above picture x of shape ( 4, 4 ) represents the input, w represents the convolutional filter of shape ( 3, 3 ) giving us A which would be our feature map. The process of convolution is to divide the input x into several smaller patches (matrices) of same size as the convolutional kernel and then performing elementwise multiply and sum with the convolutional kernel. . In the following example we have shown patches of size ( 3, 3 ) created out of input x and then performing convolution with the filter w would lead us value a11. Matrix of size (4,4) with kernel of size (3,3) and stride = 1 should give us 4 patches and hence the final result has 4 values. . . Now the element-wise multiply and sum would require us to iterate over element using for loops and then returning the sum like shown below. . from fastai.vision.all import * np.set_printoptions(linewidth=140) . x = np.arange(16, dtype=np.int32).reshape(4, 4) w = np.random.random(size=(3, 3)) x1 = x[:3, :3] c = 0 for i in range(x1.shape[0]): for j in range(x1.shape[1]): c += (x1[i, j] * w[i, j]) print(c) . 16.574367764256806 . Key insight is to view convlution as matrix multiplication which we can clearly see from the above two loops that it is doing element-wise multiplication and addition which is what we will get if we do a dot product of a row-vector and a column-vector so the idea is to transform the patch into a row-vector and the weights into a column vector. . . Perform the same operation over all the 4 patches giving us a new matrix which we will call X . . In the same spirit we can transform w as a column vector and we will call it W . . Now convolution is just a matrix multiplication between X and W . . Let&#39;s see how could we perform these operations using numpy, we will validate the results using the inbuild F.conv2d provided by Pytorch. . Single channel input . x = np.arange(16, dtype=np.int32).reshape(4, 4) x . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], dtype=int32) . The first patch would look something like . x1 = x[:3, :3] x1 . array([[ 0, 1, 2], [ 4, 5, 6], [ 8, 9, 10]], dtype=int32) . Now transform this into row vector . x1 = x1.reshape(1, -1) x1 . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10]], dtype=int32) . Do this for rest of the patches and stack them together to get the matrix . x2 = x[:3, 1:4] x2 = x2.reshape(1, -1) x3 = x[1:4, :3] x3 = x3.reshape(1, -1) x4 = x[1:4, 1:4] x4 = x4.reshape(1, -1) X = np.vstack((x1, x2, x3, x4)) X . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10], [ 1, 2, 3, 5, 6, 7, 9, 10, 11], [ 4, 5, 6, 8, 9, 10, 12, 13, 14], [ 5, 6, 7, 9, 10, 11, 13, 14, 15]], dtype=int32) . Now all of the row vectors have been stacked together to form X . Transform convolution kernel into a weight column vector. . np.random.seed(41) w = np.random.random(size=(3, 3)) w . array([[0.25092362, 0.04609582, 0.67681624], [0.04346949, 0.1164237 , 0.60386569], [0.19093066, 0.66851572, 0.91744785]]) . W = w.reshape(-1, 1) W . array([[0.25092362], [0.04609582], [0.67681624], [0.04346949], [0.1164237 ], [0.60386569], [0.19093066], [0.66851572], [0.91744785]]) . conv = np.dot(X, W) conv . array([[22.49748414], [26.01197293], [36.55543931], [40.0699281 ]]) . conv.reshape(2, 2) . array([[22.49748414, 26.01197293], [36.55543931, 40.0699281 ]]) . Verify if 2D convolution from Pytorch yields the same value or not . F.conv2d(tensor(x[None, None, :, :]).float(), tensor(w[None, None, :, :]), bias=None, stride=1) . tensor([[[[22.4975, 26.0120], [36.5554, 40.0699]]]]) . Indeed the values match up!! . Let&#39;s take a look at how does convolution operations changes when we consider multi-channel input and multiple filters. . Multi-channel input . Let&#39;s start with a case where we have a multi-channel input, so earlier we considered inputs of shape (H, W) now we would consider inputs like (C, H, W) and see how could we use our im2col algorithm to perform the convolution operation. . x = np.arange(32, dtype=np.int32).reshape(2, 4, 4) x . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]], dtype=int32) . In all of the cases presented here we have assumed stride=1 for simplicity, let&#39;s write a method that captures a patch and transforms into a row vector and then stack all of the row vectors together to form a matrix which we are referring to as X. . def patches_to_row(inp, kh, kw): H, W = inp.shape xm = H - kh ym = W - kw X = [] for i in range(0,xm+1): for j in range(0,ym+1): rv = inp[i:i+kh, j:j+kw] rv = rv.reshape(1, -1) X.append(rv) return np.vstack(X) mat = patches_to_row(np.arange(16, dtype=np.int32).reshape(4, 4), 3, 3) mat . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10], [ 1, 2, 3, 5, 6, 7, 9, 10, 11], [ 4, 5, 6, 8, 9, 10, 12, 13, 14], [ 5, 6, 7, 9, 10, 11, 13, 14, 15]], dtype=int32) . For multi-channel input we perform the same operation for both the channels and then stack them together to give us X . X1 = patches_to_row(x[0, :, :], kh=3, kw=3) X2 = patches_to_row(x[1, :, :], kh=3, kw=3) . X1 . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10], [ 1, 2, 3, 5, 6, 7, 9, 10, 11], [ 4, 5, 6, 8, 9, 10, 12, 13, 14], [ 5, 6, 7, 9, 10, 11, 13, 14, 15]], dtype=int32) . X2 . array([[16, 17, 18, 20, 21, 22, 24, 25, 26], [17, 18, 19, 21, 22, 23, 25, 26, 27], [20, 21, 22, 24, 25, 26, 28, 29, 30], [21, 22, 23, 25, 26, 27, 29, 30, 31]], dtype=int32) . Now we would like to concatenate 1st row of X1 with 1st row of X2. . X = np.hstack((X1, X2)) X . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10, 16, 17, 18, 20, 21, 22, 24, 25, 26], [ 1, 2, 3, 5, 6, 7, 9, 10, 11, 17, 18, 19, 21, 22, 23, 25, 26, 27], [ 4, 5, 6, 8, 9, 10, 12, 13, 14, 20, 21, 22, 24, 25, 26, 28, 29, 30], [ 5, 6, 7, 9, 10, 11, 13, 14, 15, 21, 22, 23, 25, 26, 27, 29, 30, 31]], dtype=int32) . We need to ensure that number of channels in the convolutional kernel matches up with the input. . np.random.seed(41) w = np.random.random(size=(2, 3, 3)) W = w.reshape(-1, 1) W . array([[0.25092362], [0.04609582], [0.67681624], [0.04346949], [0.1164237 ], [0.60386569], [0.19093066], [0.66851572], [0.91744785], [0.41878009], [0.33225985], [0.28303364], [0.18628227], [0.31711047], [0.48116867], [0.06952047], [0.70498257], [0.31467693]]) . fmap = np.dot(X, W) fmap.reshape(2, 2) . array([[ 88.38632016, 95.0086239 ], [114.87553514, 121.49783888]]) . F.conv2d(tensor(x[None, :, :, :]).float(), tensor(w[None, :, :, :]).float(), bias=None, stride=1 ) . tensor([[[[ 88.3863, 95.0086], [114.8755, 121.4978]]]]) . As we can see the two values match, so far so good. . Multi channel input and multiple filters . In the previous case we considered only 1 filter which had 2 channels and it lead us to a feature map of the same size that we got in case of single input channel, now we can use multiple conv filters to capture different features in the image. . x = np.arange(32, dtype=np.int32).reshape(2, 4, 4) x . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]], dtype=int32) . Now we would like to consider 2 weight filters so our filter&#39;s shape would be (2, 2, 3, 3) representing (kn, C, kh, kw) -&gt; kn number of filters, C number of channels, kh - height of the filter, kw width of filter. . np.random.seed(41) w = np.random.random(size=(2, 2, 3, 3)) w . array([[[[0.25092362, 0.04609582, 0.67681624], [0.04346949, 0.1164237 , 0.60386569], [0.19093066, 0.66851572, 0.91744785]], [[0.41878009, 0.33225985, 0.28303364], [0.18628227, 0.31711047, 0.48116867], [0.06952047, 0.70498257, 0.31467693]]], [[[0.74528235, 0.3982128 , 0.60822646], [0.72845649, 0.42175804, 0.39390821], [0.23222257, 0.4416646 , 0.37302139]], [[0.58360604, 0.10003138, 0.74135188], [0.08319793, 0.12622394, 0.32289153], [0.64292729, 0.99947173, 0.28100165]]]]) . X1 = patches_to_row(x[0, :, :], kh=3, kw=3) X2 = patches_to_row(x[1, :, :], kh=3, kw=3) X = np.hstack((X1, X2)) print(X) print(X.shape) . [[ 0 1 2 4 5 6 8 9 10 16 17 18 20 21 22 24 25 26] [ 1 2 3 5 6 7 9 10 11 17 18 19 21 22 23 25 26 27] [ 4 5 6 8 9 10 12 13 14 20 21 22 24 25 26 28 29 30] [ 5 6 7 9 10 11 13 14 15 21 22 23 25 26 27 29 30 31]] (4, 18) . For convolutional filters first convert them into column vectors then combine them together into W = [W1, W2] . np.random.seed(41) w = np.random.random(size=(2, 2, 3, 3)) W = np.hstack((w[0, :, :, :].reshape(-1, 1), w[1, :, :, :].reshape(-1, 1))) W . array([[0.25092362, 0.74528235], [0.04609582, 0.3982128 ], [0.67681624, 0.60822646], [0.04346949, 0.72845649], [0.1164237 , 0.42175804], [0.60386569, 0.39390821], [0.19093066, 0.23222257], [0.66851572, 0.4416646 ], [0.91744785, 0.37302139], [0.41878009, 0.58360604], [0.33225985, 0.10003138], [0.28303364, 0.74135188], [0.18628227, 0.08319793], [0.31711047, 0.12622394], [0.48116867, 0.32289153], [0.06952047, 0.64292729], [0.70498257, 0.99947173], [0.31467693, 0.28100165]]) . fmap = np.dot(X, W) fmap.T.reshape(2, 2, 2) . array([[[ 88.38632016, 95.0086239 ], [114.87553514, 121.49783888]], [[102.08763737, 110.31109367], [134.98146258, 143.20491888]]]) . F.conv2d(tensor(x[None, :, :, :]).float(), tensor(w).float(), bias=None, stride=1 ) . tensor([[[[ 88.3863, 95.0086], [114.8755, 121.4978]], [[102.0876, 110.3111], [134.9815, 143.2049]]]]) . Checking with the pytorch&#39;s implementation to ensure correctness. . Batch of inputs with multi-channels and multiple convolutional filters. . Extending to batch input would only require us to concatenate the output of the individual inputs in the batch to form X . Let&#39; take an example where we have only 2 inputs in our batch. . x = np.arange(64, dtype=np.int32).reshape(2, 2, 4, 4) x . array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]], [[[32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47]], [[48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]]]], dtype=int32) . X11 = patches_to_row(x[0, 0, :, :], kh=3, kw=3) X12 = patches_to_row(x[0, 1, :, :], kh=3, kw=3) X1 = np.hstack((X11, X12)) X21 = patches_to_row(x[1, 0, :, :], kh=3, kw=3) X22 = patches_to_row(x[1, 1, :, :], kh=3, kw=3) X2 = np.hstack((X21, X22)) X = np.vstack((X1, X2)) X.shape . (8, 18) . Each input produces 4 row vectors so joining the outputs of 2 inputs would yield us a matrix of shape (8, 18). . np.random.seed(41) w = np.random.random(size=(2, 2, 3, 3)) W = np.hstack((w[0, :, :, :].reshape(-1, 1), w[1, :, :, :].reshape(-1, 1))) W . array([[0.25092362, 0.74528235], [0.04609582, 0.3982128 ], [0.67681624, 0.60822646], [0.04346949, 0.72845649], [0.1164237 , 0.42175804], [0.60386569, 0.39390821], [0.19093066, 0.23222257], [0.66851572, 0.4416646 ], [0.91744785, 0.37302139], [0.41878009, 0.58360604], [0.33225985, 0.10003138], [0.28303364, 0.74135188], [0.18628227, 0.08319793], [0.31711047, 0.12622394], [0.48116867, 0.32289153], [0.06952047, 0.64292729], [0.70498257, 0.99947173], [0.31467693, 0.28100165]]) . fmap = np.dot(X, W) fmap.T.reshape(2, 2, 2, 2) . array([[[[ 88.38632016, 95.0086239 ], [114.87553514, 121.49783888]], [[300.30003998, 306.92234373], [326.78925496, 333.4115587 ]]], [[[102.08763737, 110.31109367], [134.98146258, 143.20491888]], [[365.238239 , 373.4616953 ], [398.1320642 , 406.3555205 ]]]]) . F.conv2d(tensor(x).float(), tensor(w).float(), bias=None, stride=1 ) . tensor([[[[ 88.3863, 95.0086], [114.8755, 121.4978]], [[102.0876, 110.3111], [134.9815, 143.2049]]], [[[300.3000, 306.9224], [326.7893, 333.4115]], [[365.2382, 373.4617], [398.1320, 406.3555]]]]) . matching again!! . Using an efficient method in Numpy to perform covolutions. . We have so far seen that extracting patch from an image based on the kernel size and patch requires most of the effort. We naively implemented it with a double loop assuming a stride of size 1 but there is a faster way to get these patches of an input based on the filter size and stride using the method . np.lib.stride_tricks.as_strided . Let&#39;s see how this works out . . Let&#39;s consider your input is X and the convlutional kernel is of size ( 2, 2 ) with stride 1 then matrix should look something like this . . X = np.arange(1, 10, dtype=np.int32).reshape(3, 3) A = np.lib.stride_tricks.as_strided(X, shape=(2, 2, 2, 2), strides=(12, 4, 12, 4)) A . array([[[[1, 2], [4, 5]], [[2, 3], [5, 6]]], [[[4, 5], [7, 8]], [[5, 6], [8, 9]]]], dtype=int32) . Lot&#39;s of stuff to unpack here, let&#39;s start with function signature and see what does individual parameters mean. . np.lib.stride_tricks.as_strided? . np.lib.stride_tricks.as_strided(x, shape=None, strides=None, subok=False, writeable=True) . This method creates a view on the matrix x, where shape and strides are attributes of the view created. We are all familiar with what shape signifies for a matrix but stride is something new. . Numpy officially describes strides as how many bytes we need to jump over in the data buffer to go from one item to the next. So to create our desired view we need to pass it&#39;s shape and strides in the func args. . Let&#39;s try to understand it with an example. . a = np.arange(9, dtype=np.int32).reshape(3,3) print(a) . [[0 1 2] [3 4 5] [6 7 8]] . so for moving from a[i, 0] to a[i, 1] we need to jump 4 bytes since dtype is int32 and across ith dimension to move from a[0, j] to a[1, j] we need to jump 12 bytes, hence strides for a would be (12, 4) . a.strides . (12, 4) . Coming back to original array A how would we determine it&#39;s strides, we could start with the lowest dimension. How many bytes we need to jump when moving from A[i, j, k, l] to A[i, j, k, l+1] and compare it with X. We know moving from previous exercise moving across lowest dimension would require only 4 bytes so strides would be (?, ?, ?, 4), also moving to higher dimension jump required to move from A[i, j, k, l] to A[i, j, k+1, l] would be 12 bytes when comparing with our previous example leading to (?, ?, 12, 4). . . For the next dimension j we need to look at the bytes we need to jump to reach from 1 to 2 marked in the above figure. Essentially we are trying to figure out jump between a[i, j, k, l] to a[i, j+1, k, l]. If we look at the original array a both 1 and 2 are beside each other in 0th row so it would require jump of only 4 bytes, hence strides would be (?, 4, 12, 4) . . For the highest dimension we are looking at jump between 1 and 4 marked in red in the figure above. If we map these numbers to our original array X we see that these two numbers are in the same colunmn but in adjacent rows hence it would take 12 bytes to reach from 1 to 4 hence the stide would be 12, so the final value of strides for the array A would be (12, 4, 12, 4) . Multi-channel inputs . X = np.arange(1, 19, dtype=np.int32).reshape(2, 3, 3) X . array([[[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]], dtype=int32) . If the size of the convlutional kernel is (2, 2) then we will get 8 matrices of size (2, 2) so therefore shape of A would be (2, 2, 2, 2, 2). If you notice it has one more dimension than the previous case and it is because we have added 2 channels instead of one so shape of A would be . A.shape = (X.shape[0], *A.shape) . For stride calcualtion, rest everything is same we have only added an additional channel dimension hence strides value would be (?, 12, 4, 12, 4) so we only need to find the value of highest dimension. . . Let&#39;s look at the strides of the input for reference here . X.strides . (36, 12, 4) . For the highest dimension taking an example here, we want to find the jump between 1 and 10. If we compare the positions of 1 and 10 in X we find out that they are 9 positions apart and to move between a single position it takes around 4 bytes so hence to move 9 positions it would take 36 bytes. . Hence the final values of strides would be (36, 12, 4, 12, 4). If we pay close attention there is a relationship between X.strides and A.strides which is the following . A.strides = (*X.strides[:-2], X.strides[-2]*s, X.strides[-1]*s, *X.strides[-2:]) . where s would be the stride used by the convolution filter to go over the input X. . Next question would be how to determine the shape of A, since X is 4-dim tensor with shape (N, C, H, W) and shape of convolutional kernel being kh and kw, then shape of A could be determined as (N, C, ?, ?, kh, kw). . The output feature map should have height oh and width ow respectively which could be calculated as following . oh = ( H - kh ) // stride + 1 ow = ( W - kw ) // stride + 1 . def get_mat_strides(X, kh, kw, s): N, C, H, W = X.shape oh = ( H - kh ) // s + 1 ow = ( W - kw ) // s + 1 strides = (*X.strides[:-2], X.strides[-2]*s, X.strides[-1]*s, *X.strides[-2:]) A = np.lib.stride_tricks.as_strided(X, shape=(N, C, oh, ow, kh, kw), strides=strides) return A X = np.arange(1, 33, dtype=np.int32).reshape(1, 2, 4, 4) A = get_mat_strides(X, kh=2, kw=2, s=1) A . array([[[[[[ 1, 2], [ 5, 6]], [[ 2, 3], [ 6, 7]], [[ 3, 4], [ 7, 8]]], [[[ 5, 6], [ 9, 10]], [[ 6, 7], [10, 11]], [[ 7, 8], [11, 12]]], [[[ 9, 10], [13, 14]], [[10, 11], [14, 15]], [[11, 12], [15, 16]]]], [[[[17, 18], [21, 22]], [[18, 19], [22, 23]], [[19, 20], [23, 24]]], [[[21, 22], [25, 26]], [[22, 23], [26, 27]], [[23, 24], [27, 28]]], [[[25, 26], [29, 30]], [[26, 27], [30, 31]], [[27, 28], [31, 32]]]]]], dtype=int32) . Numpy provides a method named np.tensordot to peform tensor multiplication. Here we would perform tensor multiplication between A and the convolution filters W. . Let&#39;s consider a single filter matching 2 channels of the input X. . np.random.seed(41) W = np.random.random(size=(1, 2, 2, 2)) W . array([[[[0.25092362, 0.04609582], [0.67681624, 0.04346949]], [[0.1164237 , 0.60386569], [0.19093066, 0.66851572]]]]) . X.shape, A.shape, W.shape . ((1, 2, 4, 4), (1, 2, 3, 3, 2, 2), (1, 2, 2, 2)) . # Shape of filter -&gt; (kn, C, kh, kw) # Shape of feature map -&gt; (N, kn, oh, ow) fmap = np.tensordot(A, W, axes=[(1, 4, 5), (1, 2, 3)]) fmap.transpose(0, 3, 1, 2) # transpose to match the output of `F.conv2d` . array([[[[35.55368844, 38.15072938, 40.74777032], [45.94185221, 48.53889315, 51.1359341 ], [56.33001598, 58.92705693, 61.52409787]]]]) . conv = F.conv2d(tensor(X).float(), tensor(W), bias=None, stride=1) conv . tensor([[[[35.5537, 38.1507, 40.7478], [45.9419, 48.5389, 51.1359], [56.3300, 58.9271, 61.5241]]]]) . As you can see both the values match one from our implmentation&#39;s of convolution using np.lib.stride_tricks.as_strided followed by np.tensordot with F.conv2d. . Note: here we have assumed that X is necessarily padded before. . %timeit -n 1000 np.tensordot(A, W, axes=[(1, 4, 5), (1, 2, 3)]) . 21 µs ± 2.76 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %timeit -n 1000 F.conv2d(tensor(X).float(), tensor(W), bias=None, stride=1) . 39.4 µs ± 4.26 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . It matches with the performance of Pytorch&#39;s implementatio as well. . It is useful to implement the core building blocks of deep learning from multiple ways to better understand what&#39;s happening behind the scenes and gain insights. This exercise gave a different perspective of why convolutions need to be modeled as matrix multiplications to gain the performance speedup of accelerators like GPU. .",
            "url": "https://numb3r33.github.io/experiments/convolution/math/deeplearning/2023/12/23/im2col.html",
            "relUrl": "/convolution/math/deeplearning/2023/12/23/im2col.html",
            "date": " • Dec 23, 2023"
        }
        
    
  
    
        ,"post5": {
            "title": "Residual Learning",
            "content": "What is a residual? . Residual is the difference between actual and estimated value. . What is residual learning? . In the context of ensemble learning, a base model is used to fit the residuals to make the ensemble model more accurate. In deep learning, various architectures use a block/layer to fit the residual to improve the performance of the DNN. . How does Gradient Boosting Machines use residuals? . We will try to deconstruct how GBM works using DecisionTrees on a regression task. . from sklearn import datasets from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt . X, y = datasets.make_regression(n_samples=1000, random_state=41) Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.2, random_state=41) . from sklearn.tree import DecisionTreeRegressor . tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg1.fit(Xtr, ytr) y2 = ytr - tree_reg1.predict(Xtr) tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg2.fit(Xtr, y2) y3 = y2 - tree_reg2.predict(Xtr) tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg3.fit(Xtr, y3) y_pred = sum(tree.predict(Xva) for tree in (tree_reg1, tree_reg2, tree_reg3)) . Gradient Boosting . How does residuals play a part in Gradient Boosting Learning? . Train a base learner tree_reg1 to fit data (X) and labels (y) | Train a base learner tree_reg2 that fits on data (X) and residuals between the label and predicted value of base learner tree_reg1. Essentially, we are using a base learner to learn the residuals. | Finally the result of all the base learners are added to make the final prediction. | . The above code is equivalent to calling the GradientBoostingRegressor with 3 base learners. . from sklearn.ensemble import GradientBoostingRegressor gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=41) gbrt.fit(Xtr, ytr) gb_preds = gbrt.predict(Xva) . sum(y_pred - gb_preds) . -4.554578936222242e-12 . Role of residual learning in training deep networks? . Why do we need ResNets? . Research to develop better architectures which perform better has led researchers to go deeper with a notion that to a certain extent going deeper would yield better performance. . But we realized that going deeper brings problems of its own, model become difficult to train. In 2014, VGG had only 19 layers while in 2015 ResNet had 152 layers and a far better performance, one can say at an initial glance that ResNet wins because it has more number of layers. Ofcourse that is the case but it also introduces a trick called &quot;residual learning&quot; that helps achieve this performance. . CNN models have evolved over time from LeNet-5 ( 5 layers ) and AlexNet ( 8 layers ) to VGGNet (16-19) and later GoogleNet ( 22 layers ). According to experimental results of VGGNet, depth of the network plays a crucial role in model&#39;s performance. . Please find below tables extracted from VGG paper that showcases that deeper we go better the effect. . . . Are deeper networks really better? . Later in various experiments it was found out that model performance increases with depth upto a certain extent further which it often decreases. What could be the reasons for that could it be following . Overfitting. | Vanishing/Exploding Gradients. | . Overfitting . In the Resnet paper the authors tried this following experiment . . The y-axis on the left figure represents training error and the y-axis on the right figure represents test error and x-axes on the both the figures represent the number of iterations. . We can see that the 20-layer network trained for a large number of iterations yields in low training error but corresponding test error is relatively large. This is a case of over-fitting ( we are performing better on training compared to test dataset ). . In addition to this the authors also trained a network with 56-layers and found out that error of this network in both training and testing is large compared to the 20-layer network. Thus performance degradation has nothing to do with overfitting. . Vanishing/Exploding Gradients . Vanishing/Exploding gradients make the model difficult to train but there are already some techniques like Batch Normalization to alleviate this problem. . Let&#39;s try to understand the example presented by author in the paper . Suppose we have a following network which can perform good on training and test datasets. . . Then we augment the architecture in the following way to add more layers. The parameters of the first 4 layers are copied from the above network and these parameters remain unchanged during training. . . In theory the performance of the second network should be better than first network since we have more layers which could extract useful features and suppose we find out that the second network performs worse, then one explanation provided by the authors is that since we have copied the parameters of first 4 layers in the second network and if they are enough to meet the performance requirements then the newly added layers are a bit redundant. To maintain the level of performance, the newly added functions has to serve as an identity mapping that is the net effect of the purple layers should be f(x) = x, in this way we would not experience model degradation. . This is what the authors observed that the non-linear expression of the traditional multi-layer network structure has difficulty expressing the identity mapping which leads to model degradation. . How do we tackle model degradation then? . Assuming that a relatively shallow network can already achieve good results, then even if the network is piled up with more layers the effect of the model should not deteriorate. . In reality, however this is the problem, doing nothing happens to be a very challenging task. . Presence of non-linear activation functions makes the input-to-output process almost irreversible. Non-linearity gives the model endless possibilities but it also makes the network forget the original intention. . The quality of not forgetting the original intention/doing nothing is managed by identity mapping. . Residual Block . . In fact, it is difficult for existing neural networks to fit the underlying identity mapping function H(x) = x.But if the network is designed such that H(x) = F(x) + x, then the identity map could be used as part of the network. . The problem can be transformed into learning a residual function F(x) = H(x) - x. As long as F(x)=0, an identity map H(x) = x is formed. The loop in the figure is called a shortcut connection. By jumping before the activation function, the output of the previous layer or layers is added to the output calculated by this layer, and the result of summation is input to the next activation function as the output of this layer. . The idea of the skip connection is to expressed the output as a linear superposition of a nonlinear transformation of the input and the input. There is no new formula, no new theory, but a new expression. . Why is residual learning relatively easier? . Intuitively, residual learning requires less learning, because residuals are generally relatively small and the learning difficulty is less. However, we can analyze this problem from a mathematical point of view. First, the residual unit can be expressed as: . . $F(x, {W_i })$ is the goal of our learning, that is, the residual of the output and input i.e. $y-x$. If we further expand . . $ sigma$ refers to Relu, while $W_1$, $W_2$ refer to two layers of weights. When $F(x, {W_i })$ learns to have a 0 value then $y = x$, this is what we call identity mapping. . Why can&#39;t we have $y=f(x, {W_i })$ instead and no skip connections? . Because $f(x, {W_i })$ has a ReLU activation function in the middle so if x &lt;= 0 then y = 0 which would violate the identity mapping principle. | . Experiments . . Taken directly from Resnet paper . Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left:plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. . Summary . Shortcut connections/residual connections/skip connections/skip connections, etc. are all one thing, there is no new theory, just a new expression. | Problem of model degradation when deepening the network could be somewhat alleviated using residual learning. | . References . Resnet paper | VGG | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/2022/09/19/residual-learning.html",
            "relUrl": "/deeplearning/math/2022/09/19/residual-learning.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "AEDA ( An Easier Data Augmentation Technique for Text Classification )",
            "content": "Paper resources . paper | code | . Objective . This paper proposes a new data augmentation technique for text classification task. | It also compares the performance of this augmentation technique with EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks and concludes that their method is simpler and produces better results. | In this experiment we will try to implement this data augmentation using fastai on a text classification task. | . Why do we need augmentations? . To have better generalizability, we need more and more comprehensive datasets but collection of these datasets and labelling is a laborious task so augmentation becomes an attractive method to introduce more examples for model to consume. | . What are the different kind of augmentations used in NLP? . For improving machine translation task, researchers have tried substituting common words with rare words thus providing more context for rare words. | Some researchers have tried replacing words with their synonyms for tweet classification. | Randomly swap two words in a sentence. | Randomly delete a word in the sentence and many more. | . What is the novel idea presented in the paper? . AEDA method proposes randomly inserting some punctuation marks as an augmentation to introduce noise. The authors report improvement performance in text classification tasks. . Can you share an example of how this augmentation would work? . Original Text: . Appropriate given recent events . . Augmented Text: . Appropriate given ; recent events . Appropriate ? given : recent events . Appropriate given , recent events . How many punctuation marks are inserted? . Between 1 to n/3 where n represents the length of the sentence. | . Why one-third of the sentence length? . The authors mention that they want to increase the complexity of the sentence but doesn&#39;t want to add too many punctuation marks which would interfere with the semantic meaning of the sentence. . At which positions should we insert these punctuation marks? . The authors inserted them at random positions in the sentence. | . What are the different punctuation marks used? . . ; ? : ! , . Why does AEDA work better compared to EDA? . EDA proposes synonym replacement, random replacement, random insertion and random deletion. These modifications could change the semantic meaning of the text. | Whereas AEDA just introduces punctuation marks which would only introduce noise and would not mess the semantic meaning or the word ordering. | . Implementation . We would be using fastai to implement this augmentation. | The authors have released the code as well which we would using. | . Dataset . We will be using this dataset used in this challenge where the goal is to predict the subreddit of a subreddit post based on their title and their description. This is an example of text categorization / text classification task | . Load libraries . import pandas as pd import numpy as np from pathlib import Path from tqdm import tqdm from fastai.text.all import * SEED = 41 . Define paths and constants . BASE_DIR = Path(&#39;~/data/dl_nlp&#39;) RAW_DATA_PATH = BASE_DIR / &#39;data&#39; OUTPUT_DIR = Path(&#39;~/data/dl_nlp/outputs&#39;) PUNCTUATIONS = [&#39;.&#39;, &#39;,&#39;, &#39;!&#39;, &#39;?&#39;, &#39;;&#39;, &#39;:&#39;] PUNC_RATIO = 0.3 . Load dataset . train = pd.read_csv(RAW_DATA_PATH / &#39;train.csv&#39;) train.head() . . Text column represents title as well as description. | Subreddit column represents our label. | . Class Distribution . train.subreddit.value_counts(normalize=True) . . We have multiple categories that our model needs to get right. | Most of the categories have similar percentage of data points in the dataset, with only SubredditSimulator category having less training examples. | . Splitter . splits = RandomSplitter(seed=41)(train) . Create a splitting strategy. | Here we plan to split our training dataframe randomly into training ( 80% ) and validation ( 20% ) datasets. | . Tokenize the training dataset . df_tok, cnt = tokenize_df(train.iloc[splits[0]], text_cols=&#39;text&#39;) . Fast.ai provides a method to tokenize our dataset. | Here we only passing our training examples as the corpus for tokenizer to create vocabulary. | We could pass in different types of tokenizers here but by default it works with WordTokenizer. | . df_tok . . Here we could see that it has split our text string into tokens and created an additional column called text_length describing the length. | It has also added some library specific tokens like xxbos, xxmaj etc. xxbos represents beginning of the sentence token. For more details please refer to fast.ai | . cnt . . Here is a snapshot of the vocabulary constructed by the tokenize_df method. | . Using fast.ai Pipeline to construct Dataset . text_pipe = Pipeline([attrgetter(&#39;text&#39;), Tokenizer.from_df(0), Numericalize(vocab=list(cnt.keys()))]) lbl_pipe = Pipeline([attrgetter(&#39;subreddit&#39;), Categorize()]) lbl_pipe.setup(train.subreddit) dsets = Datasets(train, [text_pipe, lbl_pipe], splits=splits, dl_type=SortedDL) . Here we use Pipeline provided by fast.ai to put together different transforms we want to run on our dataframe. | text_pipe represents the Pipeline that we would like to run on our text column in the dataframe. | lbl_pipe represents the Pipeline that we would like to run on our subreddit column in the dataframe. | Numericalize transform takes in our vocabulary and converts the tokens to ids. | Categorize transforms converts our labels to categories. | Tokenizer.from_df transform tokenizes the text stored in our dataframe. | . AEDA data augmentation as fast.ai transform . np.random.seed(0) PUNCTUATIONS = [&#39;.&#39;, &#39;,&#39;, &#39;!&#39;, &#39;?&#39;, &#39;;&#39;, &#39;:&#39;] PUNC_RATIO = 0.3 class InsertPunctuation(Transform): split_idx = 0 def __init__(self, o2i, punc_ratio=PUNC_RATIO): self.o2i = o2i self.punc_ratio = punc_ratio def encodes(self, words:TensorText): new_line = [] q = random.randint(1, int(self.punc_ratio * len(words) + 1)) qs = random.sample(range(0, len(words)), q) for j, word in enumerate(words): if j in qs: new_line.append(self.o2i[PUNCTUATIONS[random.randint(0, len(PUNCTUATIONS)-1)]]) new_line.append(int(word)) else: new_line.append(int(word)) return TensorText(new_line) . We have taken the implementation from the github shared by the authors and created a fast.ai tranform that would take in the PUNC_RATIO and o2i as parameters and inserts punctuations at random positions in the sentence. | PUNC_RATIO by default takes a value of 0.3 which represents the 1/3rd of the sentence length mentioned in the paper. | o2i is mapping between token to token_id. | . Construct dataloaders . seq_len = 72 dls_kwargs = { &#39;after_item&#39; : InsertPunctuation(dsets.o2i), &#39;before_batch&#39;: Pad_Chunk(seq_len=seq_len) } dls = dsets.dataloaders(bs=32, seq_len=seq_len, **dls_kwargs) . When creating fast.ai dataloders we could perform operations on some of the events emitted. | Here we have made use of two such events, after_item callback is used to run our augmentation and add punctuation marks. | before_batch callback is used to make sure that we have paddded the tokens to make sure they are of same size before we collate them to form a batch. | . dls.show_batch(max_n=3) . . dls.show_batch gives a glimpse of the batch | . Using the classic TextCNN model introduced by Yoon Kim, paper . class TextCNN(Module): def __init__(self, n_embed, embed_dim, num_filters, filter_sizes, num_classes, dropout=0.5, pad_idx=1): store_attr(&#39;n_embed,embed_dim&#39;) self.embed = nn.Embedding(num_embeddings=n_embed, embedding_dim=embed_dim, padding_idx=1 ) self.convs = nn.ModuleList([ nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embed_dim) ) for k in filter_sizes ]) self.dropout = nn.Dropout(dropout) self.relu = nn.ReLU() self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes) def _conv_and_pool(self, x, conv): x = self.relu(conv(x)).squeeze(3) x = F.max_pool1d(x, x.size(2)).squeeze(2) return x def forward(self, x): out = self.embed(x) out = out.unsqueeze(1) out = torch.cat([self._conv_and_pool(out, conv) for conv in self.convs], 1) out = self.dropout(out) out = self.fc(out) return out . vocab = dls.train_ds.vocab num_classes = get_c(dls) model = TextCNN(len(vocab[0]), embed_dim=300, num_filters=100, filter_sizes=[1, 2, 3], num_classes=num_classes, ) . Define learner . learn = Learner(dls, model, metrics=[accuracy, F1Score(average=&#39;weighted&#39;)]) . Using F1 score weighted metric for multi-class classification. | . learn.fit_one_cycle(n_epoch=25, lr_max=3e-4, cbs=EarlyStoppingCallback(patience=3)) . . We are getting a F1( weighted ) score of 0.869 without using any pre-trained embeddings. | . References . AEDA: An Easier Data Augmentation Technique for Text Classification | AEDA code | EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks | Fast.AI | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/nlp/2022/01/31/aeda-text-augmentation.html",
            "relUrl": "/deeplearning/math/fastai/nlp/2022/01/31/aeda-text-augmentation.html",
            "date": " • Jan 31, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Temporal Convolution Networks",
            "content": "Objective . sequence modelling had become synonymous with recurrent networks. | This paper shows that convolutional networks can outperform recurrent networks on some of the tasks. | paper concludes that common association between sequence modelling and recurrent networks should be reconsidered. | . Temporal Convolution Networks? . A new general architecture for convolutional sequence prediction. | This new general architecture is referred to as Temporal Convolutional Networks abbreviated as TCN. | Convolutions in this architecture are causal which means that there is no information leakage. | Architecture can take in a sequence of arbitrary length and map it to an output sequence of the same length, just like RNNs. ( But tcn achieves this function not through seq2seq but simply using convolutional layers. ) | Also this paper highlights how we could combine deep networks ( with residual structures ) and dilated convolutions could be used to build long term dependencies. ( ability of an model to look back in past to make future predictions ) | . What is a sequence modelling task? . Taken directly from paper . Before defining the network structure, we highlight the nature of the sequence modeling task. Suppose that we are given an input sequence $x_0$, . . . , $x_T$ , and wish to predict some corresponding outputs $y_0$, . . . , $y_T$ at each time. The key constraint is that to predict the output $y_t$ for some time t, we are constrained to only use those inputs that have been previously observed:$x_0$, . . . , $x_t$. Formally, a sequence modeling network is any function f : $X_{T +1}$ → $Y_{T +1}$ that produces the mapping . $y_0$, . . . , $y_T$ = f($x_0$, . . . , $x_T$ ) (1) . if it satisfies the causal constraint that $y_t$ depends only on $x_0$, . . . , $x_t$ and not on any “future” inputs $x_{t+1}$, . . . , $x_T$. The goal of learning in the sequence modeling setting is to find a network f that minimizes some expected loss between the actual outputs and the predictions. . L($y_0$, . . . , $y_T$ , f($x_0$, . . . , $x_T$)), where the sequences and outputs are drawn according to some distribution. . What is a 1D convolution? . Before we jump into the paper we must understand what is a 1D convolution since it is used in the causal convolutional layer in TCN . 1D Convolution takes in a 3D tensor as input and outputs a 3D tensor as output. | Shape of the input tensor in TCN would have following dimension ( batch_size, input_length, input_size ) and the output tensor has shape ( batch_size, input_length, output_size ) | Each layer in TCN has same input and output length so only the third dimension would change. | . . Image courtesy . In the above figure we can notice the follwing . to compute a single output we need to look at 3 consecutive values of the input sequence, it is because we are using a kernel of size 3 here. | to maintain that input and output sequences be of the same size we have to pad the input sequence with zeros on both sides. | 1d convolution is a special case of 2d convolution | . . Image courtesy . How is 1d convolution a special case of 2d convolution? . In both time series and NLP, data is laid out in a similar manner, in the figure above we have embedded the words I like this movie very much ! into a 7 x 5 embedding matrix and then we use 1d convolution on this 2D matrix. . 1d convolution is a special case of 2d convolution where kernel size of the 1d convolution is it&#39;s height. The width of the kernel is defined by the embedding size, which is 5 here and it is fixed. So it means that we can only slide vertically and not horizontally which makes it 1D convolution. . What is causal convolution? . Causality means that an element in the output sequence can only depend on elements that precede it in the input sequence. | In order to ensure that an output tensor has the same length as the input tensor, we need to do zero padding. | If we only pad the left side of the input tensor with zeros, then causal convolution is guaranteed. | $x^{&#39;}_4$ in the figure below is generated by combining $x_2$, $x_3$, $x_4$ which ensures no leakage of information. | . . This operation generates $x^{&#39;}_5$ and $x^{&#39;}_6$ which are extraneous and should be removed before passing the output to the next layer. We have to take care of it in the implementation. . How many zeros would be required to make sure that the output would be of same length as input? (kernel_size - 1) . How it all fits together? . TCN has two basic principles: . input and output length of the sequences remain same. | there can be no leakage from the past. | . To achieve the first point TCN makes use of 1D FCN ( Fully Convolutional Network ) and to achieve the second point TCN makes use of causal convolutions. . Disadvantages of the above architecture . To model long term dependencies, we need a very deep network or very large convolution kernels, neither of which turned out to be particularly feasible in the experiments. | . Dilated Convolutions . A desirable quality of a the model is that the value of a particular entry in the output depends on all previous entries in the input. . | This is achieved when the size of the receptive field is equal to the length of the input. . | We could expand our receptive field when we stack multiple layers together. In the figure below we can see that by stacking two layers with kernel_size 3, we get a receptive field size of 5. . | . . In general, the receptive field r of a 1D convolutional network with n layers and kernel_size k is . $r = 1 + n * ( k - 1 )$ . To know how many layers are needed for full coverage, we can set the receptive field size to input_length l and solve for the number of layers n (non-integer values need to be rounded): . $ lceil frac{(l-1)}{(k-1)} rceil$ . This means that, with a fixed kernel_size, the number of layers required for complete coverage would be linear in input length. This will cause the network to become very deep and very fast, resulting in models with a large number of parameters that take longer to train. . How could we solve this issue? . One way to increase the size of the receptive field while keeping the number of layers relatively small is to introduce the concept of dilation. . Dilation in the context of convolutional layers refers to the distance between elements of the input sequence that are used to compute one entry of the output sequence. Therefore, a traditional convolutional layer can be viewed as a layer dilated by 1, because the input elements involved in calculating output value are adjacent. . The image below shows an example of how the receptive field grows when we introduce dilation. The right side image uses a dilation rate r 1 in the first layer with kernel_size 3 which is how a traditional conv layer would work although in the next layer we use r=2 which makes sure that we combine input elements that are 2 elements apart when producing output for the next layer and so on. . . To overcome the problem of number of layers required for covering the entire input length we must progressively increase the dilation rate over multiple layers. . This problem can be solved by exponentially increasing the value of d as we move up in the layer. To do this, we choose a constant dilation_base integer b that will allow us to calculate the dilation d for a particular layer based on the number of layers i under it, i.e. $d = b^i$. . The figure below shows a network with an input_length of 10, a kernel_size of 3, and a dilation_base of 2, which would result in a complete coverage of the 3 dilated convolutional layers. . . Here we can see that the all input values are used to produce the last value in the output layer. With the above mentioned setup we could have an input of length 15 while maintaining the full coverage. . How did we calculate that the receptive width is 15? . When a layer is added to the architecture the receptive field is increased by $d*(k-1)$ | So if we have n layers with kernel_size k and dilation base rate as b then receptive width is calculated as | . $w=1+(k-1) frac{b^n-1}{b-1}$ . but depending on values of b and k the architecture could have many holes in it. . What does that mean? . . Here we can see not all inputs are used to compute the last value of the output, even though w is greater than the input size. To fix this we would have to either increase the kernel size or decrease the dilation rate from 3 to 2. In general we must ensure that kernel_size is atleast equal to dilation rate to avoid such cases. . How many layers would be required for full coverage? . Given a kernel size k, a dilation base b where k ≥ b, and an input length l, in order to achieve full coverage following condition must be satisfied . $1+(k-1) frac{b^n-1}{b-1} geq l$, then . $n= lceil log_b( frac{(l-1)*(b-1)}{k-1}+1) rceil$ . Now number of layers is lograthmic in input layer length l which is what we wanted. This is a significant improvement that can be achieved without sacrificing receptive field coverage. . Now, the only thing that needs to be specified is the number of zero-padded items required for each layer. Assuming that the dilation expansion base is b, the kernel size is k, and there are i layers below the current layer, the number p of zero-padding items required by the current layer is calculated as follows: . $p=b^i*(k-1)$ . Temporal Residual Block . Now let&#39;s discuss the basic building blocks of TCN network. . . Residual links have proven to be an effective way to train deep networks, which allow the network to pass information in a cross-layer manner. | This paper constructs a residual block to replace one layer of convolution. As shown in the figure above, a residual block contains two layers of convolution and nonlinear mapping, and WeightNorm and Dropout are added to each layer to regularize the network. | . . Each hidden layer has the same length as the input layer, and is padded with zeros to ensure subsequent layers have the same length. . | For the output at time t, the causal convolution (convolution with causal constraints) uses the input at time t and the previous layer at an earlier time (see the blue line connection at the bottom of the figure above). . | Causal convolution is not a new idea, but the paper incorporates very deep networks to allow for long-term efficient histories. . | . Residual Link . . Residual blocks (originally from ResNet) have repeatedly shown to benefit very deep networks. | Since the receptive field of a TCN depends on the network depth n as well as the convolution kernel size k and dilation factor d, it becomes very important to stabilize deeper and larger TCNs. | Predictions may depend on long historical values and high-dimensional input sequences. e.g An input sequence of size $2^{12}$ may require a network of up to 12 layers. | In standard ResNet, the input is directly added to the output of the residual function, while in TCN the input and output can have different widths. To account for the difference in input-output width, an additional 1x1 convolution is used to ensure that element-wise addition receives tensors of the same shape. | . . Conclusion . The innovation in the TCN model is to sort out how to use causal and dilated convolutions to solve the sequence modelling task. | Causal and Dilated convolutions have already been proposed earlier but this paper highlights how they could be combined together for sequence modelling tasks | . Advantages . Parallelism. When given a sentence, TCN can process the sentence in parallel without the need for sequential processing like RNN. . | Flexible receptive field. The size of the receptive field of TCN is determined by the number of layers, the size of the convolution kernel, and the expansion coefficient. It can be flexibly customized according to different characteristics of different tasks. . | Stable gradient. RNN often has the problems of vanishing gradients and gradient explosion, which are mainly caused by sharing parameters in different time periods. Like traditional convolutional neural networks, TCN does not have the problem of gradient disappearance and explosion. . | Lower memory requirements. When RNN is used, it needs to save the information of each step, which will occupy a lot of memory. The convolution kernel of TCN is shared in one layer, and hence lower memory usage. . | . Disadvantages . TCN may not be so adaptable in transfer learning. This is because the amount of historical information required for model predictions may be different in different domains. Therefore, when migrating a model from a problem that requires less memory information to a problem that requires longer memory, TCN may perform poorly because its receptive field is not large enough. . | The TCN described in the paper is also a one-way structure. In tasks such as speech recognition and speech synthesis, the pure one-way structure is quite useful. However, most of the texts use a bidirectional structure. Of course, it is easy to expand the TCN into a bidirectional structure. Instead of using causal convolution, the traditional convolution structure can be used. . | TCN is a variant of convolutional neural network after all. Although the receptive field can be expanded by using dilated convolution, it is still limited. Compared with Transformer, it is still poor in capturing relevant information of any length. The application of TCN to text remains to be tested. . | . Tips for implementation . Next we would highlight things to keep in mind if you plan to implement the paper . After the convolution, the size of the output data after the convolution is greater than the size of the input data | This is caused owing to padding both sides, so we chomp off extra padded 0s from right side to get the desired data values. | . We have taken this (https://github.com/locuslab/TCN/blob/2221de3323/TCN/tcn.py) implementation of TCN and implemented in fast.ai and tsai to demonstrate how TCN could be used for sequence modelling. . How to prepare dataset? . from tsai.all import * computer_setup() . . We are going to select appliances energy dataset recently released by Monash, UEA &amp; UCR Time Series Extrinsic Regression Repository (2020) . dsid = &#39;AppliancesEnergy&#39; X, y, splits = get_regression_data(dsid, split_data=False) X.shape, y.shape, y[:10] . . check_data(X, y, splits) . . tfms = [None, [TSRegression()]] batch_tfms = TSStandardize(by_sample=True, by_var=True) dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=128) dls.one_batch() . . dls.show_batch() . . Model implementation . from fastai.torch_basics import * from fastai.tabular.core import * from torch.nn.utils import weight_norm . class Chomp1d(Module): def __init__(self, chomp_size): store_attr() def forward(self, x): return x[:, :, :-self.chomp_size].contiguous() def get_conv_block(n_inputs, n_outputs, kernel_size, stride, padding, dilation, dropout): conv = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation )) chomp = Chomp1d(padding) relu = nn.ReLU() drop = nn.Dropout(dropout) return nn.Sequential(*(conv, chomp, relu, drop )) class TemporalBlock(Module): def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.5): store_attr() self.in_conv_blk = get_conv_block(n_inputs, n_outputs, kernel_size, stride, padding, dilation, dropout ) self.out_conv_blk = get_conv_block(n_outputs, n_outputs, kernel_size, stride, padding, dilation, dropout ) self.net = nn.Sequential(*(self.in_conv_blk, self.out_conv_blk)) self.downsample_conv = nn.Conv1d(n_inputs, n_outputs, kernel_size=1) if n_inputs != n_outputs else None self.relu = nn.ReLU() self.init_weights() def init_weights(self): # 0 index represents the convolutional layer self.in_conv_blk[0].weight.data.normal_(0, 0.01) self.out_conv_blk[0].weight.data.normal_(0, 0.01) if self.downsample_conv is not None: self.downsample_conv.weight.data.normal_(0, 0.01) def forward(self, x): out = self.net(x) res = x if self.downsample_conv is None else self.downsample_conv(x) return self.relu(out + res) class TemporalConvNet(Module): def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2): layers = [] num_levels = len(num_channels) for i in range(num_levels): dilation_size = 2 ** i in_channels = num_inputs if i == 0 else num_channels[i-1] out_channels = num_channels[i] layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=(kernel_size-1) * dilation_size, dropout=dropout ) ] self.network = nn.Sequential(*layers) def forward(self, x): return self.network(x) class TCN(Module): def __init__(self, input_size, output_size, num_channels, kernel_size, dropout): self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout) self.linear = nn.Linear(num_channels[-1], output_size) self.init_weights() def init_weights(self): self.linear.weight.data.normal_(0, 0.01) def forward(self, x): y1 = self.tcn(x) return self.linear(y1[:, :, -1]) . model = TCN(input_size=24, output_size=1, num_channels=[24, 32, 64], kernel_size=2, dropout=0.2 ) learn = Learner(dls, model, metrics=[mae, rmse], cbs=ShowGraph()) learn.lr_find() . . model = TCN(input_size=24, output_size=1, num_channels=[24, 32, 64], kernel_size=2, dropout=0.2 ) learn = Learner(dls, model, metrics=[mae, rmse], cbs=ShowGraph()) learn.fit_one_cycle(100, 8e-4) . . References . An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling | Fast.AI | tsai | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/tsai/sequencemodelling/2021/12/22/temporal-convolutional-networks.html",
            "relUrl": "/deeplearning/math/fastai/tsai/sequencemodelling/2021/12/22/temporal-convolutional-networks.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Do we need downsampling?",
            "content": "Objective . Deep CNN architecutures are made up of many components like conv layer, activation function, pooling layers, batch-norm layer etc.All of them are designed for specific reasons and to better understand the effect of these components it is important to play with them. For example, we include layers like pooling, strided convolution etc to reduce the size of the input. . If we look at the architecture of VGG below, we see that lot of max_pooling layers are used. The idea is to increase the receptive field and decrease the number of parameters by reducing the size of the input. But then a question arises, do we really need to downsample? Luckily I came across this repository where this specific question is addressed and the author has tried to replace downsampling layers with dilated convolutions or large kernels with appropriate padding to address this issue. The purpose of this post is to implement and validate these ideas by performing the above mentioned experiments on CIFAR-10 dataset using fastai. The intention is to show how easy it is for us to experiment using fastai. . . Libraries . let&#39;s start by installing fastai2 and keras . !pip install fastai2 keras &gt; /dev/null . from fastai2.vision.all import * from keras.datasets import cifar10 . Dataset . We are going to train our network on CIFAR-10 dataset. CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. . (x_train,y_train),(x_test,y_test) = cifar10.load_data() . Datasets API . This is where fastai comes in with its flexible api to form a dataset that is easily consumable by the models. We are passing a list of pairs of (image, label) to the pipeline. No data augmentation is applied. To validate our ideas we will keep a separate holdout set. If you want to understand more about Datasets api please read official docs . items = np.array(list(zip(x_train, y_train.ravel()))) # 80-20 percent split splits = RandomSplitter(seed=42)(items) tfms = [[lambda x: x[0], PILImage.create], [lambda x: x[1], Categorize()]] dsets = Datasets(items, tfms, splits=splits) dls = dsets.dataloaders(bs=64, after_item=[ToTensor(), IntToFloatTensor()]) . dls.show_batch(figsize=(4, 4)) . . VGG ( 4 layer ) network . Let&#39;s try to train a 4 layer VGG based network with downsampling and see the performance on CIFAR-10. . class VGG_4(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG_4, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=16, out_channels=24, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=24, out_channels=32, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=48, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . learn = Learner(dls, VGG_4(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . Let&#39;s train it for 30 epochs . learn.fit_one_cycle(30, 1e-2) . . learner.summary() . . VGG with dilated convolution . Idea is to progressively increase the dilation from 1, 2, 4, 8 to increase the receptive field of the network. . class VGG4_Dilation(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG4_Dilation, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3), dilation=1), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.Conv2d(in_channels=16, out_channels=24, padding=(1, 1), kernel_size=(3, 3), dilation=2), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.Conv2d(in_channels=24, out_channels=32, padding=(1, 1), kernel_size=(3, 3), dilation=4), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(in_channels=32, out_channels=48, padding=(1, 1), kernel_size=(3, 3), dilation=8), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . In the head of the model instead of using a fully connected layer we are using AdaptiveAvgPool2d with 1x1 convolution layer, it is because researchers have observed that using AdaptiveAvgPool2d with 1x1 convolution layer decreases the total number of parameters without taking a hit on the performance. . learn = Learner(dls, VGG4_Dilation(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . learn.fit_one_cycle(30, 1e-2) . . learn.summary() . . Note: There is no change in number of trainable in params in vanilla vgg 4 layer model with dowsampling and vgg 4 layer model with dilation. . VGG with large kernels . We plan to progressively increase the size of the kernels from 3 to 9. Increasing the kernel size would enable us to increase the receptive field of the network but we have to make sure that we use adequate padding so as to not shrink our input. . class VGG4_large_filter(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG4_large_filter, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.Conv2d(in_channels=16, out_channels=24, padding=(2, 2), kernel_size=(5, 5)), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.Conv2d(in_channels=24, out_channels=32, padding=(3, 3), kernel_size=(7, 7)), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(in_channels=32, out_channels=48, padding=(4, 4), kernel_size=(9, 9)), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . learn = Learner(dls, VGG4_large_filter(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . learn.fit_one_cycle(30, 5e-3) . . learn.summary() . . Even though there is an improvement in terms of loss but number of trainable parameters have jumped from 25,474 to 172, 930 . Conclusion . If we look at the performance of 4 layer VGG network with downsampling and compare it with dilated and large kernel size, we observe that their is an increase in the performance. | Using a large kernel will improve performance but at the cost of increased number of trainable parameters. | Using dilation would improve performance without increasing the number of trainable parameters | . Next steps . To make it generalizable, we would have to test this idea on other architectures e.g. (resnet18) and see if it increases performance or not. | Also it would be interesting to see what kind of features will our models learn if we use dilation. | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/fastai/cnn/2020/05/02/Do-we-need-downsampling.html",
            "relUrl": "/deeplearning/fastai/cnn/2020/05/02/Do-we-need-downsampling.html",
            "date": " • May 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Machine Learning Engineer with 6+ years of experience in machine learning, deep learning and MLOPs. Presently working in Gojek on building a Data Science Platform on top of Kubeflow to enable DS teams to take models to production faster. | Working on a real-time feature store to manage data pipelines and features required for running real-time ML models in production. | Prior to moving into building the platform worked as data scientist in user-growth team in Gopay ( financial arm of Gojek ) to build models for estimating Customer Lifetime Value to help business target and cater to user needs in a better way. We have been working on a model that has helped organisation champion organic growth and other growth efforts. This model has helped organisation bring down cost spent on promotions by 22% without taking hit on growth efforts. | Worked as data scientist in Gopay to build models for fraud and account takeover detection ( ATO ). This model predicts whether an incoming request is an ATO or not. This model has been running in real-time setting and has resulted in 33% reduction in tickets. | Before joining Gojek, worked in DeltaX and worked on Data Driven Attribution where we created data-driven models by borrowing ideas from Game Theory like Ordered Shapley Value to attribute conversions tracked by our system which served as building block for another model that takes into account converting and non-converting user journeys to calculate the importance of a campaign for the advertiser. It helped advertisers get better understanding of their audiences. | Also worked on budget distribution problem where we helped develop an algorithm that would distribute budgets among competing ad-groups running on Facebook while optimizing the business objective of the advertiser. This is a big step up from traditional approaches which mostly look at single indicator and it helped advertisers plan better. | . | Mentor at Scaler academy where mentored lots of Machine Learning enthusiasts on how to take their career forward and setup Machine Learning infrastructure, models and practices in their respective organizations. | Open Source contributor at numb3r33 | Interested in competitive machine learning Winner of Deep Learning Hackathon organized by HackerEarth (https://www.hackerearth.com/challenges/competitive/deep-learning-beginner-challenge/leaderboard/). | Ranked in Top-100 data scientists on Analytics Vidhya platform which has around 3500 competitors at the time of writing. | . | Interested in blogging about machine learning, deep learning and mathematics, you can read my posts here | . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://numb3r33.github.io/experiments/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://numb3r33.github.io/experiments/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}