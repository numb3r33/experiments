{
  
    
        "post0": {
            "title": "Creating a Maze Solver using Pix2Pix",
            "content": "The experiment is simple but powerful: generate a dataset of maze images paired with their solutions, then train a Pix2Pix model to translate from &#39;unsolved maze&#39; to &#39;solved maze.&#39; Unlike traditional algorithms that traverse paths step-by-step, this neural network would learn to &#39;see&#39; the entire solution at once. . Dataset Generation . I created a custom maze generator using a depth-first search algorithm with backtracking. Each maze was represented as a binary image where black pixels (0) represented walls and white pixels (255) represented paths. Start and end points were marked with distinct grayscale values. For each maze, I also generated the correct solution path using breadth-first search, marking the path with a medium gray color. . # This script generates a dataset of maze images and their solutions # for training the pix2pix GAN model import numpy as np import matplotlib.pyplot as plt from PIL import Image, ImageDraw import random from pathlib import Path import argparse import time # Constants WALL = 0 # Black PATH = 255 # White START = 200 # Light gray END = 150 # Darker gray SOLUTION = 100 # Medium gray . def generate_maze(width, height): &quot;&quot;&quot;Generate a maze using Depth-First Search with backtracking. Args: width: Width of the maze (in cells) height: Height of the maze (in cells) Returns: A 2D numpy array representing the maze &quot;&quot;&quot; # Initialize maze with walls maze = np.zeros((height * 2 + 1, width * 2 + 1), dtype=np.uint8) # Create a grid to track visited cells visited = np.zeros((height, width), dtype=bool) # Stack for backtracking stack = [] # Start at a random cell x, y = random.randint(0, width - 1), random.randint(0, height - 1) visited[y, x] = True stack.append((x, y)) # Possible directions: right, down, left, up directions = [(1, 0), (0, 1), (-1, 0), (0, -1)] # DFS to carve passages while stack: x, y = stack[-1] # Find unvisited neighbors neighbors = [] for dx, dy in directions: nx, ny = x + dx, y + dy if 0 &lt;= nx &lt; width and 0 &lt;= ny &lt; height and not visited[ny, nx]: neighbors.append((nx, ny, dx, dy)) if neighbors: # Choose a random neighbor nx, ny, dx, dy = random.choice(neighbors) # Remove wall between current cell and chosen neighbor maze[2*y + 1 + dy, 2*x + 1 + dx] = PATH # Mark cell as visited and add to stack visited[ny, nx] = True maze[2*ny + 1, 2*nx + 1] = PATH stack.append((nx, ny)) else: # Backtrack stack.pop() # Make sure the outer walls are solid maze[0, :] = WALL maze[-1, :] = WALL maze[:, 0] = WALL maze[:, -1] = WALL # Make all paths white maze[maze &gt; 0] = PATH return maze def mark_start_end(maze): &quot;&quot;&quot;Mark start and end points on the maze. Args: maze: 2D numpy array representing the maze Returns: The maze with start and end points marked &quot;&quot;&quot; # Find all path cells path_cells = np.where(maze == PATH) path_indices = list(zip(path_cells[0], path_cells[1])) # Choose start point (prefer top-left area) start_candidates = [p for p in path_indices if p[0] &lt; maze.shape[0]//3 and p[1] &lt; maze.shape[1]//3] if not start_candidates: start_candidates = path_indices start_y, start_x = random.choice(start_candidates) # Choose end point (prefer bottom-right area) end_candidates = [p for p in path_indices if p[0] &gt; 2*maze.shape[0]//3 and p[1] &gt; 2*maze.shape[1]//3] if not end_candidates: end_candidates = [p for p in path_indices if p != (start_y, start_x)] end_y, end_x = random.choice(end_candidates) # Mark start and end maze[start_y, start_x] = START maze[end_y, end_x] = END return maze, (start_y, start_x), (end_y, end_x) def solve_maze(maze, start, end): &quot;&quot;&quot;Solve the maze using Breadth-First Search. Args: maze: 2D numpy array representing the maze start: (y, x) coordinates of the start point end: (y, x) coordinates of the end point Returns: A copy of the maze with the solution path marked &quot;&quot;&quot; # Create a copy of the maze solution = maze.copy() # Queue for BFS queue = [(start, [start])] visited = {start} # Directions: up, right, down, left directions = [(-1, 0), (0, 1), (1, 0), (0, -1)] while queue: (y, x), path = queue.pop(0) # Check if we&#39;ve reached the end if (y, x) == end: # Mark the solution path for py, px in path: if (py, px) != start and (py, px) != end: solution[py, px] = SOLUTION return solution # Try each direction for dy, dx in directions: ny, nx = y + dy, x + dx # Check if the new position is valid if (0 &lt;= ny &lt; maze.shape[0] and 0 &lt;= nx &lt; maze.shape[1] and maze[ny, nx] != WALL and (ny, nx) not in visited): queue.append(((ny, nx), path + [(ny, nx)])) visited.add((ny, nx)) # No solution found print(&quot;No solution found for the maze!&quot;) return solution def create_binary_image(arr, size=256): &quot;&quot;&quot;Convert a numpy array to a binary PIL Image with specified size. Args: arr: 2D numpy array size: Target size for the image Returns: PIL Image object &quot;&quot;&quot; # Create PIL image img = Image.fromarray(arr.astype(np.uint8)) # Resize to target size img = img.resize((size, size), Image.NEAREST) return img def generate_dataset(num_mazes, output_dir, size=256, maze_size=(25, 25)): &quot;&quot;&quot;Generate a dataset of mazes and their solutions. Args: num_mazes: Number of mazes to generate output_dir: Directory to save the images size: Size of the output images maze_size: Size of the maze in cells (width, height) &quot;&quot;&quot; # Create output directories output_dir = Path(output_dir) maze_dir = output_dir / &quot;mazes&quot; solution_dir = output_dir / &quot;solutions&quot; maze_dir.mkdir(parents=True, exist_ok=True) solution_dir.mkdir(parents=True, exist_ok=True) for i in range(num_mazes): print(f&quot;Generating maze {i+1}/{num_mazes}...&quot;) # Generate maze maze = generate_maze(maze_size[0], maze_size[1]) # Mark start and end points maze, start, end = mark_start_end(maze) # Solve maze solution = solve_maze(maze, start, end) # Create images maze_img = create_binary_image(maze, size) solution_img = create_binary_image(solution, size) # Save images maze_img.save(maze_dir / f&quot;maze_{i:04d}.png&quot;) solution_img.save(solution_dir / f&quot;solution_{i:04d}.png&quot;) print(f&quot;Dataset generation complete. {num_mazes} mazes created in {output_dir}&quot;) def visualize_maze_pair(maze_path, solution_path): &quot;&quot;&quot;Visualize a maze and its solution side by side.&quot;&quot;&quot; maze_img = Image.open(maze_path) solution_img = Image.open(solution_path) plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plt.imshow(maze_img, cmap=&#39;gray&#39;) plt.title(&quot;Original Maze&quot;) plt.axis(&#39;off&#39;) plt.subplot(1, 2, 2) plt.imshow(solution_img, cmap=&#39;gray&#39;) plt.title(&quot;Solution&quot;) plt.axis(&#39;off&#39;) plt.tight_layout() plt.show() def process_existing_dataset(input_dir, output_dir, size=256): &quot;&quot;&quot;Process an existing dataset of mazes to prepare for pix2pix training. This function assumes the input directory contains raw maze images, and will generate solutions for them. Args: input_dir: Directory containing maze images output_dir: Directory to save processed images size: Target size for output images &quot;&quot;&quot; input_dir = Path(input_dir) output_dir = Path(output_dir) maze_dir = output_dir / &quot;mazes&quot; solution_dir = output_dir / &quot;solutions&quot; maze_dir.mkdir(parents=True, exist_ok=True) solution_dir.mkdir(parents=True, exist_ok=True) # Get all maze images maze_files = list(input_dir.glob(&quot;*.png&quot;)) + list(input_dir.glob(&quot;*.jpg&quot;)) for i, maze_file in enumerate(maze_files): print(f&quot;Processing maze {i+1}/{len(maze_files)}: {maze_file.name}&quot;) # Load maze image maze_img = Image.open(maze_file).convert(&#39;L&#39;) # Convert to grayscale maze_img = maze_img.resize((size, size), Image.NEAREST) # Convert to numpy array maze_array = np.array(maze_img) # Threshold to create binary maze (assuming white paths, black walls) threshold = 128 maze_binary = np.zeros_like(maze_array) maze_binary[maze_array &gt; threshold] = PATH # Try to identify start and end points # This is a simplification - you may need a more sophisticated approach start = tuple(np.array(np.where(maze_binary == PATH))[:, 0]) end_y, end_x = np.where(maze_binary == PATH) end = (end_y[-1], end_x[-1]) if len(end_y) &gt; 0 else start # Mark start and end maze_binary[start] = START maze_binary[end] = END # Solve maze solution = solve_maze(maze_binary, start, end) # Save processed images maze_img = create_binary_image(maze_binary, size) solution_img = create_binary_image(solution, size) maze_img.save(maze_dir / f&quot;{maze_file.stem}.png&quot;) solution_img.save(solution_dir / f&quot;{maze_file.stem}.png&quot;) print(f&quot;Dataset processing complete. {len(maze_files)} mazes processed in {output_dir}&quot;) . def generate_maze(width, height): &quot;&quot;&quot;Generate a maze using Depth-First Search with backtracking. Args: width: Width of the maze (in cells) height: Height of the maze (in cells) Returns: A 2D numpy array representing the maze &quot;&quot;&quot; # Initialize maze with walls maze = np.zeros((height * 2 + 1, width * 2 + 1), dtype=np.uint8) # Create a grid to track visited cells visited = np.zeros((height, width), dtype=bool) # Stack for backtracking stack = [] # Start at a random cell x, y = random.randint(0, width - 1), random.randint(0, height - 1) visited[y, x] = True stack.append((x, y)) # Possible directions: right, down, left, up directions = [(1, 0), (0, 1), (-1, 0), (0, -1)] # DFS to carve passages while stack: x, y = stack[-1] # Find unvisited neighbors neighbors = [] for dx, dy in directions: nx, ny = x + dx, y + dy if 0 &lt;= nx &lt; width and 0 &lt;= ny &lt; height and not visited[ny, nx]: neighbors.append((nx, ny, dx, dy)) if neighbors: # Choose a random neighbor nx, ny, dx, dy = random.choice(neighbors) # Remove wall between current cell and chosen neighbor maze[2*y + 1 + dy, 2*x + 1 + dx] = PATH # Mark cell as visited and add to stack visited[ny, nx] = True maze[2*ny + 1, 2*nx + 1] = PATH stack.append((nx, ny)) else: # Backtrack stack.pop() # Make sure the outer walls are solid maze[0, :] = WALL maze[-1, :] = WALL maze[:, 0] = WALL maze[:, -1] = WALL # Make all paths white maze[maze &gt; 0] = PATH return maze def mark_start_end(maze): &quot;&quot;&quot;Mark start and end points on the maze. Args: maze: 2D numpy array representing the maze Returns: The maze with start and end points marked &quot;&quot;&quot; # Find all path cells path_cells = np.where(maze == PATH) path_indices = list(zip(path_cells[0], path_cells[1])) # Choose start point (prefer top-left area) start_candidates = [p for p in path_indices if p[0] &lt; maze.shape[0]//3 and p[1] &lt; maze.shape[1]//3] if not start_candidates: start_candidates = path_indices start_y, start_x = random.choice(start_candidates) # Choose end point (prefer bottom-right area) end_candidates = [p for p in path_indices if p[0] &gt; 2*maze.shape[0]//3 and p[1] &gt; 2*maze.shape[1]//3] if not end_candidates: end_candidates = [p for p in path_indices if p != (start_y, start_x)] end_y, end_x = random.choice(end_candidates) # Mark start and end maze[start_y, start_x] = START maze[end_y, end_x] = END return maze, (start_y, start_x), (end_y, end_x) def solve_maze(maze, start, end): &quot;&quot;&quot;Solve the maze using Breadth-First Search. Args: maze: 2D numpy array representing the maze start: (y, x) coordinates of the start point end: (y, x) coordinates of the end point Returns: A copy of the maze with the solution path marked &quot;&quot;&quot; # Create a copy of the maze solution = maze.copy() # Queue for BFS queue = [(start, [start])] visited = {start} # Directions: up, right, down, left directions = [(-1, 0), (0, 1), (1, 0), (0, -1)] while queue: (y, x), path = queue.pop(0) # Check if we&#39;ve reached the end if (y, x) == end: # Mark the solution path for py, px in path: if (py, px) != start and (py, px) != end: solution[py, px] = SOLUTION return solution # Try each direction for dy, dx in directions: ny, nx = y + dy, x + dx # Check if the new position is valid if (0 &lt;= ny &lt; maze.shape[0] and 0 &lt;= nx &lt; maze.shape[1] and maze[ny, nx] != WALL and (ny, nx) not in visited): queue.append(((ny, nx), path + [(ny, nx)])) visited.add((ny, nx)) # No solution found print(&quot;No solution found for the maze!&quot;) return solution def create_binary_image(arr, size=256): &quot;&quot;&quot;Convert a numpy array to a binary PIL Image with specified size. Args: arr: 2D numpy array size: Target size for the image Returns: PIL Image object &quot;&quot;&quot; # Create PIL image img = Image.fromarray(arr.astype(np.uint8)) # Resize to target size img = img.resize((size, size), Image.NEAREST) return img def generate_dataset(num_mazes, output_dir, size=256, maze_size=(25, 25)): &quot;&quot;&quot;Generate a dataset of mazes and their solutions. Args: num_mazes: Number of mazes to generate output_dir: Directory to save the images size: Size of the output images maze_size: Size of the maze in cells (width, height) &quot;&quot;&quot; # Create output directories output_dir = Path(output_dir) maze_dir = output_dir / &quot;mazes&quot; solution_dir = output_dir / &quot;solutions&quot; maze_dir.mkdir(parents=True, exist_ok=True) solution_dir.mkdir(parents=True, exist_ok=True) for i in range(num_mazes): print(f&quot;Generating maze {i+1}/{num_mazes}...&quot;) # Generate maze maze = generate_maze(maze_size[0], maze_size[1]) # Mark start and end points maze, start, end = mark_start_end(maze) # Solve maze solution = solve_maze(maze, start, end) # Create images maze_img = create_binary_image(maze, size) solution_img = create_binary_image(solution, size) # Save images maze_img.save(maze_dir / f&quot;maze_{i:04d}.png&quot;) solution_img.save(solution_dir / f&quot;solution_{i:04d}.png&quot;) print(f&quot;Dataset generation complete. {num_mazes} mazes created in {output_dir}&quot;) def visualize_maze_pair(maze_path, solution_path): &quot;&quot;&quot;Visualize a maze and its solution side by side.&quot;&quot;&quot; maze_img = Image.open(maze_path) solution_img = Image.open(solution_path) plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plt.imshow(maze_img, cmap=&#39;gray&#39;) plt.title(&quot;Original Maze&quot;) plt.axis(&#39;off&#39;) plt.subplot(1, 2, 2) plt.imshow(solution_img, cmap=&#39;gray&#39;) plt.title(&quot;Solution&quot;) plt.axis(&#39;off&#39;) plt.tight_layout() plt.show() def process_existing_dataset(input_dir, output_dir, size=256): &quot;&quot;&quot;Process an existing dataset of mazes to prepare for pix2pix training. This function assumes the input directory contains raw maze images, and will generate solutions for them. Args: input_dir: Directory containing maze images output_dir: Directory to save processed images size: Target size for output images &quot;&quot;&quot; input_dir = Path(input_dir) output_dir = Path(output_dir) maze_dir = output_dir / &quot;mazes&quot; solution_dir = output_dir / &quot;solutions&quot; maze_dir.mkdir(parents=True, exist_ok=True) solution_dir.mkdir(parents=True, exist_ok=True) # Get all maze images maze_files = list(input_dir.glob(&quot;*.png&quot;)) + list(input_dir.glob(&quot;*.jpg&quot;)) for i, maze_file in enumerate(maze_files): print(f&quot;Processing maze {i+1}/{len(maze_files)}: {maze_file.name}&quot;) # Load maze image maze_img = Image.open(maze_file).convert(&#39;L&#39;) # Convert to grayscale maze_img = maze_img.resize((size, size), Image.NEAREST) # Convert to numpy array maze_array = np.array(maze_img) # Threshold to create binary maze (assuming white paths, black walls) threshold = 128 maze_binary = np.zeros_like(maze_array) maze_binary[maze_array &gt; threshold] = PATH # Try to identify start and end points # This is a simplification - you may need a more sophisticated approach start = tuple(np.array(np.where(maze_binary == PATH))[:, 0]) end_y, end_x = np.where(maze_binary == PATH) end = (end_y[-1], end_x[-1]) if len(end_y) &gt; 0 else start # Mark start and end maze_binary[start] = START maze_binary[end] = END # Solve maze solution = solve_maze(maze_binary, start, end) # Save processed images maze_img = create_binary_image(maze_binary, size) solution_img = create_binary_image(solution, size) maze_img.save(maze_dir / f&quot;{maze_file.stem}.png&quot;) solution_img.save(solution_dir / f&quot;{maze_file.stem}.png&quot;) print(f&quot;Dataset processing complete. {len(maze_files)} mazes processed in {output_dir}&quot;) . Example 5x5 maze . . Pix2Pix Architecture . Pix2Pix is a conditional GAN architecture designed for image-to-image translation tasks. It consists of two main components: a U-Net generator that actually produces the maze solutions, and a PatchGAN discriminator that learns to distinguish between real and fake solutions. The generator takes an unsolved maze as input and attempts to produce its solution, while the discriminator evaluates how realistic these solutions appear compared to the ground truth. . Training Procedure . I trained the model on two distinct datasets: simpler 5×5 mazes and more complex 10×10 mazes. Each training run involved 50 epochs with a batch size of 8. To balance the different aspects of learning, I used a combination of adversarial loss (to make solutions look realistic) and L1 loss (to ensure accuracy in the solution paths). . import os import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from fastai.vision.all import * from fastcore.all import * from pathlib import Path import matplotlib.pyplot as plt # Set random seeds for reproducibility torch.manual_seed(42) np.random.seed(42) # Step 1: Define the U-Net Generator Architecture class UNetBlock(Module): def __init__(self, in_channels, out_channels, use_dropout=False): super().__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False) self.norm = nn.BatchNorm2d(out_channels) self.relu = nn.LeakyReLU(0.2, inplace=True) self.use_dropout = use_dropout self.dropout = nn.Dropout(0.5) if use_dropout else None def forward(self, x): x = self.conv(x) x = self.norm(x) x = self.relu(x) if self.use_dropout: x = self.dropout(x) return x class UNetUpBlock(Module): def __init__(self, in_channels, out_channels, use_dropout=False): super().__init__() self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False) self.norm = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.use_dropout = use_dropout self.dropout = nn.Dropout(0.5) if use_dropout else None def forward(self, x, skip_connection): x = self.upconv(x) x = self.norm(x) x = self.relu(x) if self.use_dropout: x = self.dropout(x) x = torch.cat([x, skip_connection], dim=1) return x class UNetGenerator(Module): def __init__(self, in_channels=1, out_channels=1): super().__init__() # Initial layer without normalization self.initial = nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1, bias=False) self.initial_relu = nn.LeakyReLU(0.2, inplace=True) # Downsampling layers self.down1 = UNetBlock(64, 128) self.down2 = UNetBlock(128, 256) self.down3 = UNetBlock(256, 512) self.down4 = UNetBlock(512, 512) self.down5 = UNetBlock(512, 512) self.down6 = UNetBlock(512, 512) self.down7 = UNetBlock(512, 512, use_dropout=False) # No normalization in bottleneck # Upsampling layers self.up1 = UNetUpBlock(512, 512, use_dropout=True) self.up2 = UNetUpBlock(1024, 512, use_dropout=True) self.up3 = UNetUpBlock(1024, 512, use_dropout=True) self.up4 = UNetUpBlock(1024, 512) self.up5 = UNetUpBlock(1024, 256) self.up6 = UNetUpBlock(512, 128) self.up7 = UNetUpBlock(256, 64) # Final layer self.final = nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1) self.final_activation = nn.Tanh() def forward(self, x): # Downsampling d1 = self.initial_relu(self.initial(x)) d2 = self.down1(d1) d3 = self.down2(d2) d4 = self.down3(d3) d5 = self.down4(d4) d6 = self.down5(d5) d7 = self.down6(d6) bottleneck = self.down7(d7) # Upsampling u1 = self.up1(bottleneck, d7) u2 = self.up2(u1, d6) u3 = self.up3(u2, d5) u4 = self.up4(u3, d4) u5 = self.up5(u4, d3) u6 = self.up6(u5, d2) u7 = self.up7(u6, d1) # Final output = self.final(u7) output = self.final_activation(output) return output # Step 2: Define the PatchGAN Discriminator class PatchGANDiscriminator(Module): def __init__(self, in_channels=2): # Input is concatenated maze and solution super().__init__() self.in_channels = in_channels # Store in_channels for debugging self.initial = nn.Sequential( nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True) ) self.block1 = nn.Sequential( nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True) ) self.block2 = nn.Sequential( nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True) ) self.block3 = nn.Sequential( nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1, bias=False), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True) ) self.final = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1) def forward(self, maze, solution): # Add debugging to help identify any shape mismatches if maze.shape[1] != 1 or solution.shape[1] != 1: print(f&quot;Warning: Unexpected channel counts. Maze: {maze.shape}, Solution: {solution.shape}&quot;) # Concatenate maze and solution along channel dimension x = torch.cat([maze, solution], dim=1) # Check channel count after concatenation if x.shape[1] != self.in_channels: print(f&quot;Warning: Channel mismatch. Expected {self.in_channels}, got {x.shape[1]}&quot;) x = self.initial(x) x = self.block1(x) x = self.block2(x) x = self.block3(x) x = self.final(x) return x # Step 3: Create Custom Loss Functions class GANLoss(Module): def __init__(self, gan_mode=&#39;vanilla&#39;, real_label=1.0, fake_label=0.0): super().__init__() self.register_buffer(&#39;real_label&#39;, torch.tensor(real_label)) self.register_buffer(&#39;fake_label&#39;, torch.tensor(fake_label)) if gan_mode == &#39;vanilla&#39;: self.loss = nn.BCEWithLogitsLoss() elif gan_mode == &#39;lsgan&#39;: self.loss = nn.MSELoss() else: raise NotImplementedError(f&#39;GAN mode {gan_mode} not implemented&#39;) def get_target_tensor(self, prediction, target_is_real): if target_is_real: target_tensor = self.real_label else: target_tensor = self.fake_label return target_tensor.expand_as(prediction) def __call__(self, prediction, target_is_real): target_tensor = self.get_target_tensor(prediction, target_is_real) return self.loss(prediction, target_tensor) # Step 4: Create Pix2Pix Model class Pix2PixModel(Module): def __init__(self, in_channels=1, out_channels=1, lambda_L1=100.0): super().__init__() self.generator = UNetGenerator(in_channels, out_channels) self.discriminator = PatchGANDiscriminator(in_channels + out_channels) self.gan_loss = GANLoss(&#39;vanilla&#39;) self.l1_loss = nn.L1Loss() self.lambda_L1 = lambda_L1 def forward(self, maze): return self.generator(maze) def generator_loss(self, fake_solution, real_solution, fake_pred): # Adversarial loss g_loss = self.gan_loss(fake_pred, True) # L1 loss l1_loss = self.l1_loss(fake_solution, real_solution) * self.lambda_L1 return g_loss + l1_loss, g_loss, l1_loss def discriminator_loss(self, real_pred, fake_pred): # Real loss real_loss = self.gan_loss(real_pred, True) # Fake loss fake_loss = self.gan_loss(fake_pred, False) # Total loss return (real_loss + fake_loss) * 0.5 # Step 5: Modified GANTrainer with loss tracking class GANTrainer(Callback): def __init__(self, lambda_L1=100.0, visualize_every=1, save_dir=None): super().__init__() self.lambda_L1 = lambda_L1 self.gen_losses = [] self.disc_losses = [] self.adv_losses = [] self.l1_losses = [] self.epoch_losses = {&#39;g_loss&#39;: [], &#39;d_loss&#39;: [], &#39;g_adv&#39;: [], &#39;g_l1&#39;: []} self.visualize_every = visualize_every # Visualize every N epochs self.save_dir = save_dir # Directory to save visualizations def before_fit(self): # Set up optimizers self.gen_opt = torch.optim.Adam(self.model.generator.parameters(), lr=2e-4, betas=(0.5, 0.999)) self.disc_opt = torch.optim.Adam(self.model.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999)) # Create save directory if specified if self.save_dir is not None: os.makedirs(self.save_dir, exist_ok=True) def before_epoch(self): # Reset per-epoch loss trackers self.epoch_g_loss = 0.0 self.epoch_d_loss = 0.0 self.epoch_g_adv = 0.0 self.epoch_g_l1 = 0.0 self.batch_count = 0 def before_batch(self): # Reset gradients (only during training) if self.training: self.gen_opt.zero_grad() self.disc_opt.zero_grad() def after_pred(self): # Generate fake solutions using generator self.learn.pred = self.model.generator(self.xb[0]) def after_loss(self): # Debug input shapes maze_shape = self.xb[0].shape real_solution_shape = self.yb[0].shape fake_solution_shape = self.learn.pred.shape # Skip actual training in validation mode if not self.training: # Just calculate losses for metrics with torch.no_grad(): # Real pairs real_pred = self.model.discriminator(self.xb[0], self.yb[0]) # Fake pairs fake_pred = self.model.discriminator(self.xb[0], self.learn.pred) # Calculate losses disc_loss = self.model.discriminator_loss(real_pred, fake_pred) gen_loss, adv_loss, l1_loss = self.model.generator_loss(self.learn.pred, self.yb[0], fake_pred) # Store losses for validation metrics self.learn.loss = gen_loss # No need for loss_grad in validation self.learn.loss_grad = None self.learn.losses = { &#39;g_loss&#39;: gen_loss.item(), &#39;d_loss&#39;: disc_loss.item(), &#39;g_adv&#39;: adv_loss.item(), &#39;g_l1&#39;: l1_loss.item() } return # Training mode - continue with normal training loop # Train discriminator first self.disc_opt.zero_grad() # Detach fake predictions to avoid gradient flow to generator fake_sol_detached = self.learn.pred.detach() # Real pairs real_pred = self.model.discriminator(self.xb[0], self.yb[0]) # Fake pairs fake_pred = self.model.discriminator(self.xb[0], fake_sol_detached) # Calculate loss disc_loss = self.model.discriminator_loss(real_pred, fake_pred) # Backward and update disc_loss.backward() self.disc_opt.step() # Train generator next with a completely fresh computation graph self.gen_opt.zero_grad() # Generate new fake predictions for G fake_sol_for_gen = self.model.generator(self.xb[0]) # Calculate discriminator output on these new predictions fake_pred_for_gen = self.model.discriminator(self.xb[0], fake_sol_for_gen) # Calculate generator loss gen_loss, adv_loss, l1_loss = self.model.generator_loss(fake_sol_for_gen, self.yb[0], fake_pred_for_gen) # Backward and update gen_loss.backward() self.gen_opt.step() # Accumulate losses for epoch average self.epoch_g_loss += gen_loss.item() self.epoch_d_loss += disc_loss.item() self.epoch_g_adv += adv_loss.item() self.epoch_g_l1 += l1_loss.item() self.batch_count += 1 # Track individual losses self.gen_losses.append(gen_loss.item()) self.disc_losses.append(disc_loss.item()) self.adv_losses.append(adv_loss.item()) self.l1_losses.append(l1_loss.item()) # Store losses for logging self.learn.loss = gen_loss.detach() # Store detached loss for metrics # Create a dummy tensor for fastai&#39;s backward pass that requires no gradient dummy_loss = torch.tensor(0.0, device=self.xb[0].device, requires_grad=True) self.learn.loss_grad = dummy_loss # Store other losses for logging self.learn.losses = { &#39;g_loss&#39;: gen_loss.item(), &#39;d_loss&#39;: disc_loss.item(), &#39;g_adv&#39;: adv_loss.item(), &#39;g_l1&#39;: l1_loss.item() } def after_epoch(self): # Calculate average losses for the epoch if self.batch_count &gt; 0: avg_g_loss = self.epoch_g_loss / self.batch_count avg_d_loss = self.epoch_d_loss / self.batch_count avg_g_adv = self.epoch_g_adv / self.batch_count avg_g_l1 = self.epoch_g_l1 / self.batch_count # Store epoch average losses self.epoch_losses[&#39;g_loss&#39;].append(avg_g_loss) self.epoch_losses[&#39;d_loss&#39;].append(avg_d_loss) self.epoch_losses[&#39;g_adv&#39;].append(avg_g_adv) self.epoch_losses[&#39;g_l1&#39;].append(avg_g_l1) print(f&quot;Epoch {self.epoch}: G_loss: {avg_g_loss:.4f}, D_loss: {avg_d_loss:.4f}, G_adv: {avg_g_adv:.4f}, G_L1: {avg_g_l1:.4f}&quot;) # Visualize progress after specified epochs if training if self.training and (self.epoch % self.visualize_every == 0 or self.epoch == self.n_epoch-1): print(f&quot;Visualizing progress at epoch {self.epoch}...&quot;) if self.save_dir: show_progress(self.learn, epoch=self.epoch, save_dir=self.save_dir) else: show_progress(self.learn, epoch=self.epoch) # Override backward to prevent double backpropagation def before_backward(self): # Skip the actual backward pass since we already did it raise CancelBackwardException() def plot_losses(self): &quot;&quot;&quot;Plot the loss curves for generator and discriminator.&quot;&quot;&quot; epochs = range(1, len(self.epoch_losses[&#39;g_loss&#39;]) + 1) plt.figure(figsize=(12, 8)) plt.subplot(2, 2, 1) plt.plot(epochs, self.epoch_losses[&#39;g_loss&#39;], label=&#39;Generator Loss&#39;) plt.plot(epochs, self.epoch_losses[&#39;d_loss&#39;], label=&#39;Discriminator Loss&#39;) plt.title(&#39;Generator and Discriminator Losses&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() plt.subplot(2, 2, 2) plt.plot(epochs, self.epoch_losses[&#39;g_adv&#39;], label=&#39;Adversarial Loss&#39;) plt.plot(epochs, self.epoch_losses[&#39;g_l1&#39;], label=&#39;L1 Loss&#39;) plt.title(&#39;Generator Loss Components&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() plt.subplot(2, 2, 3) plt.plot(self.gen_losses, label=&#39;Generator&#39;) plt.title(&#39;Generator Loss per Batch&#39;) plt.xlabel(&#39;Batch&#39;) plt.ylabel(&#39;Loss&#39;) plt.subplot(2, 2, 4) plt.plot(self.disc_losses, label=&#39;Discriminator&#39;) plt.title(&#39;Discriminator Loss per Batch&#39;) plt.xlabel(&#39;Batch&#39;) plt.ylabel(&#39;Loss&#39;) plt.tight_layout() plt.show() # Function to show progress after each epoch def show_progress(learner, n=4, epoch=None, save_dir=None): &quot;&quot;&quot;Show original mazes, ground truth solutions, and predicted solutions.&quot;&quot;&quot; # Get a batch from the validation dataloader batch = learner.dls.valid.one_batch() x, y = batch # Only take the first n items x = x[:n] y = y[:n] fig, axes = plt.subplots(n, 3, figsize=(12, 4*n)) for i in range(n): # Get maze and solution from batch maze = x[i] solution = y[i] # Original maze - denormalize from [-1,1] to [0,1] maze_img = (maze * 0.5 + 0.5).cpu() axes[i, 0].imshow(maze_img.permute(1, 2, 0), cmap=&#39;gray&#39;) axes[i, 0].set_title(&quot;Original Maze&quot;) axes[i, 0].axis(&#39;off&#39;) # Ground truth solution - denormalize from [-1,1] to [0,1] solution_img = (solution * 0.5 + 0.5).cpu() axes[i, 1].imshow(solution_img.permute(1, 2, 0), cmap=&#39;gray&#39;) axes[i, 1].set_title(&quot;Ground Truth Solution&quot;) axes[i, 1].axis(&#39;off&#39;) # Predicted solution with torch.no_grad(): pred_solution = learner.model.generator(maze.unsqueeze(0))[0].cpu() # Convert from [-1,1] to [0,1] range pred_solution = (pred_solution * 0.5 + 0.5) axes[i, 2].imshow(pred_solution.permute(1, 2, 0), cmap=&#39;gray&#39;) axes[i, 2].set_title(&quot;Predicted Solution&quot;) axes[i, 2].axis(&#39;off&#39;) plt.tight_layout() # Save the figure if save_dir is provided if save_dir is not None and epoch is not None: os.makedirs(save_dir, exist_ok=True) plt.savefig(f&quot;{save_dir}/epoch_{epoch:03d}.png&quot;) plt.close() else: plt.show() # Step 6: Modified Pix2PixLearner with progress visualization method class Pix2PixLearner(Learner): def predict(self, maze_img): &quot;&quot;&quot;Generate solution for a single maze image.&quot;&quot;&quot; self.model.eval() with torch.no_grad(): # Convert to tensor and normalize if isinstance(maze_img, PILImage): tensor_img = PILToTensor(maze_img).float() / 127.5 - 1.0 # Normalize to [-1, 1] else: tensor_img = maze_img.float() / 127.5 - 1.0 if len(tensor_img.shape) == 3: # Add batch dimension if needed tensor_img = tensor_img.unsqueeze(0) return self.model.generator(tensor_img) def visualize_progress(self, n=4): &quot;&quot;&quot;Visualize the current model&#39;s performance.&quot;&quot;&quot; show_progress(self, n=n) def plot_training_losses(self): &quot;&quot;&quot;Plot the training losses.&quot;&quot;&quot; # Find the GANTrainer callback gan_trainer = None for cb in self.cbs: if isinstance(cb, GANTrainer): gan_trainer = cb break if gan_trainer is not None: gan_trainer.plot_losses() else: print(&quot;GANTrainer callback not found.&quot;) # Step 7: Generating or Loading Dataset def prepare_maze_dataset(maze_dir, solution_dir, size=256): &quot;&quot;&quot;Create Dataset from directories containing maze and solution images.&quot;&quot;&quot; # Print paths for debugging print(f&quot;Maze directory: {maze_dir}&quot;) print(f&quot;Solution directory: {solution_dir}&quot;) maze_files = get_image_files(maze_dir) print(f&quot;Found {len(maze_files)} maze files&quot;) maze_ids = [f.stem for f in maze_files] # Ensure matching solution files exist solution_files = [Path(solution_dir)/f&quot;{id_.replace(&#39;maze_&#39;, &#39;solution_&#39;)}.png&quot; for id_ in maze_ids] valid_pairs = [(maze, sol) for maze, sol in zip(maze_files, solution_files) if sol.exists()] print(f&quot;Found {len(valid_pairs)} matching maze-solution pairs&quot;) if len(valid_pairs) &lt; len(maze_files): print(f&quot;Warning: Found only {len(valid_pairs)} matching maze-solution pairs out of {len(maze_files)} mazes.&quot;) # Define transforms to ensure consistent dimensions and normalization item_tfms = [Resize(size, method=&#39;pad&#39;, pad_mode=&#39;zeros&#39;)] # Create DataBlock - explicitly specify that both inputs are grayscale images maze_dblock = DataBlock( blocks=(ImageBlock(cls=PILImageBW), ImageBlock(cls=PILImageBW)), get_items=lambda _: valid_pairs, get_x=ItemGetter(0), get_y=ItemGetter(1), item_tfms=item_tfms, batch_tfms=[Normalize.from_stats(0.5, 0.5)] ) return maze_dblock # Step 8: Modified Training Function with improved visualization and tuning options def train_maze_solver(maze_dir, solution_dir, batch_size=4, num_epochs=50, lr=2e-4, lambda_L1=100.0, save_progress=True, visualize_every=1): &quot;&quot;&quot;Train the maze solver model with detailed progress visualization. Args: maze_dir: Directory containing maze images solution_dir: Directory containing solution images batch_size: Batch size for training num_epochs: Number of epochs to train lr: Learning rate lambda_L1: Weight for L1 loss term save_progress: Whether to save progress images visualize_every: Visualize progress every N epochs &quot;&quot;&quot; # Prepare dataset maze_dblock = prepare_maze_dataset(maze_dir, solution_dir) dls = maze_dblock.dataloaders(Path(&#39;.&#39;), bs=batch_size) # Log shape information for debugging print(&quot;Checking batch shapes:&quot;) try: # Try to get one batch x, y = dls.one_batch() print(f&quot;Input maze batch shape: {x.shape}&quot;) print(f&quot;Target solution batch shape: {y.shape}&quot;) except Exception as e: print(f&quot;Error getting one batch: {e}&quot;) # Fallback: try to get first batch from training dataloader for x, y in dls.train: print(f&quot;Input maze batch shape: {x.shape}&quot;) print(f&quot;Target solution batch shape: {y.shape}&quot;) break # Create model with correct channel counts based on the data model = Pix2PixModel(in_channels=1, out_channels=1, lambda_L1=lambda_L1) # Create a directory to save progress images progress_dir = &quot;training_progress&quot; if save_progress: os.makedirs(progress_dir, exist_ok=True) else: progress_dir = None # Create learner with GAN trainer with visualization capability learn = Pix2PixLearner(dls, model, loss_func=lambda x, y: torch.tensor(0., device=x.device), cbs=[GANTrainer(lambda_L1=lambda_L1, visualize_every=visualize_every, save_dir=progress_dir)]) # Run learning rate finder before training (optional) # learn.lr_find() # Train model print(f&quot;Starting training with lambda_L1={lambda_L1}, lr={lr}, batch_size={batch_size}...&quot;) learn.fit_one_cycle(num_epochs, lr) # Plot final losses learn.plot_training_losses() return learn # Step 9: Visualization Functions def show_maze_solutions(learner, n=4): &quot;&quot;&quot;Show original mazes, ground truth solutions, and predicted solutions.&quot;&quot;&quot; # Get a batch from the validation dataloader instead of using sample batch = learner.dls.valid.one_batch() x, y = batch # Only take the first n items x = x[:n] y = y[:n] fig, axes = plt.subplots(n, 3, figsize=(12, 4*n)) for i in range(n): # Get maze and solution from batch maze = x[i] solution = y[i] # Original maze - denormalize from [-1,1] to [0,1] maze_img = (maze * 0.5 + 0.5).cpu() axes[i, 0].imshow(maze_img.permute(1, 2, 0), cmap=&#39;gray&#39;) axes[i, 0].set_title(&quot;Original Maze&quot;) axes[i, 0].axis(&#39;off&#39;) # Ground truth solution - denormalize from [-1,1] to [0,1] solution_img = (solution * 0.5 + 0.5).cpu() axes[i, 1].imshow(solution_img.permute(1, 2, 0), cmap=&#39;gray&#39;) axes[i, 1].set_title(&quot;Ground Truth Solution&quot;) axes[i, 1].axis(&#39;off&#39;) # Predicted solution with torch.no_grad(): pred_solution = learner.model.generator(maze.unsqueeze(0))[0].cpu() # Convert from [-1,1] to [0,1] range pred_solution = (pred_solution * 0.5 + 0.5) axes[i, 2].imshow(pred_solution.permute(1, 2, 0), cmap=&#39;gray&#39;) axes[i, 2].set_title(&quot;Predicted Solution&quot;) axes[i, 2].axis(&#39;off&#39;) plt.tight_layout() plt.show() # Function to create a grid comparing different hyperparameter settings def create_comparison_grid(results, base_dir=&quot;.&quot;): &quot;&quot;&quot;Create a grid visualization comparing the best hyperparameter settings. Args: results: List of result dictionaries from hyperparameter_search base_dir: Base directory where experiment folders are located &quot;&quot;&quot; n_results = len(results) if n_results == 0: return # Find the last saved progress images in each experiment directory example_images = [] for result in results: dir_path = Path(base_dir) / result[&#39;directory&#39;] image_files = sorted(list(dir_path.glob(&quot;epoch_*.png&quot;))) if image_files: example_images.append(str(image_files[-1])) # Take the last image else: # If no epoch images found, look for other visualizations other_images = list(dir_path.glob(&quot;*.png&quot;)) if other_images: example_images.append(str(other_images[0])) else: example_images.append(None) # Create a summary grid fig, axes = plt.subplots(n_results, 1, figsize=(15, 5*n_results)) if n_results == 1: axes = [axes] # Ensure axes is a list for consistency for i, (result, img_path) in enumerate(zip(results, example_images)): # Add text with hyperparameter info title = (f&quot;Rank {i+1}: lambda_L1={result[&#39;lambda_L1&#39;]}, &quot; f&quot;lr={result[&#39;lr&#39;]}, batch_size={result[&#39;batch_size&#39;]} n&quot; f&quot;G_loss: {result[&#39;final_g_loss&#39;]:.4f}, D_loss: {result[&#39;final_d_loss&#39;]:.4f}&quot;) axes[i].set_title(title) axes[i].axis(&#39;off&#39;) # Display the example image if available if img_path and Path(img_path).exists(): img = plt.imread(img_path) axes[i].imshow(img) else: axes[i].text(0.5, 0.5, &quot;No image available&quot;, horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;) plt.tight_layout() plt.savefig(f&quot;{base_dir}/hyperparameter_comparison.png&quot;) plt.close() . maze_dir = Path(&quot;/kaggle/input/maze-dataset-pix2pix-training/mazes&quot;) solution_dir = Path(&quot;/kaggle/input/maze-dataset-pix2pix-training/solutions&quot;) # Train model learn = train_maze_solver(maze_dir, solution_dir, batch_size=8, num_epochs=50, lr=2e-4, lambda_L1=100) # Save model learn.save(&quot;maze_solver_model&quot;) # Visualize results show_maze_solutions(learn, n=4) . Results and Observations . The results revealed something fascinating about neural network learning capabilities. For simpler 5×5 mazes, the model performed remarkably well, generating solutions nearly indistinguishable from the ground truth. The network learned to identify valid paths and correctly trace routes from start to end. However, when faced with more complex 10×10 mazes, the model&#39;s performance noticeably deteriorated. While it still understood the general concept of path-finding, its solutions became less precise. In several instances, the predicted paths contained discontinuities or slight deviations from the optimal route. . . Analysis and Observations . This performance gap illustrates a fundamental challenge in neural network learning: as the problem space grows, so does the complexity of the required understanding. For larger mazes, the network needs to grasp longer-range dependencies and more complex spatial relationships. What&#39;s particularly interesting is that the network isn&#39;t simply failing on larger mazes - it&#39;s still identifying major portions of the correct paths. This suggests it has learned generalizable concepts about maze-solving rather than simply memorizing solutions. . Exmamples of solutions for 5x5 maze . . Examples of solutions for 10x10 maze . . Future Directions . There are several promising directions to enhance this approach: . Architecture scaling: Deeper networks with more capacity might better handle complex mazes | Curriculum learning: Training first on simple mazes, then gradually introducing more complex ones | Hybrid approaches: Combining neural predictions with traditional pathfinding algorithms. | . References . https://arxiv.org/abs/1611.07004 | .",
            "url": "https://numb3r33.github.io/experiments/math/deeplearning/gan/2025/03/29/pix2pix-maze-solver.html",
            "relUrl": "/math/deeplearning/gan/2025/03/29/pix2pix-maze-solver.html",
            "date": " • Mar 29, 2025"
        }
        
    
  
    
        ,"post1": {
            "title": "Why Batch Size Matters - The Surprising Difference Between Batch Training and Averaging",
            "content": "Let&#39;s look at this interesting question that appears simple but reveals profound insights about optimization in deep learning: . 1.1 Approach 1: Batch Size 256 . Pick 256 images randomly: $$B = {(x_1, y_1), ldots, (x_{256}, y_{256}) }$$ . Compute the average gradient: $$ nabla L_B( theta) = frac{1}{256} sum_{i=1}^{256} nabla ell(f(x_i; theta), y_i)$$ . Update once: $$ theta_{ text{new}} = theta_{ text{old}} - eta nabla L_B( theta_{ text{old}})$$ . This is one step using a mini-batch of 256. . 1.2 Approach 2: Batch Size 1, 256 Times, Then Average . Use the same 256 images. . For each image $(x_i, y_i)$, start from the same initial weights $ theta_{ text{old}}$: . Compute the gradient: $$ nabla ell(f(x_i; theta_{ text{old}}), y_i)$$ . Update: $$ theta_i = theta_{ text{old}} - eta nabla ell(f(x_i; theta_{ text{old}}), y_i)$$ . After 256 updates, average the weights: $$ theta_{ text{avg}} = frac{1}{256} sum_{i=1}^{256} theta_i$$ . Set the model&#39;s weights to $ theta_{ text{avg}}$. . 1.3 Are They the Same? . Let&#39;s compute $ theta_{ text{avg}}$: . $$ theta_i = theta_{ text{old}} - eta nabla ell(f(x_i; theta_{ text{old}}), y_i)$$ . $$ theta_{ text{avg}} = frac{1}{256} sum_{i=1}^{256} theta_i = frac{1}{256} sum_{i=1}^{256} ( theta_{ text{old}} - eta nabla ell(f(x_i; theta_{ text{old}}), y_i))$$ . $$= theta_{ text{old}} - eta left( frac{1}{256} sum_{i=1}^{256} nabla ell(f(x_i; theta_{ text{old}}), y_i) right)$$ . $$= theta_{ text{old}} - eta nabla L_B( theta_{ text{old}})$$ . This looks identical to Approach 1! So why do they differ in practice? Let&#39;s question this. . 2. Why Aren&#39;t They Equivalent? . 2.1 Assumption: Linear Gradients . The equivalence holds if the gradient $ nabla ell(f(x_i; theta), y_i)$ is linear in $ theta$. Let&#39;s test this with a simple example: a quadratic loss, $ ell( theta, y_i) = frac{1}{2}( theta - y_i)^2$, where $ theta$ is a scalar. . Gradient: $ nabla ell( theta, y_i) = theta - y_i$ . Approach 1: $$ nabla L_B( theta_{ text{old}}) = frac{1}{256} sum_{i=1}^{256} ( theta_{ text{old}} - y_i) = theta_{ text{old}} - bar{y}, quad bar{y} = frac{1}{256} sum_{i=1}^{256} y_i$$ . $$ theta_{ text{new}} = theta_{ text{old}} - eta ( theta_{ text{old}} - bar{y}) = (1 - eta) theta_{ text{old}} + eta bar{y}$$ . Approach 2: $$ theta_i = theta_{ text{old}} - eta ( theta_{ text{old}} - y_i) = (1 - eta) theta_{ text{old}} + eta y_i$$ . $$ theta_{ text{avg}} = frac{1}{256} sum_{i=1}^{256} ((1 - eta) theta_{ text{old}} + eta y_i) = (1 - eta) theta_{ text{old}} + eta left( frac{1}{256} sum_{i=1}^{256} y_i right) = (1 - eta) theta_{ text{old}} + eta bar{y}$$ . They&#39;re the same! For linear gradients, the order of operations (averaging gradients vs. averaging weights) doesn&#39;t matter. . 2.2 Neural Networks Are Nonlinear . In a neural network, $f(x; theta)$ involves layers with nonlinear activations (e.g., ReLU: $ max(0,z)$). The loss $ ell(f(x_i; theta),y_i)$ is a complex, nonlinear function of $ theta$. The gradient $ nabla ell$ changes with $ theta$, so $ nabla ell(f(x_i; theta_{ text{old}}+ delta),y_i) neq nabla ell(f(x_i; theta_{ text{old}}),y_i)$. Our proof assumed all gradients are computed at $ theta_{ text{old}}$, which holds for Approach 2 as coded, but in practice, frameworks might do more. . 2.3 Optimizers with State . Real optimizers (e.g., SGD with momentum, Adam) have memory: . SGD with Momentum: $$v = mu v + nabla L_B( theta), quad theta_{ text{new}} = theta_{ text{old}} - eta v$$ . $v$ (velocity) accumulates past gradients. In Approach 1, $v$ updates once. In Approach 2, if the optimizer state isn&#39;t reset each time, $v$ accumulates over 256 steps, even though weights are reset. . Adam: Uses moving averages of gradients and their squares, which persist across calls unless reset. . 2.4 Batch Normalization . Batch normalization (BN) normalizes layer outputs: . $$ hat{z} = frac{z - mu_B}{ sqrt{ sigma_B^2 + epsilon}}, quad mu_B = frac{1}{B} sum_{i=1}^{B}z_i, quad sigma_B^2 = frac{1}{B} sum_{i=1}^{B}(z_i - mu_B)^2$$ . During training, $ mu_B$ and $ sigma_B^2$ are batch-specific. BN also tracks moving averages for inference. With batch size 1, these statistics are noisy and updated 256 times, differing from one update with batch size 256. . Experiments . from fastai.vision.all import * import copy # Set random seed for reproducibility set_seed(42) # Define a simple model architecture without batch normalization def create_simple_model(): return nn.Sequential( nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(64 * 7 * 7, 128), nn.ReLU(), nn.Linear(128, 10) ) # Define a similar model with batch normalization def create_bn_model(): return nn.Sequential( nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(64 * 7 * 7, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Linear(128, 10) ) def run_experiment(use_batch_norm=False, momentum=0.0, learning_rate=0.1): &quot;&quot;&quot; Run an experiment comparing batch training vs individual training with averaging Args: use_batch_norm (bool): Whether to use batch normalization in the model momentum (float): Momentum value for SGD optimizer learning_rate (float): Learning rate for the optimizer &quot;&quot;&quot; print(f&quot; n Experiment: BatchNorm={use_batch_norm}, Momentum={momentum}, LR={learning_rate} &quot;) # Create the model based on configuration model = create_bn_model() if use_batch_norm else create_simple_model() # Get MNIST data path = untar_data(URLs.MNIST) mnist_block = DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(train_name=&#39;training&#39;, valid_name=&#39;testing&#39;), get_y=parent_label, item_tfms=[ToTensor()], batch_tfms=[] ) # Create DataLoaders dls = mnist_block.dataloaders(path, bs=256) # Get a fixed batch of 256 examples to ensure consistency across experiments fixed_batch = dls.train.one_batch() x_batch, y_batch = fixed_batch # One step with batch_size=256 # batch_model = copy.deepcopy(model) learn_batch = Learner(dls, batch_model, loss_func=CrossEntropyLossFlat(), opt_func=partial(SGD, mom=momentum)) # Set learning rate and create optimizer learn_batch.lr = learning_rate learn_batch.create_opt() # Perform one gradient update with the full batch batch_model.train() learn_batch.opt.zero_grad() preds = batch_model(x_batch) loss = learn_batch.loss_func(preds, y_batch) loss.backward() learn_batch.opt.step() # Store the batch updated weights batch_weights = {k: v.clone() for k, v in batch_model.state_dict().items()} # Store initial weights initial_model = copy.deepcopy(model) initial_weights = {k: v.clone() for k, v in initial_model.state_dict().items()} # Create accumulator for weights weight_sums = None # Process each example individually for i in range(len(x_batch)): # Reset to original weights individual_model = copy.deepcopy(initial_model) # Create a fresh optimizer learn_indiv = Learner(dls, individual_model, loss_func=CrossEntropyLossFlat(), opt_func=partial(SGD, mom=momentum)) learn_indiv.lr = learning_rate learn_indiv.create_opt() # Get individual example (keeping dimensions correct) x_i = x_batch[i:i+1] y_i = y_batch[i:i+1] # IMPORTANT FIX: When using BatchNorm, we need special handling: # For batch_size=1, either use eval mode to use running stats # or track running stats but skip the training updates for BN layers if use_batch_norm: # Option 1: Use eval mode for batch norm layers only for module in individual_model.modules(): if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)): module.eval() # Use running statistics else: module.train() # Keep other layers in training mode else: # Normal training mode if no batch norm individual_model.train() # Forward, backward, optimize learn_indiv.opt.zero_grad() preds = individual_model(x_i) loss = learn_indiv.loss_func(preds, y_i) loss.backward() learn_indiv.opt.step() # Get updated weights updated_weights = {k: v.clone() for k, v in individual_model.state_dict().items()} # Accumulate weights if weight_sums is None: weight_sums = updated_weights else: for k in weight_sums: weight_sums[k] += updated_weights[k] # Average the accumulated weights avg_weights = {k: v / len(x_batch) for k, v in weight_sums.items()} # Calculate differences between batch and averaged individual weights differences = {} for k in batch_weights: # Only compare parameters, not buffers (like running mean in batch norm) if k in avg_weights and &#39;running&#39; not in k and &#39;num_batches&#39; not in k: differences[k] = torch.abs(batch_weights[k] - avg_weights[k]) # Print statistics about the differences max_diff = max(diff.max().item() for diff in differences.values()) avg_diff = sum(diff.mean().item() for diff in differences.values()) / len(differences) print(f&quot;Maximum absolute difference: {max_diff:.8f}&quot;) print(f&quot;Average absolute difference: {avg_diff:.8f}&quot;) # Detailed differences by layer (optional) print(&quot; nDifferences by layer:&quot;) for k, diff in differences.items(): if len(diff.shape) &gt; 0: # Skip scalar parameters print(f&quot;{k}: mean={diff.mean().item():.8f}, max={diff.max().item():.8f}&quot;) # Evaluate validation performance # Create two models with the different weight sets batch_eval_model = copy.deepcopy(model) batch_eval_model.load_state_dict(batch_weights) avg_eval_model = copy.deepcopy(model) avg_eval_model.load_state_dict(avg_weights) # Set up learners learn_batch_eval = Learner(dls, batch_eval_model, loss_func=CrossEntropyLossFlat()) learn_avg_eval = Learner(dls, avg_eval_model, loss_func=CrossEntropyLossFlat()) # Evaluate and compare batch_valid_loss = learn_batch_eval.validate() avg_valid_loss = learn_avg_eval.validate() print(f&quot; nValidation Results:&quot;) print(f&quot;Batch model: loss={batch_valid_loss}&quot;) print(f&quot;Averaged individual models: loss={avg_valid_loss}&quot;) return { &#39;max_diff&#39;: max_diff, &#39;avg_diff&#39;: avg_diff, &#39;batch_valid_loss&#39;: batch_valid_loss, &#39;avg_valid_loss&#39;: avg_valid_loss } # Run a series of experiments with different configurations results = [] # 1. Vanilla SGD without batch norm print(&quot; n===== VANILLA SGD WITHOUT BATCH NORM =====&quot;) vanilla_results = run_experiment(use_batch_norm=False, momentum=0.0, learning_rate=0.1) results.append((&#39;Vanilla SGD&#39;, vanilla_results)) # 2. SGD with momentum, without batch norm print(&quot; n===== SGD WITH MOMENTUM, WITHOUT BATCH NORM =====&quot;) momentum_results = run_experiment(use_batch_norm=False, momentum=0.9, learning_rate=0.1) results.append((&#39;SGD+Momentum&#39;, momentum_results)) # 3. Vanilla SGD with batch norm print(&quot; n===== VANILLA SGD WITH BATCH NORM =====&quot;) bn_results = run_experiment(use_batch_norm=True, momentum=0.0, learning_rate=0.1) results.append((&#39;SGD+BatchNorm&#39;, bn_results)) # 4. SGD with momentum and batch norm print(&quot; n===== SGD WITH MOMENTUM AND BATCH NORM =====&quot;) full_results = run_experiment(use_batch_norm=True, momentum=0.9, learning_rate=0.1) results.append((&#39;SGD+Momentum+BatchNorm&#39;, full_results)) # Summary of all experiments print(&quot; n n========== SUMMARY OF ALL EXPERIMENTS ==========&quot;) print(f&quot;{&#39;Configuration&#39;:&lt;25} {&#39;Max Diff&#39;:&lt;15} {&#39;Avg Diff&#39;:&lt;15} &quot;) print(&quot;-&quot; * 85) for name, result in results: print(f&quot;{name:&lt;25} {result[&#39;max_diff&#39;]:&lt;15.8f} {result[&#39;avg_diff&#39;]:&lt;15.8f}&quot;) . . . . . Conclusions: . With vanilla SGD without batch norm, batch=256 and averaged batch=1 should be nearly equivalent | When momentum is added, batch=256 and averaged batch=1 should show differences due to momentum&#39;s stateful updates | With batch norm, significant differences should occur because: In batch=256, statistics are calculated across all examples at once | In batch=1, we use running statistics instead of batch statistics (otherwise it would error) | . | The batch norm with momentum case should show the largest differences | These approaches are mathematically equivalent only for vanilla SGD without any stateful components like momentum or batch normalization. . The experiment shows virtually no difference between batch_size=256 and averaged batch_size=1 for the momentum case, which is unexpected and contradicts our theoretical understanding. The key issue is that momentum requires state that builds up over time, but our experiment only performs a single gradient update. Let&#39;s fix it . def run_multi_step_experiment(num_steps=5, momentum=0.9, learning_rate=0.01): &quot;&quot;&quot; Run an experiment with multiple update steps to show the effect of momentum Args: num_steps (int): Number of update steps to perform momentum (float): Momentum value for SGD optimizer learning_rate (float): Learning rate for the optimizer &quot;&quot;&quot; print(f&quot; n Multi-Step Experiment: Steps={num_steps}, Momentum={momentum}, LR={learning_rate} &quot;) # Create a simple model without batch norm model = nn.Sequential( nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(64 * 7 * 7, 128), nn.ReLU(), nn.Linear(128, 10) ) # Get MNIST data path = untar_data(URLs.MNIST) # Define DataBlock for consistent processing mnist_block = DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(train_name=&#39;training&#39;, valid_name=&#39;testing&#39;), get_y=parent_label, item_tfms=[ToTensor()], batch_tfms=[] ) # Create DataLoaders with a smaller batch size to have multiple batches dls = mnist_block.dataloaders(path, bs=64) # Get multiple batches for the experiment batches = [dls.train.one_batch() for _ in range(num_steps)] # APPROACH 1: Train with full batches # -- batch_model = copy.deepcopy(model) learn_batch = Learner(dls, batch_model, loss_func=CrossEntropyLossFlat(), opt_func=partial(SGD, mom=momentum)) learn_batch.lr = learning_rate learn_batch.create_opt() batch_model.train() # Apply multiple update steps with full batches for step in range(num_steps): x_batch, y_batch = batches[step] learn_batch.opt.zero_grad() preds = batch_model(x_batch) loss = learn_batch.loss_func(preds, y_batch) loss.backward() learn_batch.opt.step() # Store the batch updated weights batch_weights = {k: v.clone() for k, v in batch_model.state_dict().items()} # APPROACH 2: Train with individual examples and maintain momentum state # individual_model = copy.deepcopy(model) learn_indiv = Learner(dls, individual_model, loss_func=CrossEntropyLossFlat(), opt_func=partial(SGD, mom=momentum)) learn_indiv.lr = learning_rate learn_indiv.create_opt() individual_model.train() # For each step (equivalent to a full batch) for step in range(num_steps): x_batch, y_batch = batches[step] # Process each example in the batch individually, but keep momentum state for i in range(len(x_batch)): x_i = x_batch[i:i+1] y_i = y_batch[i:i+1] learn_indiv.opt.zero_grad() preds = individual_model(x_i) loss = learn_indiv.loss_func(preds, y_i) loss.backward() learn_indiv.opt.step() # Store final individual weights individual_weights = {k: v.clone() for k, v in individual_model.state_dict().items()} # APPROACH 3: Train with individual examples, averaging gradients (no momentum state) # - avg_model = copy.deepcopy(model) # For each step (equivalent to a full batch) for step in range(num_steps): x_batch, y_batch = batches[step] batch_size = len(x_batch) # Create a fresh optimizer for each batch learn_avg = Learner(dls, avg_model, loss_func=CrossEntropyLossFlat(), opt_func=partial(SGD, mom=momentum)) learn_avg.lr = learning_rate learn_avg.create_opt() avg_model.train() # Accumulate gradients for all examples in the batch accumulated_grads = None for i in range(batch_size): x_i = x_batch[i:i+1] y_i = y_batch[i:i+1] learn_avg.opt.zero_grad() preds = avg_model(x_i) loss = learn_avg.loss_func(preds, y_i) loss.backward() # Extract and store gradients current_grads = {name: param.grad.clone() for name, param in avg_model.named_parameters() if param.grad is not None} if accumulated_grads is None: accumulated_grads = current_grads else: for name in accumulated_grads: accumulated_grads[name] += current_grads[name] # Apply averaged gradients manually with torch.no_grad(): for name, param in avg_model.named_parameters(): if name in accumulated_grads: # Manually apply SGD update with the averaged gradient param -= learning_rate * (accumulated_grads[name] / batch_size) # Store final averaged weights avg_weights = {k: v.clone() for k, v in avg_model.state_dict().items()} # COMPARISON AND ANALYSIS # -- # Calculate differences between approaches diff_batch_vs_indiv = {} diff_batch_vs_avg = {} for k in batch_weights: if k in individual_weights and k in avg_weights: diff_batch_vs_indiv[k] = torch.abs(batch_weights[k] - individual_weights[k]) diff_batch_vs_avg[k] = torch.abs(batch_weights[k] - avg_weights[k]) # Calculate statistics max_diff_indiv = max(diff.max().item() for diff in diff_batch_vs_indiv.values()) avg_diff_indiv = sum(diff.mean().item() for diff in diff_batch_vs_indiv.values()) / len(diff_batch_vs_indiv) max_diff_avg = max(diff.max().item() for diff in diff_batch_vs_avg.values()) avg_diff_avg = sum(diff.mean().item() for diff in diff_batch_vs_avg.values()) / len(diff_batch_vs_avg) print(&quot; nDifferences between batch training and individual training (WITH momentum state):&quot;) print(f&quot;Maximum absolute difference: {max_diff_indiv:.8f}&quot;) print(f&quot;Average absolute difference: {avg_diff_indiv:.8f}&quot;) print(&quot; nDifferences between batch training and gradient averaging (NO momentum state):&quot;) print(f&quot;Maximum absolute difference: {max_diff_avg:.8f}&quot;) print(f&quot;Average absolute difference: {avg_diff_avg:.8f}&quot;) # Evaluate validation performance learn_batch_eval = Learner(dls, batch_model, loss_func=CrossEntropyLossFlat()) learn_indiv_eval = Learner(dls, individual_model, loss_func=CrossEntropyLossFlat()) learn_avg_eval = Learner(dls, avg_model, loss_func=CrossEntropyLossFlat()) batch_valid_loss = learn_batch_eval.validate() indiv_valid_loss = learn_indiv_eval.validate() avg_valid_loss = learn_avg_eval.validate() print(&quot; nValidation Results:&quot;) print(f&quot;Batch model: loss={batch_valid_loss}&quot;) print(f&quot;Individual model (with momentum): loss={indiv_valid_loss}&quot;) print(f&quot;Averaged model (no momentum): loss={avg_valid_loss}&quot;) return { &#39;max_diff_indiv&#39;: max_diff_indiv, &#39;avg_diff_indiv&#39;: avg_diff_indiv, &#39;max_diff_avg&#39;: max_diff_avg, &#39;avg_diff_avg&#39;: avg_diff_avg } # Run experiments with different numbers of steps print(&quot; n===== EXPERIMENT WITH MOMENTUM OVER MULTIPLE STEPS =====&quot;) # With a small number of steps small_steps_results = run_multi_step_experiment(num_steps=2, momentum=0.9, learning_rate=0.01) # With more steps to show accumulated momentum effects more_steps_results = run_multi_step_experiment(num_steps=5, momentum=0.9, learning_rate=0.01) # With even more steps many_steps_results = run_multi_step_experiment(num_steps=10, momentum=0.9, learning_rate=0.01) print(&quot; n n========== SUMMARY OF MULTI-STEP EXPERIMENTS ==========&quot;) print(f&quot;{&#39;Configuration&#39;:&lt;15} {&#39;Max Diff (Individual)&#39;:&lt;22} {&#39;Avg Diff (Individual)&#39;:&lt;22} {&#39;Max Diff (Averaged)&#39;:&lt;22} {&#39;Avg Diff (Averaged)&#39;:&lt;22}&quot;) print(&quot;-&quot; * 120) print(f&quot;{&#39;2 Steps&#39;:&lt;15} {small_steps_results[&#39;max_diff_indiv&#39;]:&lt;22.8f} {small_steps_results[&#39;avg_diff_indiv&#39;]:&lt;22.8f} {small_steps_results[&#39;max_diff_avg&#39;]:&lt;22.8f} {small_steps_results[&#39;avg_diff_avg&#39;]:&lt;22.8f}&quot;) print(f&quot;{&#39;5 Steps&#39;:&lt;15} {more_steps_results[&#39;max_diff_indiv&#39;]:&lt;22.8f} {more_steps_results[&#39;avg_diff_indiv&#39;]:&lt;22.8f} {more_steps_results[&#39;max_diff_avg&#39;]:&lt;22.8f} {more_steps_results[&#39;avg_diff_avg&#39;]:&lt;22.8f}&quot;) print(f&quot;{&#39;10 Steps&#39;:&lt;15} {many_steps_results[&#39;max_diff_indiv&#39;]:&lt;22.8f} {many_steps_results[&#39;avg_diff_indiv&#39;]:&lt;22.8f} {many_steps_results[&#39;max_diff_avg&#39;]:&lt;22.8f} {many_steps_results[&#39;avg_diff_avg&#39;]:&lt;22.8f}&quot;) . . . Conclusions: . This experiment compares three approaches: . Batch training: Regular updates with batch_size=64 | Individual training: One example at a time but maintaining momentum state | Gradient averaging: One example at a time, accumulating gradients, then applying averaged gradients | . | Key findings: . With more steps, the differences between batch and individual training WITH momentum state grow larger | The differences between batch and gradient averaging are much higher | This demonstrates that momentum creates a cumulative effect that makes batch training and individual training non-equivalent, even without batch normalization | . | Explanation: . Momentum depends on the history of gradients | In batch training, we compute one gradient from the entire batch | In individual training, we update the momentum buffer after each example | These momentum buffers evolve differently over time, causing weights to diverge | . | References: . https://arxiv.org/abs/1812.06162 | .",
            "url": "https://numb3r33.github.io/experiments/math/deeplearning/2025/03/21/batch-size-theory-vs-practice.html",
            "relUrl": "/math/deeplearning/2025/03/21/batch-size-theory-vs-practice.html",
            "date": " • Mar 21, 2025"
        }
        
    
  
    
        ,"post2": {
            "title": "Attention Paths and Rank Collapse Part 1",
            "content": "Introduction . Transformers have revolutionized deep learning across domains from NLP to computer vision, but how do they actually work? While we know how to build and train these models, their internal mechanics remain somewhat mysterious. . Recently I came across this paper Attention is not all you need which tries to highlight how attention is not all you need and how MLP and skip connections play huge role in learning. We&#39;ll start by exploring two fascinating concepts: attention paths and rank collapse. . The Mathematical Foundation of Self-Attention . Before diving into the more complex concepts, let&#39;s review the basic building blocks of self-attention networks (SANs). . In a SAN, the input is a sequence of vectors $X = [x_1, x_2, ..., x_N]$, where $x_i in mathbb{R}^d$ represents a token embedding, $N$ is the sequence length, and $d$ is the embedding dimension. . Queries, Keys, Values . For each attention head, we transform the input into three roles: . Queries: $Q = XW_Q$, where $W_Q in mathbb{R}^{d times d_k}$ | Keys: $K = XW_K$, where $W_K in mathbb{R}^{d times d_k}$ | Values: $V = XW_V$, where $W_V in mathbb{R}^{d times d_v}$ | . Here, $d_k$ and $d_v$ are dimensions of keys and values, typically $d_k = d_v = d/H$ where $H$ is the number of heads. . Attention Scores . We compute how much each token attends to others: . $A = text{softmax} left( frac{QK^T}{ sqrt{d_k}} right)$ . $QK^T in mathbb{R}^{N times N}$ is a matrix of dot products measuring similarity between queries and keys | $ sqrt{d_k}$ scales the scores to prevent large values that could destabilize the softmax | $A_{ij}$ is the attention weight from token $i$ to token $j$ | . Output . The output of a single attention head is: . $ text{head} = AV$ . So, $ text{head} in mathbb{R}^{N times d_v}$. . Multi-Head Attention . The complete multi-head attention operation uses $H$ heads, each with its own $W_Q^h, W_K^h, W_V^h$: . $ text{MultiHead}(X) = text{Concat}( text{head}_1, ..., text{head}_H)W_O$ . where $W_O in mathbb{R}^{Hd_v times d}$ projects the concatenated outputs back to $d$-dimensional space. . Layer Structure . In a SAN layer, we add residual connections and an MLP: . $X&#39; = X + text{MultiHead}(X) text{ (residual connection)}$ . $X_{out} = X&#39; + text{MLP}(X&#39;) text{ (another residual connection)}$ . The MLP typically consists of two linear layers with a nonlinearity: $ text{MLP}(x) = W_2 cdot text{ReLU}(W_1 x + b_1) + b_2$ . Understanding Attention Paths . What is a Path? . In the context of transformers, a path refers to a specific route through the self-attention network. At each layer, you can choose one of the $H$ attention heads or bypass the layer via a skip connection. . Mathematically, we can define a path $P = (p_1, p_2, ..., p_L)$, where $p_l in {0, 1, ..., H }$: . $p_l = h$ (1 to $H$): Use head $h$ at layer $l$ | $p_l = 0$: Bypass the layer (skip connection) | . . Visualizing Paths . Let&#39;s interpret two example paths: . Red Path (2, ..., 1): This path starts with head 2 in the first layer and ends with head 1 in the last layer, with unspecified heads in between. This represents a diverse approach, exploring different aspects of the input at different layers. . | Blue Path (H, 2, ..., 2): This path starts with the last head ($H$) and then consistently uses head 2 for all subsequent layers. This represents a more specialized strategy - beginning broadly but then maintaining consistency. . | Why Do Paths Matter? . The total output $X_L$ of a SAN can be decomposed as a sum of contributions from all possible paths: . $X_L = sum_{P in mathcal{P}} w_P X_L^P$ . where $ mathcal{P}$ is the set of all possible paths (e.g., $(H+1)^L$ choices with $H$ heads + skip), $X_L^P$ is the output of path $P$, and $w_P$ weights its contribution. . This decomposition reveals: . Interpretability: By analyzing individual paths, we can better understand how information flows through the network | Efficiency: If only a few paths contribute significantly, we could potentially prune the others | Theory: It provides a framework for understanding how depth functions in these networks | The challenge? With $H=8$ and $L=6$, there are $9^6 approx 531,441$ paths! This complexity motivates the study of &quot;weakly interdependent&quot; paths. . Weak Interdependence Between Paths . For paths to be &quot;weakly interdependent,&quot; their interactions must be small or negligible. Several mathematical properties can characterize this: . 1. Orthogonality of Path Contributions . If the outputs $X_L^P$ and $X_L^Q$ of two distinct paths $P$ and $Q$ are orthogonal: . $ langle X_L^P, X_L^Q rangle = 0 text{ for } P neq Q$ . This ensures each path contributes to the output in a way that is linearly independent of other paths. . 2. Low Covariance Between Paths . If the covariance between path outputs is small: . $ text{Cov}(X_L^P, X_L^Q) = mathbb{E}[(X_L^P - mathbb{E}[X_L^P])(X_L^Q - mathbb{E}[X_L^Q])^T] approx 0$ . This indicates that fluctuations in one path&#39;s contribution don&#39;t strongly correlate with fluctuations in another&#39;s. . 3. Additive Contributions with Minimal Interaction . If the output $X_L$ is well-approximated by the linear sum $ sum_P w_P X_L^P$, with higher-order interaction terms (e.g., products like $X_L^P X_L^Q$) being negligible. . 4. Small Cross-Path Gradients . For parameters $ theta_P$ and $ theta_Q$ associated with paths $P$ and $Q$, the cross-gradient terms in the loss function $L$ are small: . $ left| frac{ partial^2 L}{ partial theta_P partial theta_Q} right| ll 1 text{ for } P neq Q$ . This means optimizing one path has little impact on another during training. . 5. Low Mutual Information . The mutual information between the outputs of paths $P$ and $Q$ is small: . $I(X_L^P; X_L^Q) approx 0$ . This formalizes the idea that the paths are nearly independent. . Gateway Choices: Rethinking Information Flow . The concept of &quot;gateway choices&quot; changes our understanding of how information flows through transformers. Rather than treating all information equally, certain components act as selective gateways that determine which information is emphasized or propagated. . This has several implications: . Selective Routing: Transformers dynamically adjust their focus based on the input, prioritizing task-critical data. | Dynamic Processing: The flow of information adapts to the context, with different aspects prioritized at different stages. | Hierarchical Refinement: Early gateways might extract broad features, while later ones refine these into more specific representations. | Mathematically, gateway choices already exist in transformers: . Attention Weights: The softmax determines how much information flows between tokens | Residual Connections: Allow some information to bypass transformation | . Rank Collapse: A Key Constraint . Perhaps the most intriguing concept is &quot;rank collapse&quot; - the phenomenon where representations or attention matrices become low-rank as they pass through the network layers. . What is Rank? . In linear algebra, the rank of a matrix is the number of linearly independent rows or columns. It reflects the amount of unique information the matrix contains. . The Rank Collapse Claim . The claim that attention &quot;doubly exponentially loses rank&quot; suggests that as we stack attention layers, the rank of the representations decreases at an accelerated rate. . This is potentially caused by several properties of the attention operation: . Softmax Saturation: If the dot products between queries and keys have large values, softmax becomes sparse, reducing the rank of the attention matrix. | Correlation in Queries and Keys: Since they&#39;re derived from the same input, this can lead to redundancy. | Layer Composition: Low rank at one layer propagates to the next, creating a feedback loop. | Scaling Effects: The scaling factor $ sqrt{d_k}$ can impact rank dynamics. | The Self-Attention Head Output . Looking at the formula for self-attention head output: . $ text{SA}_{(h)}(X) = P_{(h)} cdot X cdot W_{(V,h)} + mathbf{1}b_{(V,h)}^T$ . The properties of $P_{(h)}$ (the attention matrix from softmax) are crucial: . Row-Stochastic: Each row sums to 1, imposing a constraint | Non-Negative: All entries are between 0 and 1 | Variable Sparsity: Depending on input patterns, it can range from uniform to nearly one-hot | . If $P_{(h)}$ becomes low-rank (e.g., due to most tokens attending to the same keys), it constrains the rank of the output, leading to rank collapse. . Coming Up in Part 2 . In the next installment, we&#39;ll explore . Dimensional Consistency in Transformers | Attention Mechanism Reformulation | The Mathematics of Rank Collapse | Concatenation vs. Summation in Multi-Head Attention | Scaling Factors and Bias Terms | . . Stay tuned for more deep dives into transformer theory in the upcoming parts of this series! . References: . https://arxiv.org/abs/2103.03404 | .",
            "url": "https://numb3r33.github.io/experiments/llm/transformers/attention/math/deeplearning/2025/03/20/attention-is-not-all-you-need.html",
            "relUrl": "/llm/transformers/attention/math/deeplearning/2025/03/20/attention-is-not-all-you-need.html",
            "date": " • Mar 20, 2025"
        }
        
    
  
    
        ,"post3": {
            "title": "Unveiling Position Encoding in Transformers - From Absolute to Relative with RoPE",
            "content": "Introduction . Let&#39;s start with the mathematical foundations of absolute position encodings, examining the limitations that led to relative approaches, exploring complex number representations, and ultimately showing how these concepts enable transformer models to effectively capture positional relationships. . 1. Position Encoding in Transformers: Evolution and Approaches . 1.1 Absolute Position Encoding: The Original Approach . The original transformer model introduced sinusoidal position encodings: . $PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$ $PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$ . Where $pos$ is the position and $i$ is the dimension. This encoding has two key properties: . It provides a unique encoding for each position | The model might extrapolate to sequence lengths unseen during training | However, this absolute approach faces limitations when dealing with relative relationships between tokens, which are crucial for many linguistic structures. . 1.2 T5&#39;s Simplification: Decoupling Content and Position . T5 (Text-to-Text Transfer Transformer) modified the attention mechanism by explicitly decoupling content and position: . $q_i cdot k_j^T = x_i W_Q W_K^T x_j^T + x_i W_Q W_K^T p_j^T + p_i W_Q W_K^T x_j^T + p_i W_Q W_K^T p_j^T$ . This expands into four distinct terms: . Input-input: $x_i W_Q W_K^T x_j^T$ (content-content interaction) | Input-position: $x_i W_Q W_K^T p_j^T$ (content attends to position) | Position-input: $p_i W_Q W_K^T x_j^T$ (position attends to content) | Position-position: $p_i W_Q W_K^T p_j^T$ (position-position interaction) | . T5&#39;s key insight was that the position-position and position-input terms could be deleted and $p_i W_Q W_K^T p_j^T$ is actually a function $B_{ij}$ which can be trained as a parameter: . $q_i cdot k_j^T = x_i W_Q W_K^T x_j^T + B_{ij}$ . Where $B_{ij} = f(i-j)$, a function assigning relative distances to &quot;buckets.&quot; Each bucket has its own bias value, and all relative distances that fall into the same bucket share that value: . Example of bucketing: . $i-j = 0: f(0) = 0$ (Same token) | $i-j = 1: f(1) = 1$ (Adjacent tokens) | $i-j = 2: f(2) = 2$ (2 tokens apart) | ... | $i-j = 8: f(8) = 8$ (8 tokens apart) | $i-j = 9: f(9) = 8$ (Mapped to the same bucket as distance 8) | $i-j = 10: f(10) = 8$ (Mapped to the same bucket as distance 8) | . This bucketing approach maps small distances (0-7) to individual buckets, while larger distances map to broader buckets, capturing the intuition that precise positioning matters more for nearby tokens. . 1.3 XLNet-Style: Relative Position Encoding . XLNet introduced a more sophisticated approach to relative position encoding, starting with the expanded attention score: . $q_i cdot k_j^T = (x_i + p_i) W_Q cdot W_K^T (x_j + p_j)^T$ . This expands to: $q_i cdot k_j^T = x_i W_Q W_K^T x_j^T + x_i W_Q W_K^T p_j^T + p_i W_Q W_K^T x_j^T + p_i W_Q W_K^T p_j^T$ . XLNet replaces $p_j$ with a relative position encoding $ mathbf{R}_{ij}$ and modifies the computation: . $q_i cdot k_j^T = x_i W_Q W_K^T x_j^T + x_i W_Q W_K^T mathbf{R}_{ij} + u W_K^T x_j^T + v W_K^T mathbf{R}_{ij}$ . Where: . $u$ and $v$ are trainable vectors that interact with content and position | $ mathbf{R}_{ij}$ is a learnable relative position encoding | . This approach addresses the challenge that encoding spaces may not match—the input embeddings are in one space (typically $ mathbb{R}^d$), while relative position encoding $ mathbf{R}_{ij}$ is a vector that encodes distance. The projection matrices $W_K$ and $W_Q$ ensure these spaces are compatible for attention calculations. . 1.4 The Complex Form Approach . Taking a step back, we can view relative position encoding through the lens of complex numbers, which provides an elegant mathematical framework. . For a vector $[x, y]$, we can treat this as a complex number $x + iy$. Using the natural representation of rotations in the complex plane, we can encode positional information by multiplying by a complex exponential: . $(x + iy) cdot e^{in theta} = (x + iy) cdot ( cos(n theta) + i sin(n theta))$ . Where $n$ is the position and $ theta$ is a fixed angle (which could be position-dependent in more sophisticated schemes). . This naturally leads us to Rotary Position Embedding (RoPE), which builds on this complex number intuition but formalizes it for high-dimensional transformer embeddings. . 2. Rotary Position Encoding: The Foundation . 2.1 The Core Idea . The central insight of RoPE is elegantly simple: represent token positions as rotations in vector space. For a sequence of tokens $(q_0, q_1, ..., q_{d-2}, q_{d-1})$, we apply a rotation to each pair of dimensions with an angle that&#39;s proportional to the position index. . Let&#39;s start with the rotation matrix $ mathbf{R}_m$ which is block-diagonal: . $$ mathbf{R}_m = text{diag}( mathbf{R}_m^0, mathbf{R}_m^1, ..., mathbf{R}_m^{d/2-1})$$ . where each block $ mathbf{R}_m^i$ is a 2D rotation matrix: . $$ mathbf{R}_m^i = begin{pmatrix} cos(m theta_i) &amp; - sin(m theta_i) sin(m theta_i) &amp; cos(m theta_i) end{pmatrix}$$ . Here, $m$ indexes the position, and $ theta_i$ is a fixed angle for dimension $i$. . 1.2 Applying Rotations to Embeddings . In the context of transformers, we apply these rotations to query and key vectors before computing attention: . $$f(q, m) = mathbf{R}_m q$$ $$f(k, n) = mathbf{R}_n k$$ . The dot product between transformed vectors then becomes: . $$ langle f(q,m), f(k,n) rangle = q^T mathbf{R}_m^T mathbf{R}_n k = q^T mathbf{R}_{n-m} k$$ . This remarkable property shows that the attention score between positions $m$ and $n$ depends only on their relative distance $n-m$, not their absolute positions. . 2. The Complex Number Approach . 2.1 Reframing in the Complex Domain . We can express the rotation operations more elegantly using complex numbers. Let&#39;s assume $f(q,m)$ has an exponential form analogous to rotation in the complex plane: . $$f(q,m) = qe^{im theta}$$ . where $q$ is a complex vector and $e^{im theta}$ rotates $q$ by an angle $m theta$. . 2.2 Verifying the Properties . Let&#39;s verify that this form exhibits the desired behavior: . For a query at position $m$: $f(q, m) = qe^{im theta}$ For a key at position $n$: $f(k, n) = ke^{in theta}$ . Computing the inner product: $$ langle f(q,m), f^*(k,n) rangle = (qe^{im theta})(k^*e^{-in theta}) = qk^*e^{i(m-n) theta}$$ . Taking the real part: $$ text{Re}[qk^*e^{i(m-n) theta}] = text{Re}[qk^*] cos((m-n) theta) - text{Im}[qk^*] sin((m-n) theta)$$ . This formula confirms that the dot product result depends on $q$, $k$, and their relative position $m-n$. . 2.3 Expanding the Multiplication . To understand how this works with real-valued vectors, let&#39;s expand the complex multiplication: . $$(x+iy)( cos(n theta) + i sin(n theta)) = x cos(n theta) - y sin(n theta) + i(x sin(n theta) + y cos(n theta))$$ . Grouping real and imaginary parts: . Real part: $x cos(n theta) - y sin(n theta)$ | Imaginary part: $x sin(n theta) + y cos(n theta)$ | . As a vector, this becomes: $$ begin{pmatrix} x cos(n theta) - y sin(n theta) x sin(n theta) + y cos(n theta) end{pmatrix}$$ . This is equivalent to applying the rotation matrix: $$ begin{pmatrix} cos(n theta) &amp; - sin(n theta) sin(n theta) &amp; cos(n theta) end{pmatrix} begin{pmatrix} x y end{pmatrix}$$ . 3. Complex Form: A Deeper Dive . 3.1 Multiple Word Vectors and Their Representation . In transformer architectures, each word $j$ at position $k$ has three sets of vectors $(v_j, w_j, θ_j)$ that are independent of position: . $v_{j,m}$: Magnitude for dimension $m$ | $w_{j,m}$: Frequency (how fast the phase changes with position) | $θ_{j,m}$: Initial phase (starting angle) | . For each word $j$ at position $k$, its embedding is a vector of $d$ complex numbers, where each component is: . $v_{j,m} cdot e^{i(w_{j,m}k+θ_{j,m})}$ . Breaking down this formula: . The term $e^{i(w_{j,m}k+θ_{j,m})}$ is a point on the unit circle in the complex plane with angle $w_{j,m}k+θ_{j,m}$ | Multiplying by $v_{j,m}$ scales this point to have radius $v_{j,m}$ | As $k$ (position) changes, the angle increases by $w_{j,m}$ times the position shift, like a rotating arrow | . Consider a word &quot;cat&quot; at position $k=1$ with dimension $m=1$. If $v_{j,1}=2, w_{j,1}=0.5, θ_{j,1}=0$, the component would be: . $2e^{i(0.5 cdot 1 + 0)} = 2e^{i0.5} = 2( cos(0.5) + i sin(0.5))$ . At $k=2$, it becomes $2e^{i1.0}$, rotating further. This form elegantly encodes where the word is positioned while preserving its content characteristics. . 3.2 Understanding the Two-Dimensional Vector Representation . To understand how these complex numbers are implemented in practice, we can view a 2D vector $[x,y]$ as the complex number $x+iy$. . Step 1: Understand the vector as a complex number Step 2: Multiply by a complex exponential . $(x+iy)e^{inθ} = (x+iy)( cos(nθ)+i sin(nθ))$ . Expanding this multiplication: . $(x+iy)( cos(nθ)+i sin(nθ)) = x cos(nθ) + xi sin(nθ) + iy cos(nθ) + iy cdot i sin(nθ)$ . Since $i^2 = -1$, substitute: $yi^2 sin(nθ) = y(-1) sin(nθ) = -y sin(nθ)$ . Rewriting the expression: $x cos(nθ) + xi sin(nθ) + iy cos(nθ) - y sin(nθ)$ . Grouping real and imaginary parts: . Real part: $x cos(nθ) - y sin(nθ)$ | Imaginary part: $x sin(nθ) + y cos(nθ)$ | . As a vector, this becomes: $ begin{pmatrix} x cos(nθ) - y sin(nθ) x sin(nθ) + y cos(nθ) end{pmatrix}$ . This is equivalent to applying the rotation matrix: $ begin{pmatrix} cos(nθ) &amp; - sin(nθ) sin(nθ) &amp; cos(nθ) end{pmatrix} begin{pmatrix} x y end{pmatrix}$ . We&#39;ve now established a direct link between complex number multiplication and rotation matrices, which is the foundation for RoPE. . 4. Scaling to Higher Dimensions . 4.1 Extending to d-dimensional Vectors . For a $d$-dimensional vector $q = (q_0, q_1, ..., q_{d-2}, q_{d-1})$, we group coordinates into pairs and apply a rotation to each pair: . Split the $d$-dimensional vector into $d/2$ pairs: $(q_0, q_1), (q_2, q_3), ..., (q_{d-2}, q_{d-1})$ | Apply rotation to each pair with potentially different angles $ theta_k$ | For example, with a 4D vector $[x_1, x_2, x_3, x_4]$: . Pair 1: $(x_1, x_2)$ → Rotate by $n theta_1$ | Pair 2: $(x_3, x_4)$ → Rotate by $n theta_2$ | . 3.2 Position Encoding Scheme . The transformation encodes position $n$ into the vector by making the rotation angle $n theta$ depend directly on position $n$: . Absolute position: Each position gets a unique transformation | Relative position: In attention mechanisms, we compute dot products between vectors (query $q_m$ and key $k_n$). The dot product of two rotated vectors includes terms like $ cos((m-n) theta)$, which depends on the relative position $(m-n)$. | . 5. RoPE: Relative Position Encoding . 5.1 Complex Form: A Unified Approach . Before diving into the specifics of RoPE, let&#39;s explore the complex form approach that combines complex numbers and positional encoding: . The idea is to represent a word $j$ at position $k$ with a vector of complex numbers: . $[v_{j,1}e^{i(w_{j,1}k+ theta_{j,1})}, v_{j,2}e^{i(w_{j,2}k+ theta_{j,2})}, ..., v_{j,d}e^{i(w_{j,d}k+ theta_{j,d})}]$ . Where: . $j$ is the word in vocabulary | $k$ is the position of the word in the sentence | $d$ is the number of dimensions in the embedding | $v_j, w_j, theta_j$ are three vectors, each with $d$ components, unique to word $j$ $v_{j,m}$: Magnitude for dimension $m$ | $w_{j,m}$: Frequency (how fast the phase changes with position) | $ theta_{j,m}$: Initial phase (starting angle) | . | . For each word $j$ at position $k$, each component of its embedding is: . $v_{j,m}e^{i(w_{j,m}k+ theta_{j,m})}$ . Breaking down this formula: . The term $e^{i(w_{j,m}k+ theta_{j,m})}$ represents a point on the unit circle with angle $w_{j,m}k+ theta_{j,m}$ | As $k$ (position) changes, the angle increases by $w_{j,m}$ times the position shift | The phase changes with position, encoding where the word is in a continuous and periodic way | $v_{j,m}$ reflects the word&#39;s inherent strength or importance | . 5.2 Key Idea of RoPE . RoPE (Rotary Position Encoding) builds on these insights, modifying token embeddings by applying a function $f$ that embeds position: . $ tilde{q}_m = f(q, m)$ $ tilde{k}_n = f(k, n)$ . The dot product becomes: $ langle tilde{q}_m, tilde{k}_n rangle = langle f(q,m), f(k,n) rangle = g(q, k, m-n)$ . Here, $g$ is some function of the original vectors $q$, $k$, and their relative distance $m-n$. . 4.2 Long-Range Attenuation . RoPE implements a natural decay of influence with distance: . As $|m-n|$ grows, terms oscillate with frequencies $ theta_i$, with $ theta_i = 10000^{-2i/d}$ for dimension $i$. Higher $i$ means faster oscillation, reducing sums via cancellation. This decay aligns with the linguistic intuition that distant tokens matter less. . 4.3 Implementation in Attention . Linear attention approximates softmax attention for efficiency: . $$ text{Attention}(q_i, k_j, v_j) = frac{ sum_j phi(q_i)^T phi(k_j) v_j}{ sum_j phi(q_i)^T phi(k_j)}$$ . RoPE applies $ mathbf{R}_m$ to $q$ and $k$ before $ phi$, preserving relative position properties in the numerator: . $$ text{Attention}(q_i, k_j, v_j) = frac{ sum_j phi( mathbf{R}_m q_i)^T phi( mathbf{R}_n k_j) v_j}{ sum_j phi( mathbf{R}_m q_i)^T phi( mathbf{R}_n k_j)}$$ . 5. Advantages of Relative Position Encoding . 5.1 Why Relative Position Works Better . Relative position encoding offers several compelling advantages: . Invariance to Absolute Positions: For tokens at positions $(i,j)$ and $(i+k, j+k)$, the relative distance $(i-j)$ remains unchanged, ensuring the model generalizes across positions. . | Efficiency: By clipping distances (e.g., $ pm 2$), we avoid $O(n^2)$ parameters for sequence length $n$. . | Linguistic Relevance: Syntactic dependencies (e.g., subject-verb) often depend on proximity. For &quot;cat sat&quot; at positions $(2,3)$ or $(3,4)$, the relative distance $i-j = -1$ ensures consistent attention weighting. . | Example: When encoding &quot;The cat sat quietly&quot;, the attention score between &quot;cat&quot; and &quot;sat&quot; uses the same relative distance encoding regardless of where they appear in the sentence, allowing the model to learn consistent syntactic relationships. . 5.2 Mathematical Proof . Claim: Relative position encoding captures dependencies based on token proximity, independent of absolute positions. . Proof: . Invariance to Shifts: For tokens at positions $(i,j)$ and $(i+k, j+k)$, the relative distance $(i-j)$ remains unchanged. Their attention score $a_{ij}$ is identical, ensuring the model generalizes across positions. . | Efficiency: By clipping distances to a fixed range (e.g., $-2$ to $+2$), we need only a constant number of embeddings regardless of sequence length. . | Linguistic Justification: For &quot;cat sat&quot; at positions $(2,3)$ with $i-j = -1$, the encoding $ mathbf{R}_{3-4} = mathbf{R}_{-1}$ emphasizes adjacency regardless of absolute positions. . | 6. Technical Implementation . 6.1 Some observations . Another useful observation relates to how relative position information is used in the model: . → The model uses relative position information to decide which positions to pay attention to (via $v_j$), but when it retrieves the information from those positions, it only uses the content itself, not positional data. . This led to T5&#39;s simplification approach: decoupling content and position. In the full attention calculation, the attention score can be expanded as: . $q_i cdot k_j^T = x_i W_Q W_K^T x_j^T + x_i W_Q W_K^T p_j^T + p_i W_Q W_K^T x_j^T + p_i W_Q W_K^T p_j^T$ . T5 noted that these can be interpreted as: . 1) Content-Content Term: $(x_i W_Q W_K^T x_j^T)$ . Measures how much the content of token $i$ (e.g., cat) attends to the content of token $j$ | . 2) Content-Position Term: $(x_i W_Q W_K^T p_j^T)$ . Measures how much the content of token $i$ attends to the absolute positions of token $j$ | . 3) Position-Content Term: $(p_i W_Q W_K^T x_j^T)$ . Measures how much the absolute position of token $i$ attends to the content of token $j$ | . 4) Position-Position Term: $(p_i W_Q W_K^T p_j^T)$ . Measures interaction between the absolute positions of $i$ &amp; $j$ | . The idea: decouple content from position → there should not be interaction between &quot;input-position&quot; &amp; &quot;position-input&quot; terms. The position-position term $(p_i W_Q W_K^T p_j^T)$ is actually $(i,j)$ which can be trained as a parameter. . 6.2 Standard Self-Attention with Absolute Position Encoding . In standard transformer models with absolute position encoding: . $q_i = (x_i + p_i)W_Q$ $k_j = (x_j + p_j)W_K$ $v_j = (x_j + p_j)W_V$ $a_{ij} = text{softmax} left( frac{q_i cdot k_j^T}{ sqrt{d}} right)$ $o_i = sum_j a_{ij}v_j$ . Expanding the attention score: . $q_i cdot k_j^T = (x_i + p_i)W_Q W_K^T(x_j + p_j)^T = x_i W_Q W_K^T x_j^T + x_i W_Q W_K^T p_j^T + p_i W_Q W_K^T x_j^T + p_i W_Q W_K^T p_j^T$ . This expands into four terms: . Content-content interaction | Content-position interaction | Position-content interaction | Position-position interaction | 6.2 RoPE&#39;s Modification . RoPE replaces absolute position terms with learnable relative position vectors: . Remove position bias from queries: $q_i = x_i W_Q$ | Replace absolute positions with relative position vector $ mathbf{R}_{ij}^K$: $k_j = x_j W_K + mathbf{R}_{ij}^K$ | Modify the attention score: $$a_{ij} = text{softmax} left( frac{x_i W_Q (x_j W_K + mathbf{R}_{ij}^K)^T}{ sqrt{d}} right)$$ | 6.3 Defining Relative Position Vectors . The relative position vectors depend on $i-j$, clipped to a fixed range $[p_{min}, p_{max}]$: . $$ mathbf{R}_{ij}^K = P_K[ text{clip}(i-j, p_{min}, p_{max})]$$ $$ mathbf{R}_{ij}^V = P_V[ text{clip}(i-j, p_{min}, p_{max})]$$ . Where $P_K$ and $P_V$ are learnable embeddings for each clipped distance. . Example: If $p_{min} = -2$ and $p_{max} = 2$, distance beyond $ pm 2$ are clipped: . For $i-j = 5$, $ text{clip}(5, -2, 2) = 2$ | Only 5 embeddings are needed: -2, -1, 0, 1, 2 | . 6.4 Why Projection Matrices are Needed . An interesting question arises: Why is $ mathbf{R}_{ij}$ preceded by $W_k^T$ and $W_h^T$? . The key vector for position $j$ is computed as $k_j = x_j W_K$, where $W_K$ is a matrix that projects $x_j$ into key space (e.g., R^{64}). . For relative position encoding $ mathbf{R}_{ij}$, we need to bring it to a compatible space so it could interact with query &amp; key vectors in the attention score. . $W_K^T$ → transpose of the key projection matrix used by $x_j$ | Key point: $ mathbf{R}_{ij}$ gets its own dedicated projection matrix, $W_{K,R}$ | $W_{K,R}$ is a separate matrix designed to transform $ mathbf{R}_{ij}$ into the key space, ensuring it aligns with dimensions and structure needed for attention calculation | . Similarly with query-side projections: . $u$ and $v$ are used as standalone vectors without needing additional transformations by $W_Q$ | In the attention score, $u$ interacts with $x_j W_K$ and $v$ interacts with projected $ mathbf{R}_{ij} W_{K,R}$ | . The full attention calculation with these projections becomes: $x_i W_Q W_K^T x_j^T + x_i W_Q W_{K,R}^T mathbf{R}_{ij} + u W_K^T x_j^T + v W_{K,R}^T mathbf{R}_{ij}$ . Conclusion . We&#39;ve taken a comprehensive journey through the evolution of position encoding in transformer models, from absolute position encodings in the original transformer, through T5&#39;s simplifications and XLNet&#39;s innovations, to the elegant mathematics of RoPE. . The key insights we&#39;ve covered: . The Problem: The inherent permutation invariance of attention mechanisms necessitates explicit position information | Early Solutions: Absolute position encodings added to token embeddings worked but had limitations | T5&#39;s Contribution: Decoupling content and position by simplifying the attention mechanism | XLNet&#39;s Approach: Combining content and relative position information with dedicated parameter vectors | Complex Numbers: Providing an elegant mathematical framework for understanding rotations | RoPE&#39;s Innovation: Encoding positions as rotations in vector space that naturally preserve relative distances | Practical Advantages: Relative position methods generalize better, require fewer parameters, and align with linguistic intuition | The mathematical elegance of position encoding methods, particularly RoPE, demonstrates how first principles can lead to powerful practical techniques. By embracing the underlying geometry of the problem, we gain a position encoding method that not only works well in practice but also has theoretical properties that justify its success. . The advantages of relative position encoding are clear: . Generalization: Handles variable sentence lengths and structures | Efficiency: Fewer parameters than absolute encoding | Linguistic Relevance: Prioritizes local dependencies that are critical for syntax | . As transformer architectures continue to evolve, understanding the mathematical foundations of components like position encoding becomes increasingly important for developing more efficient and effective models. The journey from absolute to relative position encodings illustrates how theoretical insights can lead to practical improvements in model performance and capabilities. . References: . https://arxiv.org/abs/2104.09864 | https://kexue.fm/archives/8265 | .",
            "url": "https://numb3r33.github.io/experiments/llm/transformers/math/deeplearning/2025/03/01/transformers-positional-encoding-evolution.html",
            "relUrl": "/llm/transformers/math/deeplearning/2025/03/01/transformers-positional-encoding-evolution.html",
            "date": " • Mar 1, 2025"
        }
        
    
  
    
        ,"post4": {
            "title": "Understanding Transformer Positional Encodings - A Mathematical Deep Dive",
            "content": "Introduction . When we process sequential data like text, the order of elements matters tremendously. The sentence &quot;dog bites man&quot; conveys a very different meaning than &quot;man bites dog,&quot; despite containing exactly the same words. This simple example highlights why position information is crucial for understanding sequences. . Transformer architectures revolutionized natural language processing with their self-attention mechanism, allowing models to process entire sequences in parallel rather than sequentially like RNNs. However, this parallelization comes with a challenge: self-attention is inherently position-agnostic. If we simply feed word embeddings into a transformer, the model has no way of knowing which word came first, second, or last. . This position-blindness creates a fundamental problem. How can transformers understand the sequential nature of language without sacrificing their parallelization advantage? The answer lies in positional encodings - specially designed vectors that inject position information into the model. In this blog, we&#39;ll take a deep mathematical dive into how transformer positional encodings work, particularly focusing on the elegant sinusoidal solution presented in the &quot;Attention Is All You Need&quot; paper. . Let&#39;s explore the mathematical elegance that allows transformers to understand sequence order while maintaining their computational advantages. . The Challenge of Position in Self-Attention . To understand why positional encodings are necessary, we must first examine the self-attention mechanism&#39;s architecture. In the most basic form, self-attention operates on a set of input vectors and computes weighted connections between them. . In the self-attention mechanism, input tokens are converted to query (q), key (k), and value (v) vectors through linear transformations. The attention weights are then computed via dot products between queries and keys, determining how much each token should &quot;attend&quot; to other tokens. . Mathematically, for each token&#39;s position i, the attention mechanism computes: . $$ text{Attention}(Q, K, V) = text{softmax} left( frac{QK^T}{ sqrt{d_k}} right)V$$ . The critical observation is that this formulation is permutation invariant. If we shuffle the order of tokens in the input sequence, the computed attention patterns would adjust accordingly but still produce mathematically equivalent results. There&#39;s nothing in the core attention mechanism that encodes or preserves information about the absolute or relative positions of tokens. . As noted in our analysis: &quot;These are indistinguishable information for self-attention because the operation of self-attention is undirected.&quot; This position-blindness is a fundamental limitation that needs to be addressed. . Positional Encoding Requirements . Before diving into specific encoding strategies, let&#39;s establish what makes a good positional encoding. From our analysis, we can identify three critical requirements: . Absolute Position Representation: The encoding must uniquely identify the absolute position of each token in the sequence (e.g., first token is 1, second token is 2). . | Relative Position Consistency: When sequences have different lengths, the relative positions/distances between tokens must remain consistent. For example, the relative distance between positions 2 and 4 should be encoded the same way regardless of whether the sequence has 10 tokens or 100 tokens. . | Length Generalization: The encoding system must work for sequence lengths that the model has never seen during training. This is crucial for practical applications where input lengths can vary widely. . | These requirements create interesting constraints on the mathematical properties our positional encoding must satisfy. Let&#39;s explore different approaches and see how they measure up against these requirements. . Approaches to Positional Encoding . Integer Positional Encoding . The most intuitive approach might be to simply use integer values to mark positions: . $$ text{position}_i = i$$ . Where i represents the position in the sequence (1st, 2nd, 3rd, etc.). This natural encoding labels the first token as 1, the second as 2, and so on. . However, this approach faces a significant problem. As our analysis points out: &quot;model may encounter sequences longer than training, not conducive to generalization of model.&quot; The issue is that position values become unbounded as the sequence length increases. If a model is trained on sequences of maximum length 512 but then encounters a sequence of length 1000, positions 513-1000 would be completely out of the training distribution. . Additionally, as the length of sequences increases, position values grow larger and larger, potentially causing numerical instability or dominating the actual content embeddings. . Bounded Range Encoding . To address the unbounded nature of integer encoding, we can normalize positions to a bounded range [0,1]: . $$ text{position}_i = frac{i-1}{L-1}$$ . Where L is the sequence length, mapping the first position to 0 and the last position to 1. . This approach ensures that regardless of sequence length, position values remain bounded between 0 and 1. For example: . For a 3-token sequence: [0, 0.5, 1] | For a 4-token sequence: [0, 0.33, 0.67, 1] | . This neatly addresses the generalization problem for variable sequence lengths. However, it introduces a new issue: the relative distances between tokens now depend on sequence length. In a 3-token sequence, adjacent tokens have a positional difference of 0.5, while in a 4-token sequence, adjacent tokens have a difference of 0.33. . This inconsistency in relative positions violates our second requirement and can make it harder for the model to learn consistent patterns across sequences of different lengths. . Vector-Based Positional Encoding . To overcome the limitations of scalar position values, we can move to vector-based representations where we use a vector with the same dimension as our token embeddings to represent position. . One approach is to use binary vectors for positional encoding. In this method, we represent positions using binary vectors where different dimensions encode different aspects of position. . For a token at position a₀ with d_model = 3, we might have: . a₀ a₁ a₂ a₃ a₄ a₅ a₆ a₇ 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 . This creates a unique binary signature for each position. However, this approach still has limitations in terms of generalization to unseen sequence lengths and maintaining consistent relative distances. . Sinusoidal Positional Encoding: The Elegant Solution . The transformer architecture introduced an elegant solution: sinusoidal positional encodings. We need functions that are bounded and continuous, with the sine function being the simplest option. . The sinusoidal encoding defines each dimension of the positional encoding vector as follows: . $$PE_{(t,2i)} = sin left( frac{t}{10000^{2i/d_{ text{model}}}} right)$$ . $$PE_{(t,2i+1)} = cos left( frac{t}{10000^{2i/d_{ text{model}}}} right)$$ . Where: . t is the position in the sequence | i is the dimension index (ranging from 0 to d_model/2-1) | d_model is the dimensionality of the model embeddings | . This creates a unique positional fingerprint for each position t, where each dimension oscillates at a different frequency. The frequencies form a geometric progression from 1 to 1/10000, providing a rich spectrum of periodic signals. . We can express the full positional encoding vector as: . $$PE_t = left[ sin( omega_1 t), cos( omega_1 t), sin( omega_2 t), cos( omega_2 t), ldots, sin( omega_{d_{ text{model}}/2} t), cos( omega_{d_{ text{model}}/2} t) right]$$ . Where: . $$ omega_i = frac{1}{10000^{2i/d_{ text{model}}}}$$ . Let&#39;s explore why this formulation is particularly effective by examining its mathematical properties. . Basic Properties of Sinusoidal Positional Encoding . 1. Uniqueness of Position Vectors . The vector representation for each position is unique because the frequencies of the sine and cosine functions are carefully selected to create distinct patterns for each position. As our analysis notes: &quot;The vector of each token is unique (the frequency of each sin function is small enough).&quot; This ensures that each position gets a distinctive representation. . 2. Bounded Values . All values in the positional encoding vector are bounded between -1 and 1, preventing numerical instability regardless of sequence length. This addresses the unboundedness issue of integer encodings. . 3. Frequency Spectrum . The use of different frequencies for different dimensions creates a rich representation. At lower values of t (positions near the beginning), high frequencies dominate, potentially creating overlap between position vectors. To avoid this, the frequencies are set to low values, achieved through the 10000 denominator term. . As noted in our analysis: &quot;Relationship between frequency, wavelength, and t → At lower values of t, frequency is high so there could be lots of overlap between position vectors. To avoid this, we try to lengthen the wavelength of function.&quot; . 4. Alternating Sine and Cosine . The alternation between sine and cosine functions for consecutive dimensions serves multiple purposes: . Creates unique vector representations for each position | Ensures values remain bounded in a continuous space | Facilitates generalization to sequence lengths not seen during training | . This alternating pattern also enables the encoding of relative positions through linear transformations, a property we&#39;ll explore next. . Advanced Mathematical Properties . The Linear Transformation Property . One of the most powerful properties of sinusoidal encodings is that they can represent both absolute and relative positions efficiently. The key insight from our analysis: &quot;Different position vectors can be obtained through linear transformation → this would help represent both absolute &amp; relative position of tokens.&quot; . Mathematically, we can express this as: . $$PE_{t+ Delta t} = [T_{ Delta t}] cdot PE_t$$ . Where $[T_{ Delta t}]$ is a linear transformation matrix that depends only on the offset $ Delta t$, not on the absolute position $t$. . This linear transformation corresponds to a rotation in the 2D subspace spanned by each sine-cosine pair. It follows from a fundamental property of sinusoidal functions: . $$ begin{pmatrix} sin(t+ Delta t) cos(t+ Delta t) end{pmatrix} = begin{pmatrix} cos Delta t &amp; sin Delta t - sin Delta t &amp; cos Delta t end{pmatrix} begin{pmatrix} sin(t) cos(t) end{pmatrix}$$ . This property means that the model can learn to &quot;shift&quot; positions through linear transformations, enabling it to understand relative positions in the sequence without explicitly computing them. . Linear Position Encoding: A Mathematical Necessity . A crucial mathematical property of effective positional encodings is that they must satisfy the condition: . $$ phi_m - phi_n = phi_{m-n}$$ . This condition ensures that the encoding correctly captures relative positions. Let&#39;s prove that linear encodings of the form $ phi_m = m theta$ are the only solution to this condition. . Step 1: Key Cases Analysis . Case 1: If n=0 $$ phi_m - phi_0 = phi_{m-0} = phi_m$$ This implies $ phi_0 = 0$. . Case 2: If n=1 $$ phi_m - phi_1 = phi_{m-1}$$ Rearranging: $$ phi_m = phi_{m-1} + phi_1$$ . This gives us a recurrence relation. . Step 2: Solving the Recurrence . Setting $ phi_1 = theta$ (a constant), we can solve the recurrence: $$ phi_2 = phi_1 + theta = 2 theta$$ $$ phi_3 = phi_2 + theta = 3 theta$$ $$ phi_4 = phi_3 + theta = 4 theta$$ . By induction, we get: $$ phi_m = m theta$$ . Step 3: Verifying the Solution . This solution satisfies the original condition: $$ phi_m - phi_n = m theta - n theta = (m-n) theta = phi_{m-n}$$ . Step 4: Why Non-Linear Solutions Fail . If $ phi_m$ were non-linear (for example, $ phi_m = m^2 theta$): $$ phi_m - phi_n = m^2 theta - n^2 theta = (m^2 - n^2) theta = (m+n)(m-n) theta$$ . But: $$ phi_{m-n} = (m-n)^2 theta$$ . Since $(m+n)(m-n) neq (m-n)^2$ in general, non-linear functions don&#39;t satisfy the condition. . Therefore, the only solution to $ phi_m - phi_n = phi_{m-n}$ is the linear function $ phi_m = m theta$. This mathematical necessity explains why sinusoidal encodings with linearly scaled frequencies are used in transformers. . Inner Product and Relative Position Dependency . Perhaps the most remarkable property of sinusoidal positional encodings is how they encode relative distances through inner products. The inner product between two position encodings depends only on the relative distance between them, not their absolute positions. . Let&#39;s derive this mathematically, starting with a complex number representation: . $$ langle PE_m, PE_n rangle = text{Re}[P_m P_n^*]$$ . Where $P_m = e^{im theta}$ and $P_n = e^{in theta}$ in the complex number representation. . Then: $$P_m P_n^* = e^{im theta} cdot e^{-in theta} = e^{i(m-n) theta}$$ . Taking the real part: $$ text{Re}[P_m P_n^*] = cos((m-n) theta)$$ . This elegant result shows that the inner product between two position encodings depends only on their relative offset (m-n), not their absolute positions. This is a crucial property for the self-attention mechanism, which relies heavily on inner products. . Extending to the full d-dimensional case: . $$ langle PE_m, PE_n rangle = sum_{i=0}^{d_{ text{model}}-1} cos((m-n) theta_i)$$ . This means that the similarity between position vectors naturally captures their relative distance, with the similarity decreasing as the distance increases. This property elegantly addresses our requirement for consistent relative position encoding. . Asymptotic Behavior and Frequency Choice . Why Inner Products Decay with Distance . A critical feature of the positional encoding is how the inner product between positions decays as their distance increases. This creates a natural attention bias toward local context while still allowing global interactions when needed. . For large values of |m-n|, we need to analyze integrals of the form: . $$I = int_0^1 e^{ix phi(t)} dt$$ . Where x = m-n (the relative distance) and $ phi(t)$ is the phase function. . For large |m-n|, these integrals decay due to rapid oscillations. This is a consequence of the Riemann-Lebesgue formula, which states that: . $$ lim_{x to infty} int_a^b e^{ix phi(t)} dt = 0$$ . Provided $ phi(t)$ is smooth and not constant. . Mathematical Analysis of Decay Rate . For the specific case of transformer positional encodings with $ phi(t) = e^{- lambda t}$ where $ lambda = ln(10000)$: . $$I = int_0^1 e^{ix e^{- lambda t}} dt$$ . The key insight is: &quot;For large x, dominant contribution comes from regions where phase $xe^{- lambda t}$ varies slowly. However, since $e^{- lambda t}$ decreases slowly, there is no stationary point and the integral decays as 1/x.&quot; . We can analyze this more rigorously through substitution and integration by parts: . Step 1: Substitution . Let $s = e^{- lambda t}$, then $t = - frac{ ln s}{ lambda}$ and $dt = - frac{1}{ lambda s}ds$. . This transforms the integral to: . $$I = frac{1}{ lambda} int_{e^{- lambda}}^1 frac{e^{ixs}}{s} ds$$ . Step 2: Integration by Parts . Using $u = frac{1}{s}$ and $dv = e^{ixs} ds$, we apply integration by parts: . $$I = frac{1}{ lambda} left[ frac{e^{ixs}}{ixs} right]_{e^{- lambda}}^1 + frac{1}{ix lambda} int_{e^{- lambda}}^1 frac{e^{ixs}}{s^2} ds$$ . For large x, the boundary term dominates: . $$I approx frac{1}{ lambda} left( frac{e^{ix} - e^{ixe^{- lambda}}}{ix} right) = O left( frac{1}{x} right)$$ . This confirms that the integral decays as O(1/x) for large values of x, which means the correlation between positions decays as the relative distance increases. . The Role of the Frequency Parameter . The specific choice of frequency parameter $ theta_k = 10000^{-2k/d_{ text{model}}}$ plays a critical role in how correlations decay with distance. . For the inner product between positions m and n: . $$ langle PE_m, PE_n rangle = sum_{k=0}^{d-1} cos((m-n) theta_k)$$ . For large d (high-dimensional embeddings), this sum can be approximated as an integral: . $$ langle PE_m, PE_n rangle approx frac{d}{2} cdot text{Re} left[ int_0^1 e^{i(m-n) theta_t} dt right]$$ . With $ theta_t = 10000^{-t}$ for $t in [0,1]$, which corresponds to $ theta_t = e^{- lambda t}$ where $ lambda = ln(10000)$. . Our analysis shows that this integral decays as O(1/|m-n|) for large |m-n|, creating a smooth falloff in attention with distance. . Interestingly, alternate frequency choices would produce different decay rates: . For θ = t^-1, I ∝ O(1/|x|) | For θ = t^-2, I ∝ O(1/|x|^(1/2)) | . The exponential frequency spacing used in transformer positional encodings creates a balance between local and global attention that works well in practice. . Absence of Stationary Points . The specific mathematical behavior of the positional encoding comes from the absence of stationary points in the phase function. For the phase function $ phi(t) = e^{- lambda t}$: . Phase Analysis: $ phi&#39;(t) = - lambda e^{- lambda t} &lt; 0$ for all $t &gt; 0$ | No Stationary Point: $ phi&#39;(t) neq 0$ for all $t in [0,1]$ | Monotonic Decay: $ phi(t)$ decreases exponentially and $ phi&#39;(t)$ never changes sign | . This absence of stationary points explains why the integral decays as O(1/|x|) rather than the faster decay rates typically seen in stationary phase approximations (which can be O(1/|x|^(1/2)) or faster). . This creates a gradual decay in attention with distance, rather than a sharp cutoff or a perfectly uniform attention pattern. . Breaking Symmetry: A Taylor Expansion Perspective . Another insightful way to understand positional encodings is through Taylor expansion. For a pure attention model without position information, the function is fully symmetric: . $$f(x_1, x_2, ..., x_n, ...) = f(x_n, ..., x_1, ...)$$ . This means transformers cannot recognize position - the output would be the same regardless of token order. By adding positional encodings, we break this symmetry. . Using Taylor expansion: . $$ tilde{f}(..., x_m + p_m, ..., x_n + p_n, ...) = f(..., x_m, ..., x_n, ...) + p_m frac{ partial f}{ partial x_m} + p_n frac{ partial f}{ partial x_n} + frac{1}{2}p_m^2 frac{ partial^2 f}{ partial x_m^2} + ...$$ . Where the terms $p_m frac{ partial f}{ partial x_m}$ contain position-dependent information. The key insight: &quot;As long as encoding vector of each position is different, this breaks the symmetry.&quot; . This Taylor expansion shows how positional information gets integrated with content information, allowing the model to distinguish between different token arrangements. . Practical Implications for Transformer Models . The mathematical properties we&#39;ve explored have significant practical implications for transformer models: . Generalization to unseen sequence lengths: Since sinusoidal encodings are defined for any position value, they can naturally handle sequences longer than those seen during training. This directly follows from the continuous nature of sine and cosine functions. . | Consistent relative positioning: The inner product properties ensure that relative positions are encoded consistently regardless of sequence length. As we proved, the inner product $ langle PE_m, PE_n rangle$ depends only on (m-n), not on absolute positions. . | Natural attention decay: The asymptotic decay properties (O(1/|m-n|)) align with the intuition that distant tokens typically have weaker relationships. This creates an inductive bias toward local context while still allowing global interactions. . | Parameter efficiency: Unlike learned positional embeddings, sinusoidal encodings don&#39;t require additional trainable parameters. The encoding is deterministic and can be computed on-the-fly. . | Computational efficiency: The encodings can be computed on-the-fly rather than stored in a lookup table, saving memory for very long sequences. . | Summary of Mathematical Properties . Bringing these mathematical analyses together: . The condition φₘ - φₙ = φₘ₋ₙ forces positional encoding angles to follow a linear pattern φₘ = mθ, which is exactly what sinusoidal encoding provides. . | The specific choice of frequency parameter θₖ = 10000⁻²ᵏ/ᵈ creates an inner product that decays as O(1/|m-n|) for large distances, due to the absence of stationary points in the phase function. . | This decay property helps the model naturally focus more on local context while still maintaining the ability to detect long-range dependencies when needed. . | As summarized in our analysis: . &quot;Sinusoidal encodings use frequencies that decay exponentially across dimensions&quot; | &quot;Inner Product Decay: Results from destructive interference in high-frequency oscillatory integrals&quot; | &quot;Design choice: θₖ = 10000⁻²ᵏ/ᵈ ensures smooth frequency coverage &amp; practical decay properties&quot; | . Conclusion . The sinusoidal positional encoding used in transformer models represents a beautiful intersection of mathematical elegance and practical utility. By encoding positions using sinusoidal functions at different frequencies, transformers gain the ability to understand sequence order while maintaining their parallelization advantages. . The key insights we&#39;ve explored include: . How sinusoidal functions provide a bounded, continuous representation of position | Why the inner product between position encodings naturally captures relative distances | How the specific frequency progression (10000⁻²ᵏ/ᵈ) creates a balanced representation | Why linear position encoding (φₘ = mθ) is the only solution that correctly encodes relative positions | How the asymptotic behavior creates a natural decay for distant token relationships | These mathematical properties combine to create a positional encoding scheme that satisfies all our requirements: representing absolute positions, maintaining consistent relative distances, and generalizing to unseen sequence lengths. . Understanding these mathematical foundations not only gives us deeper insight into transformer models but also opens doors to potential improvements and adaptations for specific tasks or domains. The elegant mathematics behind sinusoidal positional encodings reveals how transformers achieve their remarkable ability to understand sequence order while maintaining their computational advantages. . References . Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (Vol. 30). | Kazemnejad, A. (2019). Transformer Architecture: The Positional Encoding. https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ | Wang, P., &amp; Peng, X. (2021). Understanding and Improving Positional Encoding for Transformers. ArXiv:2012.15832. | .",
            "url": "https://numb3r33.github.io/experiments/llm/transformers/math/deeplearning/2025/02/22/transformers-positional-encoding.html",
            "relUrl": "/llm/transformers/math/deeplearning/2025/02/22/transformers-positional-encoding.html",
            "date": " • Feb 22, 2025"
        }
        
    
  
    
        ,"post5": {
            "title": "Understanding and Preventing Collapse in Self-Supervised Learning A Deep Dive into BYOL",
            "content": "Introduction . Why do machine learning models typically need labeled data to learn? The core challenge in self-supervised learning is to extract meaningful representations without explicit labels. Instead of relying on labeled datasets, can we learn representations simply by understanding relationships between examples? . Imagine teaching someone to distinguish between cats and dogs, but instead of providing explicit labels, you only tell them whether two animals belong to the same or different categories. Could they eventually learn the distinction? This thought experiment highlights a key insight: understanding relationships can sometimes be sufficient for learning meaningful representations. . BYOL (Bootstrap Your Own Latent) is a self-supervised learning framework that leverages this idea. Unlike earlier contrastive methods that rely on negative pairs, BYOL innovates by learning without explicit dissimilarity constraints. How does it avoid collapse while training with only positive pairs? Let’s dive into the details. . . 1. Traditional Supervised vs. Self-Supervised Learning . 1.1 Supervised Learning (ImageNet Pre-training) . Requires millions of labeled images | Each image is categorized into one of 1000 classes | Learns features through explicit supervision (e.g., &quot;this is a dog&quot;) | . 1.2 Self-Supervised Learning (BYOL Approach) . Uses unlabeled images | No need for human categorization | Learns features by understanding relationships between images | . Thought Experiment: Imagine you have: . 1 million labeled images (supervised approach) | 100 million unlabeled images (self-supervised approach) | Could self-supervised learning from a larger unlabeled dataset provide better or different kinds of features compared to supervised learning from fewer labeled examples? . . 2. How BYOL Works: Learning Without Negative Pairs . Unlike earlier contrastive methods (e.g., SimCLR), BYOL does not require negative pairs. Instead, it relies on two key mechanisms: . Data Augmentation to Create Positive Pairs: Given an image, two augmented views are created (e.g., cropping, color jittering, rotation). | The model learns to predict one view from the other. | . | Asymmetric Network Design: An online network and a target network work together. | The online network learns directly through gradient updates, while the target network updates slowly via an exponential moving average (EMA). | . | 2.1 Architectural Components of BYOL . Online Network: Encoder: $f_ theta$ | Projector: $g_ theta$ | Predictor: $q_ theta$ | . | Target Network: Encoder: $f_ xi$ | Projector: $g_ xi$ (EMA of online parameters) | . | 2.2 Loss Function . The BYOL loss is: $$ L_{ text{BYOL}} = mathbb{E} left[ |q_ theta(g_ theta(f_ theta(x))) - g_ xi(f_ xi(x^+)) |^2 right] $$ Unlike contrastive loss, it does not rely on negative pairs. . . 3. Avoiding Collapse: The Role of Asymmetry . 3.1 Representation Collapse in SSL . Collapse occurs when a model maps all inputs to the same representation, leading to trivial solutions: . Constant Representations: . ∃𝑐∈ℝ𝑑 s.t. 𝔼𝑥[|𝑓𝜃(𝑥)−𝑐|2]≈0 . | Dimensionality Collapse: Representations lie in a low-dimensional subspace. . | 3.2 How BYOL Avoids Collapse . BYOL prevents collapse through two mechanisms: . Predictor Asymmetry: . The predictor 𝑞𝜃 ensures that the representations do not collapse. | The loss function minimizes: . min𝜃‖𝑞𝜃(𝑧𝜃)−𝑧′𝜉‖2 . | . | EMA Dynamics: The target network updates via: 𝜉←𝜏𝜉+(1−𝜏)𝜃 | This creates a slowly evolving target that prevents trivial solutions. | . | . 4. Mathematical Insights . 4.1 Conditional Variance Perspective . BYOL minimizes: $$ mathbb{E}[ text{Var}(z&#39;_ xi | z_ theta)] $$ Using the law of total variance: $$ text{Var}(z&#39;_ xi) = mathbb{E}[ text{Var}(z&#39;_ xi|z_ theta)] + text{Var}( mathbb{E}[z&#39;_ xi|z_ theta]) $$ BYOL prevents collapse by balancing these terms. . 4.2 Stability Analysis . BYOL’s predictor introduces an implicit regularization: $$ min_ theta |q_ theta(z_ theta) - z&#39;_ xi |^2 quad text{s.t. } z_ theta = f_ theta(x), z&#39;_ xi = text{EMA}(f_ theta(x)) $$ . . 5. Empirical Insights and Practical Considerations . 5.1 Detecting Collapse . Feature Variance: $$ text{Var}(z_ theta) = frac{1}{d} sum_{i=1}^d text{Var}(z_ theta^{(i)}) $$ | Eigenvalue Spectrum: Collapse occurs when: $$ frac{ lambda_{ max}(C)}{ lambda_{ min}(C)} &gt; 10^3 $$ | 5.2 Recommended Hyperparameters . EMA Momentum: Warm up $ tau$ from 0.996 to 0.999 over 1000 steps. | Predictor Learning Rate: Should be 10× encoder LR. | Augmentations: Strong augmentations remain crucial. | . . 6. Key Takeaways . BYOL learns representations without negative pairs by leveraging moving target networks. | Architectural asymmetry is crucial in preventing collapse. | The predictor network acts as an implicit constraint, ensuring diverse representations. | The exponential moving average (EMA) smooths updates, maintaining stable learning dynamics. | . 7. Practical Implementation . Let&#39;s use fast.ai to implement the paper from scratch using a manageble dataset ( CIFAR10 ) and resnet18 for faster iterations. We can later scale it to larger datasets and models. . import torch import torch.nn as nn import torch.nn.functional as F from copy import deepcopy from fastai.vision.all import * import albumentations as A from albumentations.pytorch import ToTensorV2 class BYOL(nn.Module): def __init__(self, encoder=&#39;resnet18&#39;, projection_size=128, projection_hidden=256, prediction_hidden=64): super().__init__() # 1. Online encoder - initialize the model first encoder_model = getattr(models, encoder)(weights=None) self.online_encoder = nn.Sequential( *list(encoder_model.children())[:-1], # Remove final FC layer nn.AdaptiveAvgPool2d((1, 1)), # Add adaptive pooling nn.Flatten() # Add flatten layer ) # Get encoder output dimension self.online_encoder.eval() with torch.no_grad(): enc_out = self.online_encoder(torch.randn(4, 3, 32, 32)) enc_dim = enc_out.shape[1] self.enc_dim = enc_dim self.online_encoder.train() # 2. Projector self.online_projector = nn.Sequential( nn.Linear(enc_dim, projection_hidden), nn.BatchNorm1d(projection_hidden), nn.ReLU(inplace=True), nn.Linear(projection_hidden, projection_size) ) # 3. Predictor self.predictor = nn.Sequential( nn.Linear(projection_size, prediction_hidden), nn.BatchNorm1d(prediction_hidden), nn.ReLU(inplace=True), nn.Linear(prediction_hidden, projection_size) ) # 4. Target network - no gradients needed self.target_encoder = None self.target_projector = None self.init_target_network() def init_target_network(self): &quot;&quot;&quot;Initialize target network with online network weights&quot;&quot;&quot; self.target_encoder = deepcopy(self.online_encoder) self.target_projector = deepcopy(self.online_projector) for param in self.target_encoder.parameters(): param.requires_grad = False for param in self.target_projector.parameters(): param.requires_grad = False def forward(self, x): &quot;&quot;&quot; x is expected to be a tuple of (view1, view2) &quot;&quot;&quot; if isinstance(x, (tuple, list)): x1, x2 = x else: x1, x2 = x, x # For validation # Online network forward passes z1 = self.online_projector(self.online_encoder(x1)) z2 = self.online_projector(self.online_encoder(x2)) # Get predictions p1 = self.predictor(z1) p1 = F.normalize(p1, dim=-1) p2 = self.predictor(z2) p2 = F.normalize(p2, dim=-1) # Target network forward passes with torch.no_grad(): t1 = self.target_projector(self.target_encoder(x1)) t2 = self.target_projector(self.target_encoder(x2)) t1 = F.normalize(t1, dim=-1) t2 = F.normalize(t2, dim=-1) return p1, p2, t1.detach(), t2.detach() @torch.no_grad() def update_target_network(self, tau): &quot;&quot;&quot; Update target network parameters using exponential moving average tau: EMA decay rate (0,1) &quot;&quot;&quot; for online_p, target_p in zip(self.online_encoder.parameters(), self.target_encoder.parameters()): target_p.data = tau * target_p.data + (1 - tau) * online_p.data for online_p, target_p in zip(self.online_projector.parameters(), self.target_projector.parameters()): target_p.data = tau * target_p.data + (1 - tau) * online_p.data class BYOLLoss(nn.Module): def forward(self, preds, *args): &quot;&quot;&quot; Compute BYOL&#39;s loss function. preds: tuple of (p1, p2, t1, t2) from BYOL model &quot;&quot;&quot; p1, p2, t1, t2 = preds # Symmetric loss loss = ( 2 - 2 * F.cosine_similarity(p1, t2.detach(), dim=-1).mean() + 2 - 2 * F.cosine_similarity(p2, t1.detach(), dim=-1).mean() ) / 2 return loss class BYOLMetrics(Callback): def __init__(self, τ_base=0.996): super().__init__() self.τ_base = τ_base self.feat_var = None # Store feat_var as instance variable self.output_similarity = None # Track output similarity def before_train(self): self.total_steps = len(self.learn.dls.train) * self.learn.n_epoch self.current_step = 0 def after_batch(self): # Update target network τ = 1 - (1 - self.τ_base) * (math.cos(math.pi * self.current_step / self.total_steps) + 1) / 2 self.learn.model.update_target_network(τ) # Compute feature variance (collapse detection) if not self.training: return with torch.no_grad(): # Handle tuple input - take first view x = self.xb[0][0] if isinstance(self.xb[0], tuple) else self.xb[0] z = self.learn.model.online_projector( self.learn.model.online_encoder(x) ) t = self.learn.model.target_projector( self.learn.model.target_encoder(x) ) # Compute cosine similarity self.output_similarity = F.cosine_similarity( z, t, dim=1 ).mean().item() self.feat_var = z.var(dim=0).mean().item() # Check for collapse if self.feat_var &lt; 1e-4: raise CancelFitException(&quot;Training collapsed!&quot;) self.current_step += 1 def after_epoch(self): &quot;&quot;&quot;Log feature variance at the end of each epoch&quot;&quot;&quot; if self.feat_var is not None: print(f&quot;Feature variance: {self.feat_var:.6f}&quot;) print(f&quot;Output similarity: {self.output_similarity:.6f}&quot;) def get_augmentations(): &quot;&quot;&quot;Get BYOL&#39;s augmentation pipeline&quot;&quot;&quot; return A.Compose([ A.RandomResizedCrop(32, 32), A.HorizontalFlip(p=0.5), A.ColorJitter( brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8 ), A.ToGray(p=0.2), A.GaussianBlur(p=0.1), A.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) ]) class TwoCropsTransform: def __init__(self, aug): self.aug1 = aug self.aug2 = deepcopy(aug) def __call__(self, x): # Convert PILImage/Tensor to numpy array if isinstance(x, torch.Tensor): img = x.numpy() elif isinstance(x, PILImage): img = np.array(x) else: img = x if img.ndim == 2: # Handle grayscale img = np.repeat(img[..., None], 3, axis=-1) # Create two views view1 = self.aug1(image=img)[&#39;image&#39;] view2 = self.aug2(image=img)[&#39;image&#39;] # Convert to NCHW format view1 = torch.from_numpy(view1).float().permute(2, 0, 1) # HWC -&gt; CHW view2 = torch.from_numpy(view2).float().permute(2, 0, 1) # HWC -&gt; CHW return ( TensorImage(view1), TensorImage(view2) ) def get_data(batch_size=256): &quot;&quot;&quot;Setup CIFAR-10 with dual augmentations&quot;&quot;&quot; path = untar_data(URLs.CIFAR) dblock = DataBlock( blocks=(ImageBlock, ImageBlock), # One block for each view get_items=get_image_files, splitter=RandomSplitter(), item_tfms=TwoCropsTransform(get_augmentations()) ) return dblock.dataloaders( path, bs=batch_size, num_workers=4 ) class FeatureVarianceMetric(Metric): def __init__(self): self.reset() def reset(self): self.total = 0 self.count = 0 def accumulate(self, learn): x = learn.xb[0][0] if isinstance(learn.xb[0], tuple) else learn.xb[0] with torch.no_grad(): z = learn.model.online_projector(learn.model.online_encoder(x)) var = z.var(dim=0).mean().item() self.total += var self.count += 1 @property def value(self): return self.total/self.count if self.count != 0 else None dls = get_data() model = BYOL() def test_target_network_initialization(): model = BYOL() for t, o in zip(model.target_encoder.parameters(), model.online_encoder.parameters()): assert torch.allclose(t, o) assert not t.requires_grad def test_ema_update(): model = BYOL() initial_weight = model.target_encoder[0].weight.clone() model.online_encoder[0].weight.data += 0.1 model.update_target_network(0.9) new_weight = model.target_encoder[0].weight expected = 0.9 * initial_weight + 0.1 * model.online_encoder[0].weight assert torch.allclose(new_weight, expected, atol=1e-6) def test_normalization(): model = BYOL() x = torch.randn(4, 3, 32, 32) p1, p2, t1, t2 = model((x, x)) assert torch.allclose(p1.norm(dim=-1), torch.ones(4), atol=1e-4) # After loss norm test_target_network_initialization() test_ema_update() test_normalization() learn = Learner( dls, model, loss_func=BYOLLoss(), metrics=[FeatureVarianceMetric()], cbs=[ BYOLMetrics(), GradientClip(1.0), MixedPrecision() ] ) learn.fit_one_cycle( 10, # epochs lr_max=2e-3 ) . . 7.1 Validation . Linear Evaluation . linear evaluation involves freezing the encoder and training a linear classifier on top using labeled data. This is a standard way to assess the quality of learned representations. . class LinearEval(nn.Module): def __init__(self, encoder, num_classes=14): super().__init__() self.encoder = encoder self.classifier = nn.Linear(learn.enc_dim, num_classes) # Freeze encoder for param in self.encoder.parameters(): param.requires_grad = False def forward(self, x): with torch.no_grad(): # Critical for correct evaluation features = self.encoder(x) return self.classifier(features) def get_linear_data(): &quot;&quot;&quot;Modified DataBlock for labeled Pets&quot;&quot;&quot; path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) pat = r&#39;^(.*)_ d+.jpg$&#39; # extract pet breed from filename, e.g. &quot;Abyssinian_39.jpg&quot; → &quot;Abyssinian&quot; return DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=RegexLabeller(pat), item_tfms=Resize(32), batch_tfms=aug_transforms() # Standard supervised augs ).dataloaders(path/&#39;images&#39;, bs=32, num_workers=4) linear_dls = get_linear_data() encoder = deepcopy(learn.model.online_encoder) linear_model = LinearEval(encoder) learn_linear = Learner( linear_dls, linear_model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=MixedPrecision() ) def test_encoder_frozen(): for param in learn_linear.model.encoder.parameters(): assert not param.requires_grad def test_feature_consistency(): x = torch.randn(4, 3, 32, 32).cuda() with torch.no_grad(): f1 = learn.model.online_encoder(x) f2 = learn.model.online_encoder(x) assert torch.allclose(f1, f2, atol=1e-6) # Deterministic features test_encoder_frozen(), test_feature_consistency() learn_linear.fit_one_cycle( 10, lr_max=3e-4 ) . . 8. Conclusion . BYOL showcases that self-supervised learning can be effective without contrastive learning. By leveraging moving target networks, asymmetric predictors, and implicit variance regularization, BYOL remains a foundational approach for learning powerful image representations. .",
            "url": "https://numb3r33.github.io/experiments/self-supervised-learning/math/deeplearning/fast.ai/2025/02/04/byol.html",
            "relUrl": "/self-supervised-learning/math/deeplearning/fast.ai/2025/02/04/byol.html",
            "date": " • Feb 4, 2025"
        }
        
    
  
    
        ,"post6": {
            "title": "Gradient Clipping and Adaptive Learning Rates",
            "content": "Introduction . Gradient clipping is a common technique to make model training more stable. It&#39;s operation could be expressed as . . The clipping threshold τ is often set to 1. Is this just a default value, or is there something deeper? | . The above formula ensures that if your gradient&#39;s magnitude exceeds τ, it gets scaled down while maintaining the direction. . But what exactly is ‖g‖, it is defined as &quot;global gradient norm&quot; in the literature | . Let&#39;s try to understand it with a simple example, imagine a tiny neural network . layer1 = Linear(2, 2) # 2x2 weight matrix + 2 biases = 6 parameters layer2 = Linear(2, 1) # 2x1 weight matrix + 1 bias = 3 parameters . When computing gradients, each parameter gets its own gradient. The global gradient norm is calculated by: . Flattening all gradients into one long vector | Computing the L2 norm ( Euclidean Norm ) of this vector | It is like measuring the &quot;overall strength&quot; of all gradients combined. . But why τ=1 works so well. What&#39;s the mathematical intuition? | . The key insight comes from looking at how model updates affect the loss function. For SGD, we can write . . For vanilla SGD $$u_t = g_t$$ . We can rewrite the above equation as . . The key insight is that change in loss is proportional to the square of the gradient norm. For stable training, we typically want |ΔL| &lt; η ( change in loss to be less than the learning rate ). . Therefore: . |-η‖gt‖²| &lt; η . η‖gt‖² &lt; η . ‖gt‖² &lt; 1 . ‖gt‖ &lt; 1 . Let&#39;s try to verify this claim by running some experiments on MNIST dataset using fast.ai . from __future__ import annotations from fastai.torch_basics import * from fastai.vision.all import * . import pandas as pd import matplotlib.pyplot as plt from torch.utils.data import DataLoader from functools import partial . class GradientTracker(Callback): &quot;&quot;&quot;Callback to track gradient norms during training&quot;&quot;&quot; def __init__(self): self.grad_norms = [] self.loss_changes = [] self.last_loss = None def before_batch(self): # Store the loss value before the batch if self.learn.loss_func is not None: self.last_loss = self.learn.loss.item() if self.learn.loss is not None else None def after_backward(self): # Calculate global gradient norm total_norm = 0. for p in self.learn.model.parameters(): if p.grad is not None: param_norm = p.grad.data.norm(2) total_norm += param_norm.item() ** 2 total_norm = total_norm ** 0.5 self.grad_norms.append(total_norm) # Calculate change in loss if self.last_loss is not None and self.learn.loss is not None: loss_change = abs(self.learn.loss.item() - self.last_loss) self.loss_changes.append(loss_change) def plot_stats(self): &quot;&quot;&quot;Plot gradient norms and loss changes&quot;&quot;&quot; fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10)) # Plot gradient norms ax1.plot(self.grad_norms) ax1.axhline(y=1.0, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=&#39;τ=1.0&#39;) ax1.set_title(&#39;Gradient Norms During Training&#39;) ax1.set_xlabel(&#39;Batch&#39;) ax1.set_ylabel(&#39;Global Gradient Norm&#39;) ax1.legend() # Plot loss changes ax2.plot(self.loss_changes) ax2.axhline(y=self.learn.lr, color=&#39;r&#39;, linestyle=&#39;--&#39;, label=f&#39;η={self.learn.lr}&#39;) ax2.set_title(&#39;Absolute Loss Changes During Training&#39;) ax2.set_xlabel(&#39;Batch&#39;) ax2.set_ylabel(&#39;|ΔL|&#39;) ax2.legend() plt.tight_layout() return fig . def create_experiment(clip=True): path = untar_data(URLs.MNIST) dls = ImageDataLoaders.from_folder(path, valid_pct=0.2, batch_size=128, item_tfms=Resize(28), n_inp=1 ) model = nn.Sequential( nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(32 * 7 * 7, 10) ) # Create learner with gradient tracking learn = Learner(dls, model, metrics=accuracy) grad_tracker = GradientTracker() learn.add_cb(grad_tracker) # Add gradient clipping if specified if clip: learn.add_cb(GradientClip(max_norm=1.0)) # Train for a few epochs learn.fit_one_cycle(5, 1e-2) return learn, grad_tracker . learn_clip, tracker_clip = create_experiment(clip=True) learn_no_clip, tracker_no_clip = create_experiment(clip=False) . . fig_clip = tracker_clip.plot_stats() plt.suptitle(&#39;With Gradient Clipping (τ=1.0)&#39;) . . fig_no_clip = tracker_no_clip.plot_stats() plt.suptitle(&#39;Without Gradient Clipping&#39;) . . comparison = pd.DataFrame({ &#39;Metric&#39;: [&#39;Mean Gradient Norm&#39;, &#39;Max Gradient Norm&#39;, &#39;Mean |ΔL|&#39;, &#39;Max |ΔL|&#39;], &#39;With Clipping&#39;: [ np.mean(tracker_clip.grad_norms), np.max(tracker_clip.grad_norms), np.mean(tracker_clip.loss_changes), np.max(tracker_clip.loss_changes) ], &#39;Without Clipping&#39;: [ np.mean(tracker_no_clip.grad_norms), np.max(tracker_no_clip.grad_norms), np.mean(tracker_no_clip.loss_changes), np.max(tracker_no_clip.loss_changes) ] }) print(&quot; nComparison Statistics:&quot;) print(comparison.to_string(index=False)) . . What the above experiment suggests that . Early Training ( 0-500 batches ): . Global Gradient Norm naturally start large ( &gt; 1) | Without clipping, they reached 8.0 | With clipping, they were capped at ~4 | . | Middle/Late Training ( 500+ batches ) . Both experiments naturally converged to gradient norms &lt; 1 | Mean gradient norms were similar ( 0.48 v 0.49 ) | . | This confirms that ||g|| &lt; 1 is indeed the natural state of a well trained model. . We also know from literature that if the gradient modulus is significanly greather than 1 in the early state, then usual strategy is warmup ir we can consider a more general strategy: set another threshold $ tau$ . . For optimizers like Adam . . How does $u_t$ becomes $sign(g_t)$ and $sign(g_t)$ . $g_t$ becomes $ |g_t |_1$? . $u_t$ = $sign(g_t)$ because Adam&#39;s updates tend to normalize gradient magnitudes while preserving direction. . And when we multiply a number by it&#39;s sign, you get its absolute value. . This explains why Adam typically needs smaller learnings rates than SGD as L1 norm is usually larger than L2 norm. . The above formula could also be written as . . Since $sign(g_t)$ is a vector of either +1 or -1 thereforme $||sign(g_t)||^2$ = N so therefore $||sign(g_t)||$ = $ sqrt(N)$ . and . a . b = ||a|| . ||b||. cos(a, b) . therefore . $sign(g_t) . gt$ = $ sqrt(N) . ||g_t|| . cos(sign(g_t), g_t)$. $cos(sign(g_t, g_t))$ is roughly constant across model scales so if we want to maintain $ Delta L$ $ eta$ should be inversely proportional to $ sqrt(N)$, which means if the number of model parameter increases by 4 times, learning rate can be halved. . Takeaways . τ=1 isn&#39;t arbitrary - it aligns with the natural scale of gradients during stable training. | The relationship |ΔL| &lt; η mathematically explains why gradients naturally stay below 1 during stable training. | Different optimizers (SGD vs Adam) have different relationships between gradients and loss changes, which affects their learning rate requirements. | When scaling to larger models, learning rates should scale with 1/√N (where N is parameter count) to maintain consistent training dynamics. | . This understanding helps us move from blindly using default values to making principled decisions about optimization hyperparameters in deep learning! . References . https://kexue.fm/archives/10657 | https://papers.cool/arxiv/2310.07831 | .",
            "url": "https://numb3r33.github.io/experiments/optimization/math/deeplearning/fast.ai/2025/01/12/gradient-clipping.html",
            "relUrl": "/optimization/math/deeplearning/fast.ai/2025/01/12/gradient-clipping.html",
            "date": " • Jan 12, 2025"
        }
        
    
  
    
        ,"post7": {
            "title": "Deep-Contextualized Embeddings ( ELMO )",
            "content": "In this post, I will explore the key ideas presented in the ELMo paper and discuss what it teaches us as a research community. The focus will be on understanding the big concepts and implementing them using fast.ai. To keep it practical, we will work on a toy dataset and prioritize implementation over reproducing the original paper’s results. . Paper details . Contextualized Word Embeddings . Limitations of Word2Vec paper . Why Settle for One Embedding When Words Have Multiple Meanings? . Consider the word stick in these sentences: . Let’s stick to the plan. | He picked up a stick. | . The meaning of stick changes based on its context. So, why not embed words in a way that captures this contextual nuance? . ELMo (Embeddings from Language Models) solves this problem by creating contextualized word embeddings. Instead of static embeddings, ELMo uses multiple layers to generate embeddings based on surrounding context. . The architecture consists of the following key components (as illustrated in the figure): . Embedding Layer | Two-layer BiLSTM | Top Layer: Contextualized Word Representation | . Character Embedding Layer . The Character Embedding Layer in ELMo represents words using character-level features. This layer helps capture morphological details such as prefixes and suffixes, which are especially useful for handling out-of-vocabulary words and subword structures. . Key components: . Character Embeddings: Each word is broken into characters, and an embedding is assigned to each character. | CNN for Feature Extraction: A Convolutional Neural Network (CNN) is applied to extract character-level features from the input word. | Multiple Convolutional Kernels: The CNN uses multiple kernels (filters), each corresponding to different window sizes. This enables the model to learn features at varying character-level contexts. | Feature Vector: The output is a fixed-size character-level feature vector that encodes morphological information for the word. | . Bi-LSTM Layer . The Bi-LSTM Layer in ELMo is designed to capture rich semantic information by processing text bidirectionally: . It scans the input left-to-right and right-to-left simultaneously. The input passes through the following sequence of layers: . Character-based CNN Encoder: Extracts character-level features. | Two Bi-LSTM Layers: Captures context-dependent word representations by combining information from both directions. | . This bidirectional approach ensures that the embeddings are deeply contextualized, incorporating information from the entire sentence. . Character CNN Encoder . The Character CNN Encoder processes the input at the character level to generate dense representations for words. Below is the step-by-step breakdown: . Input Shape: | . [batch, max_sentence_len, max_word_len] . Reshape: The input is reshaped to: | . [batch * max_sentence_len, max_word_len] . Character Embedding: Pass the reshaped input through an embedding layer to get: | . [batch * max_sentence_len, max_word_len, embedding_dim] . embedding_dim is a predefined hyperparameter, e.g., 16. The embedding weights are learned during training. This step maps the one-hot encoded input of size 50 to a dense representation of size 16. . Transpose: Transpose the output to prepare it for convolution: | . [batch * max_sentence_len, embedding_dim, max_word_len] . Convolutional Layers: Pass the transposed output through multiple convolutional layers with varying kernel sizes and output channels: | . [batch * max_sentence_len, out_channels, new_h] . Max Pooling: Apply max pooling along the h dimension to obtain: | . [batch * max_sentence_len, out_channels] . Concatenation: Concatenate the outputs of all convolutional layers along the out_channels dimension to form: | . [batch * max_sentence_len, n_filters] n_filters represents the total number of output channels summed across all kernels. . Highway Layers: Pass the concatenated output through two highway layers, defined as: | . y = g * x + (1 - g) * f(A(x)) where g = Sigmoid(B(x)) . This operation preserves the output shape. . Linear Projection: Apply a linear projection to map the output to the desired dimension: | . [batch * max_sentence_len, output_dim] . Final Reshape: Reshape the output back to: | . [batch, max_sentence_len, output_dim] . . Bi-LSTM . Char CNN&#39;s output passes through forward layer [ batch, max_sentence_len, hidden_size ] and backward layer [ batch, max_sentence_len, hidden_size ] | Concat forward and backward data in the hidden_size dim to get [b, max_sentence_len, 2 * hidden_size] | . from fastai.torch_basics import * from fastai.text.all import * . from datasets import load_dataset dsd = load_dataset(&#39;karpathy/tiny_shakespeare&#39;, trust_remote_code=True) train, valid, test = dsd[&#39;train&#39;][0][&#39;text&#39;], dsd[&#39;validation&#39;][0][&#39;text&#39;], dsd[&#39;test&#39;][0][&#39;text&#39;] . We would use the tiny-shakespeare dataset which contains 40,000 lines of Shakespeare from a variety of Shakespeare&#39;s plays. . @delegates() class ELMODataLoader(LMDataLoader): &quot;A `DataLoader` suitable for Continuous Bag of Words (CBOW)&quot; def __init__(self, dataset, context_size=2, bs=64, num_workers=0, **kwargs): self.context_size = context_size super().__init__(dataset, bs=bs, num_workers=num_workers, **kwargs) def create_item(self, idx): if idx is None: idx = 0 if is_listy(self.dataset[idx][0]): tokens, char_tokens = self.dataset[idx][0][0], self.dataset[idx][0][1] else: tokens, char_tokens = self.dataset[idx][0], self.dataset[idx][1] if len(tokens) &lt; 2 * self.context_size + 1: raise IndexError(&quot;Sentence too short for the given context size&quot;) context = torch.hstack((tokens[:self.context_size], tokens[self.context_size+1:])) target = tokens[self.context_size] return (TensorText(context), char_tokens), TensorCategory(target) @delegates(TfmdDL.new) def new(self, dataset=None, context_size=None, **kwargs): context_size = self.context_size if context_size is None else context_size return super().new(dataset=dataset, context_size=context_size, **kwargs) . Extending LMDataLoader for CBOW-style Data . To prepare data in the Continuous Bag of Words (CBOW) style, we modify the LMDataLoader to return (context, target word) pairs. Here, the context is a group of surrounding words used to predict the target word. . Example: Given the sequence: . x1, x2, y, x3, x4 . If the context size is 2, the model will use: . Context: x1, x2, x3, and x4 Target: y . texts = list(filter(lambda x: len(x.split()) &gt; 7, train[:100000].split(&#39; n&#39;))) n_trn = len(texts) val_txts = list(filter(lambda x: len(x.split()) &gt; 7, valid.split(&#39; n&#39;))) texts.extend(val_txts) # Create a DataFrame df = pd.DataFrame({&#39;texts&#39;: texts}) splits = L([np.arange(n_trn), np.arange(n_trn, n_trn+len(val_txts))]) df.shape . Only consider those lines which have atleast 7 words in the training and validation dataset. . class NGramsTokenizer(): def __init__(self, n=1): self.n = n def __call__(self, sent): words = sent.split() ngrams = [[&#39;&#39;.join(word[i:i+self.n]) for i in range(len(word)-self.n+1)] for word in words] return ngrams class NumericalizeChars(Numericalize): def encodes(self, x): return [[self.o2i[l_] for l_ in o_ ] for o_ in x] class ElmoTransform(Transform): def __init__(self, word_vocab, char_vocab=None): self.num_word = Numericalize(min_freq=2) self.num_char = NumericalizeChars(min_freq=2) self.num_word.setup(word_vocab) self.num_char.setup(char_vocab) def encodes(self, x): tok_word = Tokenizer.from_df(&#39;texts&#39;)(x) tok_char = NGramsTokenizer()(x.lower()) numer_words = self.num_word(tok_word) numer_chars = self.num_char(tok_char) return (numer_words,numer_chars) word_vocab = L(x for x in df.texts.str.split()) char_vocab = L(x for x in df.texts.map(set)) pipe = TfmdLists(df, [attrgetter(&#39;texts&#39;), ElmoTransform(word_vocab=word_vocab, char_vocab=char_vocab)]) pipe[0] . Define a custom transform to numericalize both word and character tokens. . class ElmoPadChunk(Transform): def __init__(self, pad_word_idx=1, pad_char_idx=0, pad_first=False, seq_len=72, word_len=32, decode=True, **kwargs): store_attr(&#39;pad_word_idx, pad_char_idx, pad_first, seq_len,seq_len,word_len&#39;) def _get_max_word_len(self, batch): return max([len(s[0][0]) for s in batch]) def _get_max_char_len(self, batch): return max([len(x) for s in batch for x in s[0][1]]) def encodes(self, batch): X, lbl = zip(*batch) xw, xc = zip(*X) xw = [(x, ) for x in xw] self.max_word_len = self._get_max_word_len(batch) self.max_char_len = self._get_max_char_len(batch) xc = [ f[:self.max_word_len] if len(f) &gt; self.max_word_len else f + [[self.pad_word_idx]]*(self.max_word_len - len(f)) for f in xc ] xc = [[w + [self.pad_char_idx]*(self.max_char_len - len(w)) for w in f] for f in xc] pw = pad_input_chunk(xw, pad_idx=self.pad_word_idx, pad_first=self.pad_first, seq_len=self.max_word_len ) pwc = default_collate([x[0] for x in pw]) pc = TensorText(xc) pc = pc.transpose(1, 2) lbls = default_collate(lbl, ) return pc, lbls . vocab_tfm = ElmoTransform(word_vocab=word_vocab, char_vocab=char_vocab) vw = vocab_tfm.num_word.vocab vc = vocab_tfm.num_char.vocab tfms = [attrgetter(&#39;texts&#39;), vocab_tfm] dsets = Datasets(df, [tfms], splits=splits, dl_type=ELMODataLoader) bs,cs = 16,1 dl_kwargs = { &#39;before_batch&#39;: ElmoPadChunk(pad_word_idx=1, pad_char_idx=1), &#39;create_batch&#39;: fa_convert } dls = dsets.dataloaders(bs=bs, context_size=cs, **dl_kwargs) . class CharacterLayer(Module): def __init__(self, input_dim, embedding_dim, output_dim, filters_list, kernel_size_list, highway_num): store_attr() self.embedding = nn.Embedding(input_dim, embedding_dim) self.convs = nn.ModuleList([ nn.Conv1d(embedding_dim, f, k, padding=&#39;same&#39;) for f, k in zip(filters_list, kernel_size_list) ]) self.dim = sum(filters_list) self.highways = nn.ModuleList([nn.Linear(self.dim, self.dim*2) for _ in range(highway_num)]) self.projection = nn.Linear(self.dim, output_dim) def forward(self, x): # x: (batch_size, seq_len, token_len) bs, seq_len, token_len = x.shape # Flatten (batch_size, seq_len) for embedding x = x.reshape(bs*seq_len, token_len) x = self.embedding(x) # (bs*seq_len, token_len, embedding_dim) x = x.permute(0, 2, 1) # (bs*seq_len, embedding_dim, token_len) # Convolutions + max pooling outs = [] for conv in self.convs: c_out = conv(x) # (bs*seq_len, f, token_len) c_out = torch.max(c_out, dim=2)[0] # max pool along token_len c_out = F.relu(c_out) outs.append(c_out) x = torch.cat(outs, dim=-1) # (bs*seq_len, sum(filters_list)) # Highway layers for hw in self.highways: hw_out = hw(x) # (bs*seq_len, 2*self.dim) a = F.relu(hw_out[:, :self.dim]) g = torch.sigmoid(hw_out[:, self.dim:]) x = a*g + x*(1-g) # Projection x = self.projection(x) # (bs*seq_len, output_dim) x = x.reshape(bs, seq_len, -1) # (bs, seq_len, output_dim) return x . class Elmo(Module): def __init__(self, lstm_num): store_attr() self.embedding = CharacterLayer( input_dim=len(vc), embedding_dim=16, output_dim=50, filters_list=[12,24], kernel_size_list=[2,3], highway_num=2 ) self.forward_lstm = nn.ModuleList() self.backward_lstm = nn.ModuleList() for _ in range(lstm_num): self.forward_lstm.append(nn.LSTM(50, 50, batch_first=True)) self.backward_lstm.append(nn.LSTM(50, 50, batch_first=True)) # Append one more forward and backward LSTM as in the original code self.forward_lstm.append(nn.LSTM(50, 50, batch_first=True)) self.backward_lstm.append(nn.LSTM(50, 50, batch_first=True)) self.forward_projection = nn.Linear(50, len(vw)) self.backward_projection = nn.Linear(50, len(vw)) def _run_lstm(self, lstm, x, backwards=False): # If backwards=True, we simulate go_backwards by flipping the sequence dimension if backwards: x = torch.flip(x, dims=[1]) x, _ = lstm(x) if backwards: x = torch.flip(x, dims=[1]) return x def forward(self, inputs): # inputs: (batch, seq_len, token_len) x = self.embedding(inputs) # (batch, seq_len, 50) # First forward and backward LSTM pass outside the loop, as in original code outputs1 = self._run_lstm(self.forward_lstm[0], x, backwards=False) outputs2 = self._run_lstm(self.backward_lstm[0], x, backwards=True) # Apply all forward LSTMs (including the first one again) for lstm in self.forward_lstm: outputs1 = self._run_lstm(lstm, outputs1, backwards=False) # Apply all backward LSTMs (including the first one again) for lstm in self.backward_lstm: outputs2 = self._run_lstm(lstm, outputs2, backwards=True) outputs1 = self.forward_projection(outputs1) # (batch, seq_len, vocab_size) outputs2 = self.forward_projection(outputs2) # Original code uses forward_projection again # Concatenate along seq_len dimension (dim=1 since batch_first=True) out = torch.cat([outputs1, outputs2], dim=1) return out.mean(dim=1) . m = Elmo(lstm_num=2).cuda() . learner = Learner(dls, m, loss_func=CrossEntropyLossFlat(), metrics=accuracy) . References . https://speakerdeck.com/i_mishramayank/deep-contextualized-word-embeddings | https://arxiv.org/abs/1802.05365 | .",
            "url": "https://numb3r33.github.io/experiments/optimization/math/deeplearning/fast.ai/2024/12/17/elmo.html",
            "relUrl": "/optimization/math/deeplearning/fast.ai/2024/12/17/elmo.html",
            "date": " • Dec 17, 2024"
        }
        
    
  
    
        ,"post8": {
            "title": "Tight-fisted Optimizer ( Tiger )",
            "content": "This post will try to breakdown the process of how to implement any optimizer ( in this case tiger ) in fast.ai. . Primer on how to implement an optimizer in fast.ai . Setup . import matplotlib as mpl mpl.rcParams[&#39;image.cmap&#39;] = &#39;gray&#39; from __future__ import annotations from fastai.torch_basics import * from fastai.vision.all import * from torcheval.metrics import MulticlassAccuracy from torch import tensor . from datasets import load_dataset . dsd = load_dataset(&#39;fashion_mnist&#39;, trust_remote_code=True) . xl,yl = &#39;image&#39;,&#39;label&#39; name = &quot;fashion_mnist&quot; dsd = load_dataset(name) . class FashionMnistTransform(Transform): def __init__(self, xl): self.xl = xl def encodes(self, o): return TensorImage(o[self.xl]) class Lblr(Transform): def __init__(self, yl): self.yl = yl def encodes(self, o): return o[self.yl] . tfms = [[FashionMnistTransform(xl), IntToFloatTensor()], [Lblr(yl), Categorize()]] dsets = Datasets(dsd[&#39;train&#39;], tfms, splits=RandomSplitter(seed=42)(dsd[&#39;train&#39;])) . dls = dsets.dataloaders(bs=64) . x, y = dls.one_batch() x.shape, y.shape . dls.show_batch() . def conv(ni, nf, ks=3, stride=2, act=True): res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res class BasicModel(Module): def __init__(self): self.m = nn.Sequential(conv(1 ,8), conv(8 ,16), conv(16,32), conv(32,64), conv(64,10, act=False), nn.Flatten() ) def forward(self, x): x = x.unsqueeze(1) return self.m(x) . bm = BasicModel() o = bm(x) . learn = Learner(dls, bm, metrics=accuracy) . learn.fit_one_cycle(3) . . Delving deep into the Optimizers in fast.ai . fast.ai provides an optimizer class which takes in params and a list of callbacks. . class Optimizer(_BaseOptimizer): &quot;Base optimizer class for the fastai library, updating `params` with `cbs`&quot; _keep_on_clear = [&#39;force_train&#39;, &#39;do_wd&#39;] def __init__(self, params:Tensor|Iterable, # Model parameters cbs:callable|MutableSequence, # `Optimizer` step callbacks **defaults # Hyper parameters default values ): . params will be used to create the param_groups of the optimizer.cbs is a list of functions that will be composed when applying the step. For instance, you can compose a function making the SGD step, with another one applying weight decay. . Additionally, each cb can have a defaults attribute that contains hyper-parameters and their default value. Those are all gathered at initialization, and new values can be passed to override those defaults with the defaults kwargs. The steppers will be called by Optimizer.step (which is the standard PyTorch name), and gradients can be cleared with Optimizer.zero_grad (also a standard PyTorch name). . Once the defaults have all been pulled off, they are copied as many times as there are param_groups and stored in hypers. To apply different hyper-parameters to different groups (differential learning rates, or no weight decay for certain layers for instance), you will need to adjust those values after the init. . Example of how to implement SGD with momentum. . begin{aligned} text{Update Step:} quad &amp; mathbf{v}_{t+1} = mu mathbf{v}_t - eta left( nabla_{ mathbf{w}} mathcal{L}( mathbf{w}_t) + lambda mathbf{w}_t right) &amp; mathbf{w}_{t+1} = mathbf{w}_t + mathbf{v}_{t+1} end{aligned} def weight_decay(p, lr, wd, do_wd=True, **kwargs): &quot;Weight decay as decaying `p` with `lr*wd`&quot; if do_wd and wd!=0: p.data.mul_(1 - lr*wd) weight_decay.defaults = dict(wd=0.) def average_grad(p, mom, dampening=False, grad_avg=None, **kwargs): &quot;Keeps track of the avg grads of `p` in `state` with `mom`.&quot; if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data) damp = 1-mom if dampening else 1. grad_avg.mul_(mom).add_(p.grad.data, alpha=damp) return {&#39;grad_avg&#39;: grad_avg} average_grad.defaults = dict(mom=0.9) def momentum_step(p, lr, grad_avg, **kwargs): &quot;Step for SGD with momentum with `lr`&quot; p.data.add_(grad_avg, alpha=-lr) def SGD( params:Tensor|Iterable, # Model parameters lr:float|slice, # Default learning rate mom:float=0., # Gradient moving average (β1) coefficient wd:Real=0., # Optional weight decay (true or L2) decouple_wd:bool=True # Apply true weight decay or L2 regularization (SGD) ) -&gt; Optimizer: &quot;A SGD `Optimizer`&quot; cbs = [weight_decay] if decouple_wd else [l2_reg] if mom != 0: cbs.append(average_grad) cbs.append(sgd_step if mom==0 else momentum_step) return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd) . Here are the equations inolved for tight-fisted optimizer . begin{align*} m_t &amp;= beta m_{t-1} + (1 - beta) g_t theta_t &amp;= theta_{t-1} - eta_t left[ operatorname{sign}(m_t) + lambda_t theta_{t-1} right] end{align*} def tiger_step(p, lr, grad_avg, wd, **kwargs): p.data.add_(grad_avg.sign(), alpha=-lr) return p def Tiger( params:Tensor|Iterable, # Model parameters lr:float|slice, # Default learning rate mom:float=0.945, # Gradient moving average (β) coefficient wd:Real=0.01, # Optional weight decay (true or L2) decouple_wd:bool=True # Apply weight decay ) -&gt; Optimizer: &quot;A Tight-fisted ( Tiger ) `Optimizer`&quot; cbs = [weight_decay] if decouple_wd else [l2_reg] cbs += [partial(average_grad, dampening=True), tiger_step] return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd) . learn_tiger = Learner(dls, bm, metrics=accuracy, opt_func=Tiger) . learn_tiger.lr_find() . . learn_tiger.fit_one_cycle(3, 3e-3) . .",
            "url": "https://numb3r33.github.io/experiments/optimization/math/deeplearning/2024/11/19/tight-optimizer.html",
            "relUrl": "/optimization/math/deeplearning/2024/11/19/tight-optimizer.html",
            "date": " • Nov 19, 2024"
        }
        
    
  
    
        ,"post9": {
            "title": "im2col",
            "content": "What is convolution? . . Convolutional is a mathematical operation which in the context of image processing is used to transform an image which could lead to blurring, sharpening, edge detection or more. It can take input from two-dimensional matrices to four-dimensional matrices and the convolutional filters which also range from two-dimensional matrices to four-dimensional matrices. The above equation gives a unified view of this, where x is the input and k is the convolutional kernel giving us a feature map called a. . . In the above picture x of shape ( 4, 4 ) represents the input, w represents the convolutional filter of shape ( 3, 3 ) giving us A which would be our feature map. The process of convolution is to divide the input x into several smaller patches (matrices) of same size as the convolutional kernel and then performing elementwise multiply and sum with the convolutional kernel. . In the following example we have shown patches of size ( 3, 3 ) created out of input x and then performing convolution with the filter w would lead us value a11. Matrix of size (4,4) with kernel of size (3,3) and stride = 1 should give us 4 patches and hence the final result has 4 values. . . Now the element-wise multiply and sum would require us to iterate over element using for loops and then returning the sum like shown below. . from fastai.vision.all import * np.set_printoptions(linewidth=140) . x = np.arange(16, dtype=np.int32).reshape(4, 4) w = np.random.random(size=(3, 3)) x1 = x[:3, :3] c = 0 for i in range(x1.shape[0]): for j in range(x1.shape[1]): c += (x1[i, j] * w[i, j]) print(c) . 16.574367764256806 . Key insight is to view convlution as matrix multiplication which we can clearly see from the above two loops that it is doing element-wise multiplication and addition which is what we will get if we do a dot product of a row-vector and a column-vector so the idea is to transform the patch into a row-vector and the weights into a column vector. . . Perform the same operation over all the 4 patches giving us a new matrix which we will call X . . In the same spirit we can transform w as a column vector and we will call it W . . Now convolution is just a matrix multiplication between X and W . . Let&#39;s see how could we perform these operations using numpy, we will validate the results using the inbuild F.conv2d provided by Pytorch. . Single channel input . x = np.arange(16, dtype=np.int32).reshape(4, 4) x . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], dtype=int32) . The first patch would look something like . x1 = x[:3, :3] x1 . array([[ 0, 1, 2], [ 4, 5, 6], [ 8, 9, 10]], dtype=int32) . Now transform this into row vector . x1 = x1.reshape(1, -1) x1 . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10]], dtype=int32) . Do this for rest of the patches and stack them together to get the matrix . x2 = x[:3, 1:4] x2 = x2.reshape(1, -1) x3 = x[1:4, :3] x3 = x3.reshape(1, -1) x4 = x[1:4, 1:4] x4 = x4.reshape(1, -1) X = np.vstack((x1, x2, x3, x4)) X . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10], [ 1, 2, 3, 5, 6, 7, 9, 10, 11], [ 4, 5, 6, 8, 9, 10, 12, 13, 14], [ 5, 6, 7, 9, 10, 11, 13, 14, 15]], dtype=int32) . Now all of the row vectors have been stacked together to form X . Transform convolution kernel into a weight column vector. . np.random.seed(41) w = np.random.random(size=(3, 3)) w . array([[0.25092362, 0.04609582, 0.67681624], [0.04346949, 0.1164237 , 0.60386569], [0.19093066, 0.66851572, 0.91744785]]) . W = w.reshape(-1, 1) W . array([[0.25092362], [0.04609582], [0.67681624], [0.04346949], [0.1164237 ], [0.60386569], [0.19093066], [0.66851572], [0.91744785]]) . conv = np.dot(X, W) conv . array([[22.49748414], [26.01197293], [36.55543931], [40.0699281 ]]) . conv.reshape(2, 2) . array([[22.49748414, 26.01197293], [36.55543931, 40.0699281 ]]) . Verify if 2D convolution from Pytorch yields the same value or not . F.conv2d(tensor(x[None, None, :, :]).float(), tensor(w[None, None, :, :]), bias=None, stride=1) . tensor([[[[22.4975, 26.0120], [36.5554, 40.0699]]]]) . Indeed the values match up!! . Let&#39;s take a look at how does convolution operations changes when we consider multi-channel input and multiple filters. . Multi-channel input . Let&#39;s start with a case where we have a multi-channel input, so earlier we considered inputs of shape (H, W) now we would consider inputs like (C, H, W) and see how could we use our im2col algorithm to perform the convolution operation. . x = np.arange(32, dtype=np.int32).reshape(2, 4, 4) x . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]], dtype=int32) . In all of the cases presented here we have assumed stride=1 for simplicity, let&#39;s write a method that captures a patch and transforms into a row vector and then stack all of the row vectors together to form a matrix which we are referring to as X. . def patches_to_row(inp, kh, kw): H, W = inp.shape xm = H - kh ym = W - kw X = [] for i in range(0,xm+1): for j in range(0,ym+1): rv = inp[i:i+kh, j:j+kw] rv = rv.reshape(1, -1) X.append(rv) return np.vstack(X) mat = patches_to_row(np.arange(16, dtype=np.int32).reshape(4, 4), 3, 3) mat . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10], [ 1, 2, 3, 5, 6, 7, 9, 10, 11], [ 4, 5, 6, 8, 9, 10, 12, 13, 14], [ 5, 6, 7, 9, 10, 11, 13, 14, 15]], dtype=int32) . For multi-channel input we perform the same operation for both the channels and then stack them together to give us X . X1 = patches_to_row(x[0, :, :], kh=3, kw=3) X2 = patches_to_row(x[1, :, :], kh=3, kw=3) . X1 . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10], [ 1, 2, 3, 5, 6, 7, 9, 10, 11], [ 4, 5, 6, 8, 9, 10, 12, 13, 14], [ 5, 6, 7, 9, 10, 11, 13, 14, 15]], dtype=int32) . X2 . array([[16, 17, 18, 20, 21, 22, 24, 25, 26], [17, 18, 19, 21, 22, 23, 25, 26, 27], [20, 21, 22, 24, 25, 26, 28, 29, 30], [21, 22, 23, 25, 26, 27, 29, 30, 31]], dtype=int32) . Now we would like to concatenate 1st row of X1 with 1st row of X2. . X = np.hstack((X1, X2)) X . array([[ 0, 1, 2, 4, 5, 6, 8, 9, 10, 16, 17, 18, 20, 21, 22, 24, 25, 26], [ 1, 2, 3, 5, 6, 7, 9, 10, 11, 17, 18, 19, 21, 22, 23, 25, 26, 27], [ 4, 5, 6, 8, 9, 10, 12, 13, 14, 20, 21, 22, 24, 25, 26, 28, 29, 30], [ 5, 6, 7, 9, 10, 11, 13, 14, 15, 21, 22, 23, 25, 26, 27, 29, 30, 31]], dtype=int32) . We need to ensure that number of channels in the convolutional kernel matches up with the input. . np.random.seed(41) w = np.random.random(size=(2, 3, 3)) W = w.reshape(-1, 1) W . array([[0.25092362], [0.04609582], [0.67681624], [0.04346949], [0.1164237 ], [0.60386569], [0.19093066], [0.66851572], [0.91744785], [0.41878009], [0.33225985], [0.28303364], [0.18628227], [0.31711047], [0.48116867], [0.06952047], [0.70498257], [0.31467693]]) . fmap = np.dot(X, W) fmap.reshape(2, 2) . array([[ 88.38632016, 95.0086239 ], [114.87553514, 121.49783888]]) . F.conv2d(tensor(x[None, :, :, :]).float(), tensor(w[None, :, :, :]).float(), bias=None, stride=1 ) . tensor([[[[ 88.3863, 95.0086], [114.8755, 121.4978]]]]) . As we can see the two values match, so far so good. . Multi channel input and multiple filters . In the previous case we considered only 1 filter which had 2 channels and it lead us to a feature map of the same size that we got in case of single input channel, now we can use multiple conv filters to capture different features in the image. . x = np.arange(32, dtype=np.int32).reshape(2, 4, 4) x . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]], dtype=int32) . Now we would like to consider 2 weight filters so our filter&#39;s shape would be (2, 2, 3, 3) representing (kn, C, kh, kw) -&gt; kn number of filters, C number of channels, kh - height of the filter, kw width of filter. . np.random.seed(41) w = np.random.random(size=(2, 2, 3, 3)) w . array([[[[0.25092362, 0.04609582, 0.67681624], [0.04346949, 0.1164237 , 0.60386569], [0.19093066, 0.66851572, 0.91744785]], [[0.41878009, 0.33225985, 0.28303364], [0.18628227, 0.31711047, 0.48116867], [0.06952047, 0.70498257, 0.31467693]]], [[[0.74528235, 0.3982128 , 0.60822646], [0.72845649, 0.42175804, 0.39390821], [0.23222257, 0.4416646 , 0.37302139]], [[0.58360604, 0.10003138, 0.74135188], [0.08319793, 0.12622394, 0.32289153], [0.64292729, 0.99947173, 0.28100165]]]]) . X1 = patches_to_row(x[0, :, :], kh=3, kw=3) X2 = patches_to_row(x[1, :, :], kh=3, kw=3) X = np.hstack((X1, X2)) print(X) print(X.shape) . [[ 0 1 2 4 5 6 8 9 10 16 17 18 20 21 22 24 25 26] [ 1 2 3 5 6 7 9 10 11 17 18 19 21 22 23 25 26 27] [ 4 5 6 8 9 10 12 13 14 20 21 22 24 25 26 28 29 30] [ 5 6 7 9 10 11 13 14 15 21 22 23 25 26 27 29 30 31]] (4, 18) . For convolutional filters first convert them into column vectors then combine them together into W = [W1, W2] . np.random.seed(41) w = np.random.random(size=(2, 2, 3, 3)) W = np.hstack((w[0, :, :, :].reshape(-1, 1), w[1, :, :, :].reshape(-1, 1))) W . array([[0.25092362, 0.74528235], [0.04609582, 0.3982128 ], [0.67681624, 0.60822646], [0.04346949, 0.72845649], [0.1164237 , 0.42175804], [0.60386569, 0.39390821], [0.19093066, 0.23222257], [0.66851572, 0.4416646 ], [0.91744785, 0.37302139], [0.41878009, 0.58360604], [0.33225985, 0.10003138], [0.28303364, 0.74135188], [0.18628227, 0.08319793], [0.31711047, 0.12622394], [0.48116867, 0.32289153], [0.06952047, 0.64292729], [0.70498257, 0.99947173], [0.31467693, 0.28100165]]) . fmap = np.dot(X, W) fmap.T.reshape(2, 2, 2) . array([[[ 88.38632016, 95.0086239 ], [114.87553514, 121.49783888]], [[102.08763737, 110.31109367], [134.98146258, 143.20491888]]]) . F.conv2d(tensor(x[None, :, :, :]).float(), tensor(w).float(), bias=None, stride=1 ) . tensor([[[[ 88.3863, 95.0086], [114.8755, 121.4978]], [[102.0876, 110.3111], [134.9815, 143.2049]]]]) . Checking with the pytorch&#39;s implementation to ensure correctness. . Batch of inputs with multi-channels and multiple convolutional filters. . Extending to batch input would only require us to concatenate the output of the individual inputs in the batch to form X . Let&#39; take an example where we have only 2 inputs in our batch. . x = np.arange(64, dtype=np.int32).reshape(2, 2, 4, 4) x . array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]], [[[32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47]], [[48, 49, 50, 51], [52, 53, 54, 55], [56, 57, 58, 59], [60, 61, 62, 63]]]], dtype=int32) . X11 = patches_to_row(x[0, 0, :, :], kh=3, kw=3) X12 = patches_to_row(x[0, 1, :, :], kh=3, kw=3) X1 = np.hstack((X11, X12)) X21 = patches_to_row(x[1, 0, :, :], kh=3, kw=3) X22 = patches_to_row(x[1, 1, :, :], kh=3, kw=3) X2 = np.hstack((X21, X22)) X = np.vstack((X1, X2)) X.shape . (8, 18) . Each input produces 4 row vectors so joining the outputs of 2 inputs would yield us a matrix of shape (8, 18). . np.random.seed(41) w = np.random.random(size=(2, 2, 3, 3)) W = np.hstack((w[0, :, :, :].reshape(-1, 1), w[1, :, :, :].reshape(-1, 1))) W . array([[0.25092362, 0.74528235], [0.04609582, 0.3982128 ], [0.67681624, 0.60822646], [0.04346949, 0.72845649], [0.1164237 , 0.42175804], [0.60386569, 0.39390821], [0.19093066, 0.23222257], [0.66851572, 0.4416646 ], [0.91744785, 0.37302139], [0.41878009, 0.58360604], [0.33225985, 0.10003138], [0.28303364, 0.74135188], [0.18628227, 0.08319793], [0.31711047, 0.12622394], [0.48116867, 0.32289153], [0.06952047, 0.64292729], [0.70498257, 0.99947173], [0.31467693, 0.28100165]]) . fmap = np.dot(X, W) fmap.T.reshape(2, 2, 2, 2) . array([[[[ 88.38632016, 95.0086239 ], [114.87553514, 121.49783888]], [[300.30003998, 306.92234373], [326.78925496, 333.4115587 ]]], [[[102.08763737, 110.31109367], [134.98146258, 143.20491888]], [[365.238239 , 373.4616953 ], [398.1320642 , 406.3555205 ]]]]) . F.conv2d(tensor(x).float(), tensor(w).float(), bias=None, stride=1 ) . tensor([[[[ 88.3863, 95.0086], [114.8755, 121.4978]], [[102.0876, 110.3111], [134.9815, 143.2049]]], [[[300.3000, 306.9224], [326.7893, 333.4115]], [[365.2382, 373.4617], [398.1320, 406.3555]]]]) . matching again!! . Using an efficient method in Numpy to perform covolutions. . We have so far seen that extracting patch from an image based on the kernel size and patch requires most of the effort. We naively implemented it with a double loop assuming a stride of size 1 but there is a faster way to get these patches of an input based on the filter size and stride using the method . np.lib.stride_tricks.as_strided . Let&#39;s see how this works out . . Let&#39;s consider your input is X and the convlutional kernel is of size ( 2, 2 ) with stride 1 then matrix should look something like this . . X = np.arange(1, 10, dtype=np.int32).reshape(3, 3) A = np.lib.stride_tricks.as_strided(X, shape=(2, 2, 2, 2), strides=(12, 4, 12, 4)) A . array([[[[1, 2], [4, 5]], [[2, 3], [5, 6]]], [[[4, 5], [7, 8]], [[5, 6], [8, 9]]]], dtype=int32) . Lot&#39;s of stuff to unpack here, let&#39;s start with function signature and see what does individual parameters mean. . np.lib.stride_tricks.as_strided? . np.lib.stride_tricks.as_strided(x, shape=None, strides=None, subok=False, writeable=True) . This method creates a view on the matrix x, where shape and strides are attributes of the view created. We are all familiar with what shape signifies for a matrix but stride is something new. . Numpy officially describes strides as how many bytes we need to jump over in the data buffer to go from one item to the next. So to create our desired view we need to pass it&#39;s shape and strides in the func args. . Let&#39;s try to understand it with an example. . a = np.arange(9, dtype=np.int32).reshape(3,3) print(a) . [[0 1 2] [3 4 5] [6 7 8]] . so for moving from a[i, 0] to a[i, 1] we need to jump 4 bytes since dtype is int32 and across ith dimension to move from a[0, j] to a[1, j] we need to jump 12 bytes, hence strides for a would be (12, 4) . a.strides . (12, 4) . Coming back to original array A how would we determine it&#39;s strides, we could start with the lowest dimension. How many bytes we need to jump when moving from A[i, j, k, l] to A[i, j, k, l+1] and compare it with X. We know moving from previous exercise moving across lowest dimension would require only 4 bytes so strides would be (?, ?, ?, 4), also moving to higher dimension jump required to move from A[i, j, k, l] to A[i, j, k+1, l] would be 12 bytes when comparing with our previous example leading to (?, ?, 12, 4). . . For the next dimension j we need to look at the bytes we need to jump to reach from 1 to 2 marked in the above figure. Essentially we are trying to figure out jump between a[i, j, k, l] to a[i, j+1, k, l]. If we look at the original array a both 1 and 2 are beside each other in 0th row so it would require jump of only 4 bytes, hence strides would be (?, 4, 12, 4) . . For the highest dimension we are looking at jump between 1 and 4 marked in red in the figure above. If we map these numbers to our original array X we see that these two numbers are in the same colunmn but in adjacent rows hence it would take 12 bytes to reach from 1 to 4 hence the stide would be 12, so the final value of strides for the array A would be (12, 4, 12, 4) . Multi-channel inputs . X = np.arange(1, 19, dtype=np.int32).reshape(2, 3, 3) X . array([[[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]], dtype=int32) . If the size of the convlutional kernel is (2, 2) then we will get 8 matrices of size (2, 2) so therefore shape of A would be (2, 2, 2, 2, 2). If you notice it has one more dimension than the previous case and it is because we have added 2 channels instead of one so shape of A would be . A.shape = (X.shape[0], *A.shape) . For stride calcualtion, rest everything is same we have only added an additional channel dimension hence strides value would be (?, 12, 4, 12, 4) so we only need to find the value of highest dimension. . . Let&#39;s look at the strides of the input for reference here . X.strides . (36, 12, 4) . For the highest dimension taking an example here, we want to find the jump between 1 and 10. If we compare the positions of 1 and 10 in X we find out that they are 9 positions apart and to move between a single position it takes around 4 bytes so hence to move 9 positions it would take 36 bytes. . Hence the final values of strides would be (36, 12, 4, 12, 4). If we pay close attention there is a relationship between X.strides and A.strides which is the following . A.strides = (*X.strides[:-2], X.strides[-2]*s, X.strides[-1]*s, *X.strides[-2:]) . where s would be the stride used by the convolution filter to go over the input X. . Next question would be how to determine the shape of A, since X is 4-dim tensor with shape (N, C, H, W) and shape of convolutional kernel being kh and kw, then shape of A could be determined as (N, C, ?, ?, kh, kw). . The output feature map should have height oh and width ow respectively which could be calculated as following . oh = ( H - kh ) // stride + 1 ow = ( W - kw ) // stride + 1 . def get_mat_strides(X, kh, kw, s): N, C, H, W = X.shape oh = ( H - kh ) // s + 1 ow = ( W - kw ) // s + 1 strides = (*X.strides[:-2], X.strides[-2]*s, X.strides[-1]*s, *X.strides[-2:]) A = np.lib.stride_tricks.as_strided(X, shape=(N, C, oh, ow, kh, kw), strides=strides) return A X = np.arange(1, 33, dtype=np.int32).reshape(1, 2, 4, 4) A = get_mat_strides(X, kh=2, kw=2, s=1) A . array([[[[[[ 1, 2], [ 5, 6]], [[ 2, 3], [ 6, 7]], [[ 3, 4], [ 7, 8]]], [[[ 5, 6], [ 9, 10]], [[ 6, 7], [10, 11]], [[ 7, 8], [11, 12]]], [[[ 9, 10], [13, 14]], [[10, 11], [14, 15]], [[11, 12], [15, 16]]]], [[[[17, 18], [21, 22]], [[18, 19], [22, 23]], [[19, 20], [23, 24]]], [[[21, 22], [25, 26]], [[22, 23], [26, 27]], [[23, 24], [27, 28]]], [[[25, 26], [29, 30]], [[26, 27], [30, 31]], [[27, 28], [31, 32]]]]]], dtype=int32) . Numpy provides a method named np.tensordot to peform tensor multiplication. Here we would perform tensor multiplication between A and the convolution filters W. . Let&#39;s consider a single filter matching 2 channels of the input X. . np.random.seed(41) W = np.random.random(size=(1, 2, 2, 2)) W . array([[[[0.25092362, 0.04609582], [0.67681624, 0.04346949]], [[0.1164237 , 0.60386569], [0.19093066, 0.66851572]]]]) . X.shape, A.shape, W.shape . ((1, 2, 4, 4), (1, 2, 3, 3, 2, 2), (1, 2, 2, 2)) . # Shape of filter -&gt; (kn, C, kh, kw) # Shape of feature map -&gt; (N, kn, oh, ow) fmap = np.tensordot(A, W, axes=[(1, 4, 5), (1, 2, 3)]) fmap.transpose(0, 3, 1, 2) # transpose to match the output of `F.conv2d` . array([[[[35.55368844, 38.15072938, 40.74777032], [45.94185221, 48.53889315, 51.1359341 ], [56.33001598, 58.92705693, 61.52409787]]]]) . conv = F.conv2d(tensor(X).float(), tensor(W), bias=None, stride=1) conv . tensor([[[[35.5537, 38.1507, 40.7478], [45.9419, 48.5389, 51.1359], [56.3300, 58.9271, 61.5241]]]]) . As you can see both the values match one from our implmentation&#39;s of convolution using np.lib.stride_tricks.as_strided followed by np.tensordot with F.conv2d. . Note: here we have assumed that X is necessarily padded before. . %timeit -n 1000 np.tensordot(A, W, axes=[(1, 4, 5), (1, 2, 3)]) . 21 µs ± 2.76 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . %timeit -n 1000 F.conv2d(tensor(X).float(), tensor(W), bias=None, stride=1) . 39.4 µs ± 4.26 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . It matches with the performance of Pytorch&#39;s implementatio as well. . It is useful to implement the core building blocks of deep learning from multiple ways to better understand what&#39;s happening behind the scenes and gain insights. This exercise gave a different perspective of why convolutions need to be modeled as matrix multiplications to gain the performance speedup of accelerators like GPU. .",
            "url": "https://numb3r33.github.io/experiments/convolution/math/deeplearning/2023/12/23/im2col.html",
            "relUrl": "/convolution/math/deeplearning/2023/12/23/im2col.html",
            "date": " • Dec 23, 2023"
        }
        
    
  
    
        ,"post10": {
            "title": "Residual Learning",
            "content": "What is a residual? . Residual is the difference between actual and estimated value. . What is residual learning? . In the context of ensemble learning, a base model is used to fit the residuals to make the ensemble model more accurate. In deep learning, various architectures use a block/layer to fit the residual to improve the performance of the DNN. . How does Gradient Boosting Machines use residuals? . We will try to deconstruct how GBM works using DecisionTrees on a regression task. . from sklearn import datasets from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt . X, y = datasets.make_regression(n_samples=1000, random_state=41) Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.2, random_state=41) . from sklearn.tree import DecisionTreeRegressor . tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg1.fit(Xtr, ytr) y2 = ytr - tree_reg1.predict(Xtr) tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg2.fit(Xtr, y2) y3 = y2 - tree_reg2.predict(Xtr) tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=41) tree_reg3.fit(Xtr, y3) y_pred = sum(tree.predict(Xva) for tree in (tree_reg1, tree_reg2, tree_reg3)) . Gradient Boosting . How does residuals play a part in Gradient Boosting Learning? . Train a base learner tree_reg1 to fit data (X) and labels (y) | Train a base learner tree_reg2 that fits on data (X) and residuals between the label and predicted value of base learner tree_reg1. Essentially, we are using a base learner to learn the residuals. | Finally the result of all the base learners are added to make the final prediction. | . The above code is equivalent to calling the GradientBoostingRegressor with 3 base learners. . from sklearn.ensemble import GradientBoostingRegressor gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=41) gbrt.fit(Xtr, ytr) gb_preds = gbrt.predict(Xva) . sum(y_pred - gb_preds) . -4.554578936222242e-12 . Role of residual learning in training deep networks? . Why do we need ResNets? . Research to develop better architectures which perform better has led researchers to go deeper with a notion that to a certain extent going deeper would yield better performance. . But we realized that going deeper brings problems of its own, model become difficult to train. In 2014, VGG had only 19 layers while in 2015 ResNet had 152 layers and a far better performance, one can say at an initial glance that ResNet wins because it has more number of layers. Ofcourse that is the case but it also introduces a trick called &quot;residual learning&quot; that helps achieve this performance. . CNN models have evolved over time from LeNet-5 ( 5 layers ) and AlexNet ( 8 layers ) to VGGNet (16-19) and later GoogleNet ( 22 layers ). According to experimental results of VGGNet, depth of the network plays a crucial role in model&#39;s performance. . Please find below tables extracted from VGG paper that showcases that deeper we go better the effect. . . . Are deeper networks really better? . Later in various experiments it was found out that model performance increases with depth upto a certain extent further which it often decreases. What could be the reasons for that could it be following . Overfitting. | Vanishing/Exploding Gradients. | . Overfitting . In the Resnet paper the authors tried this following experiment . . The y-axis on the left figure represents training error and the y-axis on the right figure represents test error and x-axes on the both the figures represent the number of iterations. . We can see that the 20-layer network trained for a large number of iterations yields in low training error but corresponding test error is relatively large. This is a case of over-fitting ( we are performing better on training compared to test dataset ). . In addition to this the authors also trained a network with 56-layers and found out that error of this network in both training and testing is large compared to the 20-layer network. Thus performance degradation has nothing to do with overfitting. . Vanishing/Exploding Gradients . Vanishing/Exploding gradients make the model difficult to train but there are already some techniques like Batch Normalization to alleviate this problem. . Let&#39;s try to understand the example presented by author in the paper . Suppose we have a following network which can perform good on training and test datasets. . . Then we augment the architecture in the following way to add more layers. The parameters of the first 4 layers are copied from the above network and these parameters remain unchanged during training. . . In theory the performance of the second network should be better than first network since we have more layers which could extract useful features and suppose we find out that the second network performs worse, then one explanation provided by the authors is that since we have copied the parameters of first 4 layers in the second network and if they are enough to meet the performance requirements then the newly added layers are a bit redundant. To maintain the level of performance, the newly added functions has to serve as an identity mapping that is the net effect of the purple layers should be f(x) = x, in this way we would not experience model degradation. . This is what the authors observed that the non-linear expression of the traditional multi-layer network structure has difficulty expressing the identity mapping which leads to model degradation. . How do we tackle model degradation then? . Assuming that a relatively shallow network can already achieve good results, then even if the network is piled up with more layers the effect of the model should not deteriorate. . In reality, however this is the problem, doing nothing happens to be a very challenging task. . Presence of non-linear activation functions makes the input-to-output process almost irreversible. Non-linearity gives the model endless possibilities but it also makes the network forget the original intention. . The quality of not forgetting the original intention/doing nothing is managed by identity mapping. . Residual Block . . In fact, it is difficult for existing neural networks to fit the underlying identity mapping function H(x) = x.But if the network is designed such that H(x) = F(x) + x, then the identity map could be used as part of the network. . The problem can be transformed into learning a residual function F(x) = H(x) - x. As long as F(x)=0, an identity map H(x) = x is formed. The loop in the figure is called a shortcut connection. By jumping before the activation function, the output of the previous layer or layers is added to the output calculated by this layer, and the result of summation is input to the next activation function as the output of this layer. . The idea of the skip connection is to expressed the output as a linear superposition of a nonlinear transformation of the input and the input. There is no new formula, no new theory, but a new expression. . Why is residual learning relatively easier? . Intuitively, residual learning requires less learning, because residuals are generally relatively small and the learning difficulty is less. However, we can analyze this problem from a mathematical point of view. First, the residual unit can be expressed as: . . $F(x, {W_i })$ is the goal of our learning, that is, the residual of the output and input i.e. $y-x$. If we further expand . . $ sigma$ refers to Relu, while $W_1$, $W_2$ refer to two layers of weights. When $F(x, {W_i })$ learns to have a 0 value then $y = x$, this is what we call identity mapping. . Why can&#39;t we have $y=f(x, {W_i })$ instead and no skip connections? . Because $f(x, {W_i })$ has a ReLU activation function in the middle so if x &lt;= 0 then y = 0 which would violate the identity mapping principle. | . Experiments . . Taken directly from Resnet paper . Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left:plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts. . Summary . Shortcut connections/residual connections/skip connections/skip connections, etc. are all one thing, there is no new theory, just a new expression. | Problem of model degradation when deepening the network could be somewhat alleviated using residual learning. | . References . Resnet paper | VGG | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/2022/09/19/residual-learning.html",
            "relUrl": "/deeplearning/math/2022/09/19/residual-learning.html",
            "date": " • Sep 19, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "AEDA ( An Easier Data Augmentation Technique for Text Classification )",
            "content": "Paper resources . paper | code | . Objective . This paper proposes a new data augmentation technique for text classification task. | It also compares the performance of this augmentation technique with EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks and concludes that their method is simpler and produces better results. | In this experiment we will try to implement this data augmentation using fastai on a text classification task. | . Why do we need augmentations? . To have better generalizability, we need more and more comprehensive datasets but collection of these datasets and labelling is a laborious task so augmentation becomes an attractive method to introduce more examples for model to consume. | . What are the different kind of augmentations used in NLP? . For improving machine translation task, researchers have tried substituting common words with rare words thus providing more context for rare words. | Some researchers have tried replacing words with their synonyms for tweet classification. | Randomly swap two words in a sentence. | Randomly delete a word in the sentence and many more. | . What is the novel idea presented in the paper? . AEDA method proposes randomly inserting some punctuation marks as an augmentation to introduce noise. The authors report improvement performance in text classification tasks. . Can you share an example of how this augmentation would work? . Original Text: . Appropriate given recent events . . Augmented Text: . Appropriate given ; recent events . Appropriate ? given : recent events . Appropriate given , recent events . How many punctuation marks are inserted? . Between 1 to n/3 where n represents the length of the sentence. | . Why one-third of the sentence length? . The authors mention that they want to increase the complexity of the sentence but doesn&#39;t want to add too many punctuation marks which would interfere with the semantic meaning of the sentence. . At which positions should we insert these punctuation marks? . The authors inserted them at random positions in the sentence. | . What are the different punctuation marks used? . . ; ? : ! , . Why does AEDA work better compared to EDA? . EDA proposes synonym replacement, random replacement, random insertion and random deletion. These modifications could change the semantic meaning of the text. | Whereas AEDA just introduces punctuation marks which would only introduce noise and would not mess the semantic meaning or the word ordering. | . Implementation . We would be using fastai to implement this augmentation. | The authors have released the code as well which we would using. | . Dataset . We will be using this dataset used in this challenge where the goal is to predict the subreddit of a subreddit post based on their title and their description. This is an example of text categorization / text classification task | . Load libraries . import pandas as pd import numpy as np from pathlib import Path from tqdm import tqdm from fastai.text.all import * SEED = 41 . Define paths and constants . BASE_DIR = Path(&#39;~/data/dl_nlp&#39;) RAW_DATA_PATH = BASE_DIR / &#39;data&#39; OUTPUT_DIR = Path(&#39;~/data/dl_nlp/outputs&#39;) PUNCTUATIONS = [&#39;.&#39;, &#39;,&#39;, &#39;!&#39;, &#39;?&#39;, &#39;;&#39;, &#39;:&#39;] PUNC_RATIO = 0.3 . Load dataset . train = pd.read_csv(RAW_DATA_PATH / &#39;train.csv&#39;) train.head() . . Text column represents title as well as description. | Subreddit column represents our label. | . Class Distribution . train.subreddit.value_counts(normalize=True) . . We have multiple categories that our model needs to get right. | Most of the categories have similar percentage of data points in the dataset, with only SubredditSimulator category having less training examples. | . Splitter . splits = RandomSplitter(seed=41)(train) . Create a splitting strategy. | Here we plan to split our training dataframe randomly into training ( 80% ) and validation ( 20% ) datasets. | . Tokenize the training dataset . df_tok, cnt = tokenize_df(train.iloc[splits[0]], text_cols=&#39;text&#39;) . Fast.ai provides a method to tokenize our dataset. | Here we only passing our training examples as the corpus for tokenizer to create vocabulary. | We could pass in different types of tokenizers here but by default it works with WordTokenizer. | . df_tok . . Here we could see that it has split our text string into tokens and created an additional column called text_length describing the length. | It has also added some library specific tokens like xxbos, xxmaj etc. xxbos represents beginning of the sentence token. For more details please refer to fast.ai | . cnt . . Here is a snapshot of the vocabulary constructed by the tokenize_df method. | . Using fast.ai Pipeline to construct Dataset . text_pipe = Pipeline([attrgetter(&#39;text&#39;), Tokenizer.from_df(0), Numericalize(vocab=list(cnt.keys()))]) lbl_pipe = Pipeline([attrgetter(&#39;subreddit&#39;), Categorize()]) lbl_pipe.setup(train.subreddit) dsets = Datasets(train, [text_pipe, lbl_pipe], splits=splits, dl_type=SortedDL) . Here we use Pipeline provided by fast.ai to put together different transforms we want to run on our dataframe. | text_pipe represents the Pipeline that we would like to run on our text column in the dataframe. | lbl_pipe represents the Pipeline that we would like to run on our subreddit column in the dataframe. | Numericalize transform takes in our vocabulary and converts the tokens to ids. | Categorize transforms converts our labels to categories. | Tokenizer.from_df transform tokenizes the text stored in our dataframe. | . AEDA data augmentation as fast.ai transform . np.random.seed(0) PUNCTUATIONS = [&#39;.&#39;, &#39;,&#39;, &#39;!&#39;, &#39;?&#39;, &#39;;&#39;, &#39;:&#39;] PUNC_RATIO = 0.3 class InsertPunctuation(Transform): split_idx = 0 def __init__(self, o2i, punc_ratio=PUNC_RATIO): self.o2i = o2i self.punc_ratio = punc_ratio def encodes(self, words:TensorText): new_line = [] q = random.randint(1, int(self.punc_ratio * len(words) + 1)) qs = random.sample(range(0, len(words)), q) for j, word in enumerate(words): if j in qs: new_line.append(self.o2i[PUNCTUATIONS[random.randint(0, len(PUNCTUATIONS)-1)]]) new_line.append(int(word)) else: new_line.append(int(word)) return TensorText(new_line) . We have taken the implementation from the github shared by the authors and created a fast.ai tranform that would take in the PUNC_RATIO and o2i as parameters and inserts punctuations at random positions in the sentence. | PUNC_RATIO by default takes a value of 0.3 which represents the 1/3rd of the sentence length mentioned in the paper. | o2i is mapping between token to token_id. | . Construct dataloaders . seq_len = 72 dls_kwargs = { &#39;after_item&#39; : InsertPunctuation(dsets.o2i), &#39;before_batch&#39;: Pad_Chunk(seq_len=seq_len) } dls = dsets.dataloaders(bs=32, seq_len=seq_len, **dls_kwargs) . When creating fast.ai dataloders we could perform operations on some of the events emitted. | Here we have made use of two such events, after_item callback is used to run our augmentation and add punctuation marks. | before_batch callback is used to make sure that we have paddded the tokens to make sure they are of same size before we collate them to form a batch. | . dls.show_batch(max_n=3) . . dls.show_batch gives a glimpse of the batch | . Using the classic TextCNN model introduced by Yoon Kim, paper . class TextCNN(Module): def __init__(self, n_embed, embed_dim, num_filters, filter_sizes, num_classes, dropout=0.5, pad_idx=1): store_attr(&#39;n_embed,embed_dim&#39;) self.embed = nn.Embedding(num_embeddings=n_embed, embedding_dim=embed_dim, padding_idx=1 ) self.convs = nn.ModuleList([ nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embed_dim) ) for k in filter_sizes ]) self.dropout = nn.Dropout(dropout) self.relu = nn.ReLU() self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes) def _conv_and_pool(self, x, conv): x = self.relu(conv(x)).squeeze(3) x = F.max_pool1d(x, x.size(2)).squeeze(2) return x def forward(self, x): out = self.embed(x) out = out.unsqueeze(1) out = torch.cat([self._conv_and_pool(out, conv) for conv in self.convs], 1) out = self.dropout(out) out = self.fc(out) return out . vocab = dls.train_ds.vocab num_classes = get_c(dls) model = TextCNN(len(vocab[0]), embed_dim=300, num_filters=100, filter_sizes=[1, 2, 3], num_classes=num_classes, ) . Define learner . learn = Learner(dls, model, metrics=[accuracy, F1Score(average=&#39;weighted&#39;)]) . Using F1 score weighted metric for multi-class classification. | . learn.fit_one_cycle(n_epoch=25, lr_max=3e-4, cbs=EarlyStoppingCallback(patience=3)) . . We are getting a F1( weighted ) score of 0.869 without using any pre-trained embeddings. | . References . AEDA: An Easier Data Augmentation Technique for Text Classification | AEDA code | EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks | Fast.AI | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/nlp/2022/01/31/aeda-text-augmentation.html",
            "relUrl": "/deeplearning/math/fastai/nlp/2022/01/31/aeda-text-augmentation.html",
            "date": " • Jan 31, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Temporal Convolution Networks",
            "content": "Objective . sequence modelling had become synonymous with recurrent networks. | This paper shows that convolutional networks can outperform recurrent networks on some of the tasks. | paper concludes that common association between sequence modelling and recurrent networks should be reconsidered. | . Temporal Convolution Networks? . A new general architecture for convolutional sequence prediction. | This new general architecture is referred to as Temporal Convolutional Networks abbreviated as TCN. | Convolutions in this architecture are causal which means that there is no information leakage. | Architecture can take in a sequence of arbitrary length and map it to an output sequence of the same length, just like RNNs. ( But tcn achieves this function not through seq2seq but simply using convolutional layers. ) | Also this paper highlights how we could combine deep networks ( with residual structures ) and dilated convolutions could be used to build long term dependencies. ( ability of an model to look back in past to make future predictions ) | . What is a sequence modelling task? . Taken directly from paper . Before defining the network structure, we highlight the nature of the sequence modeling task. Suppose that we are given an input sequence $x_0$, . . . , $x_T$ , and wish to predict some corresponding outputs $y_0$, . . . , $y_T$ at each time. The key constraint is that to predict the output $y_t$ for some time t, we are constrained to only use those inputs that have been previously observed:$x_0$, . . . , $x_t$. Formally, a sequence modeling network is any function f : $X_{T +1}$ → $Y_{T +1}$ that produces the mapping . $y_0$, . . . , $y_T$ = f($x_0$, . . . , $x_T$ ) (1) . if it satisfies the causal constraint that $y_t$ depends only on $x_0$, . . . , $x_t$ and not on any “future” inputs $x_{t+1}$, . . . , $x_T$. The goal of learning in the sequence modeling setting is to find a network f that minimizes some expected loss between the actual outputs and the predictions. . L($y_0$, . . . , $y_T$ , f($x_0$, . . . , $x_T$)), where the sequences and outputs are drawn according to some distribution. . What is a 1D convolution? . Before we jump into the paper we must understand what is a 1D convolution since it is used in the causal convolutional layer in TCN . 1D Convolution takes in a 3D tensor as input and outputs a 3D tensor as output. | Shape of the input tensor in TCN would have following dimension ( batch_size, input_length, input_size ) and the output tensor has shape ( batch_size, input_length, output_size ) | Each layer in TCN has same input and output length so only the third dimension would change. | . . Image courtesy . In the above figure we can notice the follwing . to compute a single output we need to look at 3 consecutive values of the input sequence, it is because we are using a kernel of size 3 here. | to maintain that input and output sequences be of the same size we have to pad the input sequence with zeros on both sides. | 1d convolution is a special case of 2d convolution | . . Image courtesy . How is 1d convolution a special case of 2d convolution? . In both time series and NLP, data is laid out in a similar manner, in the figure above we have embedded the words I like this movie very much ! into a 7 x 5 embedding matrix and then we use 1d convolution on this 2D matrix. . 1d convolution is a special case of 2d convolution where kernel size of the 1d convolution is it&#39;s height. The width of the kernel is defined by the embedding size, which is 5 here and it is fixed. So it means that we can only slide vertically and not horizontally which makes it 1D convolution. . What is causal convolution? . Causality means that an element in the output sequence can only depend on elements that precede it in the input sequence. | In order to ensure that an output tensor has the same length as the input tensor, we need to do zero padding. | If we only pad the left side of the input tensor with zeros, then causal convolution is guaranteed. | $x^{&#39;}_4$ in the figure below is generated by combining $x_2$, $x_3$, $x_4$ which ensures no leakage of information. | . . This operation generates $x^{&#39;}_5$ and $x^{&#39;}_6$ which are extraneous and should be removed before passing the output to the next layer. We have to take care of it in the implementation. . How many zeros would be required to make sure that the output would be of same length as input? (kernel_size - 1) . How it all fits together? . TCN has two basic principles: . input and output length of the sequences remain same. | there can be no leakage from the past. | . To achieve the first point TCN makes use of 1D FCN ( Fully Convolutional Network ) and to achieve the second point TCN makes use of causal convolutions. . Disadvantages of the above architecture . To model long term dependencies, we need a very deep network or very large convolution kernels, neither of which turned out to be particularly feasible in the experiments. | . Dilated Convolutions . A desirable quality of a the model is that the value of a particular entry in the output depends on all previous entries in the input. . | This is achieved when the size of the receptive field is equal to the length of the input. . | We could expand our receptive field when we stack multiple layers together. In the figure below we can see that by stacking two layers with kernel_size 3, we get a receptive field size of 5. . | . . In general, the receptive field r of a 1D convolutional network with n layers and kernel_size k is . $r = 1 + n * ( k - 1 )$ . To know how many layers are needed for full coverage, we can set the receptive field size to input_length l and solve for the number of layers n (non-integer values need to be rounded): . $ lceil frac{(l-1)}{(k-1)} rceil$ . This means that, with a fixed kernel_size, the number of layers required for complete coverage would be linear in input length. This will cause the network to become very deep and very fast, resulting in models with a large number of parameters that take longer to train. . How could we solve this issue? . One way to increase the size of the receptive field while keeping the number of layers relatively small is to introduce the concept of dilation. . Dilation in the context of convolutional layers refers to the distance between elements of the input sequence that are used to compute one entry of the output sequence. Therefore, a traditional convolutional layer can be viewed as a layer dilated by 1, because the input elements involved in calculating output value are adjacent. . The image below shows an example of how the receptive field grows when we introduce dilation. The right side image uses a dilation rate r 1 in the first layer with kernel_size 3 which is how a traditional conv layer would work although in the next layer we use r=2 which makes sure that we combine input elements that are 2 elements apart when producing output for the next layer and so on. . . To overcome the problem of number of layers required for covering the entire input length we must progressively increase the dilation rate over multiple layers. . This problem can be solved by exponentially increasing the value of d as we move up in the layer. To do this, we choose a constant dilation_base integer b that will allow us to calculate the dilation d for a particular layer based on the number of layers i under it, i.e. $d = b^i$. . The figure below shows a network with an input_length of 10, a kernel_size of 3, and a dilation_base of 2, which would result in a complete coverage of the 3 dilated convolutional layers. . . Here we can see that the all input values are used to produce the last value in the output layer. With the above mentioned setup we could have an input of length 15 while maintaining the full coverage. . How did we calculate that the receptive width is 15? . When a layer is added to the architecture the receptive field is increased by $d*(k-1)$ | So if we have n layers with kernel_size k and dilation base rate as b then receptive width is calculated as | . $w=1+(k-1) frac{b^n-1}{b-1}$ . but depending on values of b and k the architecture could have many holes in it. . What does that mean? . . Here we can see not all inputs are used to compute the last value of the output, even though w is greater than the input size. To fix this we would have to either increase the kernel size or decrease the dilation rate from 3 to 2. In general we must ensure that kernel_size is atleast equal to dilation rate to avoid such cases. . How many layers would be required for full coverage? . Given a kernel size k, a dilation base b where k ≥ b, and an input length l, in order to achieve full coverage following condition must be satisfied . $1+(k-1) frac{b^n-1}{b-1} geq l$, then . $n= lceil log_b( frac{(l-1)*(b-1)}{k-1}+1) rceil$ . Now number of layers is lograthmic in input layer length l which is what we wanted. This is a significant improvement that can be achieved without sacrificing receptive field coverage. . Now, the only thing that needs to be specified is the number of zero-padded items required for each layer. Assuming that the dilation expansion base is b, the kernel size is k, and there are i layers below the current layer, the number p of zero-padding items required by the current layer is calculated as follows: . $p=b^i*(k-1)$ . Temporal Residual Block . Now let&#39;s discuss the basic building blocks of TCN network. . . Residual links have proven to be an effective way to train deep networks, which allow the network to pass information in a cross-layer manner. | This paper constructs a residual block to replace one layer of convolution. As shown in the figure above, a residual block contains two layers of convolution and nonlinear mapping, and WeightNorm and Dropout are added to each layer to regularize the network. | . . Each hidden layer has the same length as the input layer, and is padded with zeros to ensure subsequent layers have the same length. . | For the output at time t, the causal convolution (convolution with causal constraints) uses the input at time t and the previous layer at an earlier time (see the blue line connection at the bottom of the figure above). . | Causal convolution is not a new idea, but the paper incorporates very deep networks to allow for long-term efficient histories. . | . Residual Link . . Residual blocks (originally from ResNet) have repeatedly shown to benefit very deep networks. | Since the receptive field of a TCN depends on the network depth n as well as the convolution kernel size k and dilation factor d, it becomes very important to stabilize deeper and larger TCNs. | Predictions may depend on long historical values and high-dimensional input sequences. e.g An input sequence of size $2^{12}$ may require a network of up to 12 layers. | In standard ResNet, the input is directly added to the output of the residual function, while in TCN the input and output can have different widths. To account for the difference in input-output width, an additional 1x1 convolution is used to ensure that element-wise addition receives tensors of the same shape. | . . Conclusion . The innovation in the TCN model is to sort out how to use causal and dilated convolutions to solve the sequence modelling task. | Causal and Dilated convolutions have already been proposed earlier but this paper highlights how they could be combined together for sequence modelling tasks | . Advantages . Parallelism. When given a sentence, TCN can process the sentence in parallel without the need for sequential processing like RNN. . | Flexible receptive field. The size of the receptive field of TCN is determined by the number of layers, the size of the convolution kernel, and the expansion coefficient. It can be flexibly customized according to different characteristics of different tasks. . | Stable gradient. RNN often has the problems of vanishing gradients and gradient explosion, which are mainly caused by sharing parameters in different time periods. Like traditional convolutional neural networks, TCN does not have the problem of gradient disappearance and explosion. . | Lower memory requirements. When RNN is used, it needs to save the information of each step, which will occupy a lot of memory. The convolution kernel of TCN is shared in one layer, and hence lower memory usage. . | . Disadvantages . TCN may not be so adaptable in transfer learning. This is because the amount of historical information required for model predictions may be different in different domains. Therefore, when migrating a model from a problem that requires less memory information to a problem that requires longer memory, TCN may perform poorly because its receptive field is not large enough. . | The TCN described in the paper is also a one-way structure. In tasks such as speech recognition and speech synthesis, the pure one-way structure is quite useful. However, most of the texts use a bidirectional structure. Of course, it is easy to expand the TCN into a bidirectional structure. Instead of using causal convolution, the traditional convolution structure can be used. . | TCN is a variant of convolutional neural network after all. Although the receptive field can be expanded by using dilated convolution, it is still limited. Compared with Transformer, it is still poor in capturing relevant information of any length. The application of TCN to text remains to be tested. . | . Tips for implementation . Next we would highlight things to keep in mind if you plan to implement the paper . After the convolution, the size of the output data after the convolution is greater than the size of the input data | This is caused owing to padding both sides, so we chomp off extra padded 0s from right side to get the desired data values. | . We have taken this (https://github.com/locuslab/TCN/blob/2221de3323/TCN/tcn.py) implementation of TCN and implemented in fast.ai and tsai to demonstrate how TCN could be used for sequence modelling. . How to prepare dataset? . from tsai.all import * computer_setup() . . We are going to select appliances energy dataset recently released by Monash, UEA &amp; UCR Time Series Extrinsic Regression Repository (2020) . dsid = &#39;AppliancesEnergy&#39; X, y, splits = get_regression_data(dsid, split_data=False) X.shape, y.shape, y[:10] . . check_data(X, y, splits) . . tfms = [None, [TSRegression()]] batch_tfms = TSStandardize(by_sample=True, by_var=True) dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=128) dls.one_batch() . . dls.show_batch() . . Model implementation . from fastai.torch_basics import * from fastai.tabular.core import * from torch.nn.utils import weight_norm . class Chomp1d(Module): def __init__(self, chomp_size): store_attr() def forward(self, x): return x[:, :, :-self.chomp_size].contiguous() def get_conv_block(n_inputs, n_outputs, kernel_size, stride, padding, dilation, dropout): conv = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation )) chomp = Chomp1d(padding) relu = nn.ReLU() drop = nn.Dropout(dropout) return nn.Sequential(*(conv, chomp, relu, drop )) class TemporalBlock(Module): def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.5): store_attr() self.in_conv_blk = get_conv_block(n_inputs, n_outputs, kernel_size, stride, padding, dilation, dropout ) self.out_conv_blk = get_conv_block(n_outputs, n_outputs, kernel_size, stride, padding, dilation, dropout ) self.net = nn.Sequential(*(self.in_conv_blk, self.out_conv_blk)) self.downsample_conv = nn.Conv1d(n_inputs, n_outputs, kernel_size=1) if n_inputs != n_outputs else None self.relu = nn.ReLU() self.init_weights() def init_weights(self): # 0 index represents the convolutional layer self.in_conv_blk[0].weight.data.normal_(0, 0.01) self.out_conv_blk[0].weight.data.normal_(0, 0.01) if self.downsample_conv is not None: self.downsample_conv.weight.data.normal_(0, 0.01) def forward(self, x): out = self.net(x) res = x if self.downsample_conv is None else self.downsample_conv(x) return self.relu(out + res) class TemporalConvNet(Module): def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2): layers = [] num_levels = len(num_channels) for i in range(num_levels): dilation_size = 2 ** i in_channels = num_inputs if i == 0 else num_channels[i-1] out_channels = num_channels[i] layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=(kernel_size-1) * dilation_size, dropout=dropout ) ] self.network = nn.Sequential(*layers) def forward(self, x): return self.network(x) class TCN(Module): def __init__(self, input_size, output_size, num_channels, kernel_size, dropout): self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout) self.linear = nn.Linear(num_channels[-1], output_size) self.init_weights() def init_weights(self): self.linear.weight.data.normal_(0, 0.01) def forward(self, x): y1 = self.tcn(x) return self.linear(y1[:, :, -1]) . model = TCN(input_size=24, output_size=1, num_channels=[24, 32, 64], kernel_size=2, dropout=0.2 ) learn = Learner(dls, model, metrics=[mae, rmse], cbs=ShowGraph()) learn.lr_find() . . model = TCN(input_size=24, output_size=1, num_channels=[24, 32, 64], kernel_size=2, dropout=0.2 ) learn = Learner(dls, model, metrics=[mae, rmse], cbs=ShowGraph()) learn.fit_one_cycle(100, 8e-4) . . References . An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling | Fast.AI | tsai | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/math/fastai/tsai/sequencemodelling/2021/12/22/temporal-convolutional-networks.html",
            "relUrl": "/deeplearning/math/fastai/tsai/sequencemodelling/2021/12/22/temporal-convolutional-networks.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Do we need downsampling?",
            "content": "Objective . Deep CNN architecutures are made up of many components like conv layer, activation function, pooling layers, batch-norm layer etc.All of them are designed for specific reasons and to better understand the effect of these components it is important to play with them. For example, we include layers like pooling, strided convolution etc to reduce the size of the input. . If we look at the architecture of VGG below, we see that lot of max_pooling layers are used. The idea is to increase the receptive field and decrease the number of parameters by reducing the size of the input. But then a question arises, do we really need to downsample? Luckily I came across this repository where this specific question is addressed and the author has tried to replace downsampling layers with dilated convolutions or large kernels with appropriate padding to address this issue. The purpose of this post is to implement and validate these ideas by performing the above mentioned experiments on CIFAR-10 dataset using fastai. The intention is to show how easy it is for us to experiment using fastai. . . Libraries . let&#39;s start by installing fastai2 and keras . !pip install fastai2 keras &gt; /dev/null . from fastai2.vision.all import * from keras.datasets import cifar10 . Dataset . We are going to train our network on CIFAR-10 dataset. CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. . (x_train,y_train),(x_test,y_test) = cifar10.load_data() . Datasets API . This is where fastai comes in with its flexible api to form a dataset that is easily consumable by the models. We are passing a list of pairs of (image, label) to the pipeline. No data augmentation is applied. To validate our ideas we will keep a separate holdout set. If you want to understand more about Datasets api please read official docs . items = np.array(list(zip(x_train, y_train.ravel()))) # 80-20 percent split splits = RandomSplitter(seed=42)(items) tfms = [[lambda x: x[0], PILImage.create], [lambda x: x[1], Categorize()]] dsets = Datasets(items, tfms, splits=splits) dls = dsets.dataloaders(bs=64, after_item=[ToTensor(), IntToFloatTensor()]) . dls.show_batch(figsize=(4, 4)) . . VGG ( 4 layer ) network . Let&#39;s try to train a 4 layer VGG based network with downsampling and see the performance on CIFAR-10. . class VGG_4(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG_4, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=16, out_channels=24, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=24, out_channels=32, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=48, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . learn = Learner(dls, VGG_4(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . Let&#39;s train it for 30 epochs . learn.fit_one_cycle(30, 1e-2) . . learner.summary() . . VGG with dilated convolution . Idea is to progressively increase the dilation from 1, 2, 4, 8 to increase the receptive field of the network. . class VGG4_Dilation(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG4_Dilation, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3), dilation=1), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.Conv2d(in_channels=16, out_channels=24, padding=(1, 1), kernel_size=(3, 3), dilation=2), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.Conv2d(in_channels=24, out_channels=32, padding=(1, 1), kernel_size=(3, 3), dilation=4), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(in_channels=32, out_channels=48, padding=(1, 1), kernel_size=(3, 3), dilation=8), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . In the head of the model instead of using a fully connected layer we are using AdaptiveAvgPool2d with 1x1 convolution layer, it is because researchers have observed that using AdaptiveAvgPool2d with 1x1 convolution layer decreases the total number of parameters without taking a hit on the performance. . learn = Learner(dls, VGG4_Dilation(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . learn.fit_one_cycle(30, 1e-2) . . learn.summary() . . Note: There is no change in number of trainable in params in vanilla vgg 4 layer model with dowsampling and vgg 4 layer model with dilation. . VGG with large kernels . We plan to progressively increase the size of the kernels from 3 to 9. Increasing the kernel size would enable us to increase the receptive field of the network but we have to make sure that we use adequate padding so as to not shrink our input. . class VGG4_large_filter(nn.Module): def __init__(self, c_in=3, n_out=10): super(VGG4_large_filter, self).__init__() self.n_out = n_out self.model = nn.Sequential(nn.Conv2d(in_channels=c_in, out_channels=16, padding=(1, 1), kernel_size=(3, 3)), nn.BatchNorm2d(16), nn.ReLU(inplace=True), nn.Conv2d(in_channels=16, out_channels=24, padding=(2, 2), kernel_size=(5, 5)), nn.BatchNorm2d(24), nn.ReLU(inplace=True), nn.Conv2d(in_channels=24, out_channels=32, padding=(3, 3), kernel_size=(7, 7)), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(in_channels=32, out_channels=48, padding=(4, 4), kernel_size=(9, 9)), nn.BatchNorm2d(48), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(output_size=(1, 1)), nn.Conv2d(in_channels=48, out_channels=self.n_out, kernel_size=(1, 1)) ) def forward(self, x): x = self.model(x) x = x.view(-1, self.n_out) return x . learn = Learner(dls, VGG4_large_filter(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . learn.lr_find() . . learn.fit_one_cycle(30, 5e-3) . . learn.summary() . . Even though there is an improvement in terms of loss but number of trainable parameters have jumped from 25,474 to 172, 930 . Conclusion . If we look at the performance of 4 layer VGG network with downsampling and compare it with dilated and large kernel size, we observe that their is an increase in the performance. | Using a large kernel will improve performance but at the cost of increased number of trainable parameters. | Using dilation would improve performance without increasing the number of trainable parameters | . Next steps . To make it generalizable, we would have to test this idea on other architectures e.g. (resnet18) and see if it increases performance or not. | Also it would be interesting to see what kind of features will our models learn if we use dilation. | .",
            "url": "https://numb3r33.github.io/experiments/deeplearning/fastai/cnn/2020/05/02/Do-we-need-downsampling.html",
            "relUrl": "/deeplearning/fastai/cnn/2020/05/02/Do-we-need-downsampling.html",
            "date": " • May 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Machine Learning Engineer with 6+ years of experience in machine learning, deep learning and MLOPs. Presently working in Gojek on building a Data Science Platform on top of Kubeflow to enable DS teams to take models to production faster. | Working on a real-time feature store to manage data pipelines and features required for running real-time ML models in production. | Prior to moving into building the platform worked as data scientist in user-growth team in Gopay ( financial arm of Gojek ) to build models for estimating Customer Lifetime Value to help business target and cater to user needs in a better way. We have been working on a model that has helped organisation champion organic growth and other growth efforts. This model has helped organisation bring down cost spent on promotions by 22% without taking hit on growth efforts. | Worked as data scientist in Gopay to build models for fraud and account takeover detection ( ATO ). This model predicts whether an incoming request is an ATO or not. This model has been running in real-time setting and has resulted in 33% reduction in tickets. | Before joining Gojek, worked in DeltaX and worked on Data Driven Attribution where we created data-driven models by borrowing ideas from Game Theory like Ordered Shapley Value to attribute conversions tracked by our system which served as building block for another model that takes into account converting and non-converting user journeys to calculate the importance of a campaign for the advertiser. It helped advertisers get better understanding of their audiences. | Also worked on budget distribution problem where we helped develop an algorithm that would distribute budgets among competing ad-groups running on Facebook while optimizing the business objective of the advertiser. This is a big step up from traditional approaches which mostly look at single indicator and it helped advertisers plan better. | . | Mentor at Scaler academy where mentored lots of Machine Learning enthusiasts on how to take their career forward and setup Machine Learning infrastructure, models and practices in their respective organizations. | Open Source contributor at numb3r33 | Interested in competitive machine learning Winner of Deep Learning Hackathon organized by HackerEarth (https://www.hackerearth.com/challenges/competitive/deep-learning-beginner-challenge/leaderboard/). | Ranked in Top-100 data scientists on Analytics Vidhya platform which has around 3500 competitors at the time of writing. | . | Interested in blogging about machine learning, deep learning and mathematics, you can read my posts here | . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://numb3r33.github.io/experiments/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://numb3r33.github.io/experiments/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}