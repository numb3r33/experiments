<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Title | experiments</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes on all experiments and learnings." />
<meta property="og:description" content="Notes on all experiments and learnings." />
<link rel="canonical" href="https://numb3r33.github.io/experiments/2025/04/26/indic-clip.html" />
<meta property="og:url" content="https://numb3r33.github.io/experiments/2025/04/26/indic-clip.html" />
<meta property="og:site_name" content="experiments" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-26T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Title","url":"https://numb3r33.github.io/experiments/2025/04/26/indic-clip.html","dateModified":"2025-04-26T00:00:00-05:00","datePublished":"2025-04-26T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://numb3r33.github.io/experiments/2025/04/26/indic-clip.html"},"description":"Notes on all experiments and learnings.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/experiments/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://numb3r33.github.io/experiments/feed.xml" title="experiments" /><link rel="shortcut icon" type="image/x-icon" href="/experiments/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/experiments/">experiments</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/experiments/about/">About Me</a><a class="page-link" href="/experiments/search/">Search</a><a class="page-link" href="/experiments/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Title</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2025-04-26T00:00:00-05:00" itemprop="datePublished">
        Apr 26, 2025
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/numb3r33/experiments/tree/master/_notebooks/2025-04-26-indic-clip.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/experiments/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/numb3r33/experiments/master?filepath=_notebooks%2F2025-04-26-indic-clip.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/experiments/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/numb3r33/experiments/blob/master/_notebooks/2025-04-26-indic-clip.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/experiments/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2025-04-26-indic-clip.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Indic-CLIP:-Multimodal-Understanding-for-Indic-Languages">Indic-CLIP: Multimodal Understanding for Indic Languages<a class="anchor-link" href="#Indic-CLIP:-Multimodal-Understanding-for-Indic-Languages"> </a></h1><blockquote><p>Indic languages, despite their cultural richness and widespread usage, have long been underserved in the field of multimodal AI. Inspired by the success of OpenAI’s CLIP (Contrastive Language-Image Pre-training), I aimed to adapt this powerful architecture to bridge the multimodal understanding gap specifically for Hindi, with an outlook towards Sanskrit. This project demonstrates how vision-language models can effectively associate images with relevant textual descriptions, opening pathways for applications such as cross-modal search and zero-shot image classification tailored to the Indian cultural context.</p>
</blockquote>
<ul>
<li>toc:true- badges: true</li>
<li>comments: true</li>
<li>sticky_rank: 1</li>
<li>author: Abhishek Sharma</li>
<li>image: images/indic_clip.png</li>
<li>categories: [deeplearning, multimodal, fastai]</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The core objective was straightforward yet ambitious: develop a foundational vision-language model leveraging the fast.ai framework. I utilized the Flickr8k-Hindi dataset, which contains approximately 8,000 images each paired with multiple Hindi captions, as my primary data source due to its accessibility and structure.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dataset-Acquisition-and-Preprocessing">Dataset Acquisition and Preprocessing<a class="anchor-link" href="#Dataset-Acquisition-and-Preprocessing"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I set up scripts to efficiently download and preprocess the Flickr8k-Hindi dataset from Kaggle (images: <a href="https://www.kaggle.com/datasets/adityajn105/flickr8k">adityajn105/flickr8k</a>, captions: <a href="https://www.kaggle.com/datasets/dsmeena/flickr8k-hindi-captions/data">dsmeena/flickr8k-hindi-captions</a>). The preprocessing pipeline included:</p>
<ul>
<li>Associating image filenames with their respective Hindi captions.</li>
<li>Filtering out corrupted or missing images.</li>
<li>Ensuring images met specified resolution and aspect ratio standards.</li>
<li>Removing captions that were either too short or excessively long.</li>
<li>Deduplicating image-caption pairs through perceptual hashing and exact caption matching.</li>
</ul>
<p>The processed data was stored in JSONL format (filtered_data.jsonl) to seamlessly integrate with fast.ai’s DataBlock API.</p>
<p>Challenges arose primarily in extending this pipeline to Sanskrit due to the complexity of manuscript processing and data scarcity, highlighting an area for future exploration.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-Architecture">Model Architecture<a class="anchor-link" href="#Model-Architecture"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Indic-CLIP model was constructed using fast.ai, integrating components from timm and transformers. Key architectural elements included:</p>
<p>Vision Encoder: Leveraged a pre-trained ResNet50 model (timm) initialized with ImageNet weights to extract visual features.</p>
<p>Text Encoder: Utilized the ai4bharat/indic-bert model from Hugging Face for generating contextual embeddings from Hindi captions, accompanied by its dedicated tokenizer.</p>
<p>Projection Layers: Added linear projections to map both visual and textual embeddings into a common 512-dimensional space.</p>
<p>Contrastive Learning: Implemented a learnable temperature scaling factor (logit_scale) to refine the similarity calculations during training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-Approach">Training Approach<a class="anchor-link" href="#Training-Approach"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Training was performed with the symmetric InfoNCE contrastive loss to encourage matching pairs while differentiating non-matching pairs within each batch. The process included:</p>
<p>Orchestrating training loops through fast.ai’s Learner API.</p>
<p>Employing the AdamW optimizer combined with fastai’s fit_one_cycle learning rate strategy.</p>
<p>Integrating optional techniques such as Automatic Mixed Precision (AMP) and Gradient Accumulation for efficient resource use.</p>
<p>Utilizing Weights &amp; Biases (wandb) for comprehensive tracking of metrics and training progress.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluation-Methods">Evaluation Methods<a class="anchor-link" href="#Evaluation-Methods"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Pix2Pix-Architecture">Pix2Pix Architecture<a class="anchor-link" href="#Pix2Pix-Architecture"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Pix2Pix is a conditional GAN architecture designed for image-to-image translation tasks. It consists of two main components: a U-Net generator that actually produces the maze solutions, and a PatchGAN discriminator that learns to distinguish between real and fake solutions. The generator takes an unsolved maze as input and attempts to produce its solution, while the discriminator evaluates how realistic these solutions appear compared to the ground truth.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-Procedure">Training Procedure<a class="anchor-link" href="#Training-Procedure"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Model evaluation focused on two core tasks:</p>
<p>Cross-Modal Retrieval: Measuring performance via Recall@k (R@1, R@5, R@10) and Mean Recall (MR), assessing how accurately the model retrieves the correct text given an image (I2T) and vice versa (T2I).</p>
<p>Zero-Shot Classification: Evaluating the model’s ability to classify unseen categories using generated textual prompts and measuring performance via Top-1 accuracy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results-and-Demonstration">Results and Demonstration<a class="anchor-link" href="#Results-and-Demonstration"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Indic-CLIP model successfully demonstrated effective multimodal understanding on the Flickr8k-Hindi dataset. To showcase its capabilities, I developed an interactive application using Gradio, hosted on Hugging Face Spaces. Users can now explore functionalities such as image-to-text retrieval, text-to-image retrieval, and zero-shot classification interactively.</p>
<p>Demo available here: [Link to Hugging Face Space]</p>
<p>Future Directions and Limitations</p>
<p>Significant potential exists for expanding this framework through:</p>
<ul>
<li>Incorporating larger, more diverse datasets including web-scraped content and specialized synthetic datasets like IndicTTI.</li>
<li>Improving Sanskrit data processing techniques, particularly advanced tokenization and Sandhi splitting.</li>
<li>Scaling model complexity by experimenting with more advanced and larger model backbones.</li>
<li>Extending the approach to support additional Indic languages.</li>
</ul>
<p>This project establishes a robust foundation and working pipeline for future multimodal research in Indic languages, validated through an accessible and practical demonstration.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2><ul>
<li><p>Kakwani, D., Kunchukuttan, A., Golla, S., N.C., G., Bhattacharyya, A., Khapra, M. M., &amp; Kumar, P. (2020). <em>IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages</em>. In <em>Findings of EMNLP</em>.</p>
</li>
<li><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). <em>Deep Residual Learning for Image Recognition</em>. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770-778.</p>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/experiments/2025/04/26/indic-clip.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/experiments/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/experiments/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/experiments/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes on all experiments and learnings.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/numb3r33" target="_blank" title="numb3r33"><svg class="svg-icon grey"><use xlink:href="/experiments/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/asreddevils" target="_blank" title="asreddevils"><svg class="svg-icon grey"><use xlink:href="/experiments/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
