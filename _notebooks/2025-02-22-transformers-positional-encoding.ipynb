{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJLGNCVtPFkRIFLMw3G91Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Understanding Transformer Positional Encodings - A Mathematical Deep Dive\n","\n","\n","> A rigorous mathematical exploration of transformer positional encodings, revealing how sinusoidal functions elegantly encode sequence order through linear transformations, inner product properties, and asymptotic decay behaviors that balance local and global attention.\n","\n","- toc: true\n","- badges: true\n","- comments: true\n","- sticky_rank: 1\n","- author: Abhishek Sharma\n","- image: images/pos_encoding.png\n","- categories: [llm, transformers, math, deeplearning]"],"metadata":{"id":"MVhKIVAlGFlS"}},{"cell_type":"markdown","source":["## Introduction\n","\n","When we process sequential data like text, the order of elements matters tremendously. The sentence \"dog bites man\" conveys a very different meaning than \"man bites dog,\" despite containing exactly the same words. This simple example highlights why position information is crucial for understanding sequences.\n","Transformer architectures revolutionized natural language processing with their self-attention mechanism, allowing models to process entire sequences in parallel rather than sequentially like RNNs. However, this parallelization comes with a challenge: self-attention is inherently position-agnostic. If we simply feed word embeddings into a transformer, the model has no way of knowing which word came first, second, or last.\n","This position-blindness creates a fundamental problem. How can transformers understand the sequential nature of language without sacrificing their parallelization advantage? The answer lies in positional encodings - specially designed vectors that inject position information into the model. In this blog, we'll take a deep mathematical dive into how transformer positional encodings work, particularly focusing on the elegant sinusoidal solution presented in the \"Attention Is All You Need\" paper.\n","Let's explore the mathematical elegance that allows transformers to understand sequence order while maintaining their computational advantages.\n","\n","##  The Challenge of Position in Self-Attention\n","\n","To understand why positional encodings are necessary, we must first examine the self-attention mechanism's architecture. In the most basic form, self-attention operates on a set of input vectors and computes weighted connections between them. The input tokens are converted to query (q), key (k), and value (v) vectors through linear transformations. The attention weights are then computed via dot products between queries and keys, determining how much each token should \"attend\" to other tokens.\n","Mathematically, for each token's position i, the attention mechanism computes:\n","$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n","The critical observation is that this formulation is permutation invariant. If we shuffle the order of tokens in the input sequence, the computed attention patterns would adjust accordingly but still produce mathematically equivalent results. There's nothing in the core attention mechanism that encodes or preserves information about the absolute or relative positions of tokens.\n","\n","These are indistinguishable information for self-attention because the operation of self-attention is undirected. This position-blindness is a fundamental limitation that needs to be addressed.\n","\n","## Positional Encoding Requirements\n","\n","Before diving into specific encoding strategies, let's establish what makes a good positional encoding. We can identify three critical requirements:\n","\n","- Absolute Position Representation: The encoding must uniquely identify the absolute position of each token in the sequence (e.g., first token is 1, second token is 2).\n","- Relative Position Consistency: When sequences have different lengths, the relative positions/distances between tokens must remain consistent. For example, the relative distance between positions 2 and 4 should be encoded the same way regardless of whether the sequence has 10 tokens or 100 tokens.\n","- Length Generalization: The encoding system must work for sequence lengths that the model has never seen during training. This is crucial for practical applications where input lengths can vary widely.\n","\n","These requirements create interesting constraints on the mathematical properties our positional encoding must satisfy. Let's explore different approaches and see how they measure up against these requirements.\n","\n","### Early Approaches: Integer and Bounded Range Encoding\n","\n","Integer Positional Encoding\n","\n","The most intuitive approach might be to simply use integer values to mark positions:\n","$$\\text{position}_i = i$$\n","Where i represents the position in the sequence (1st, 2nd, 3rd, etc.). This natural encoding labels the first token as 1, the second as 2, and so on.\n","However, this approach faces a significant problem. The \"model may encounter sequences longer than training, not conducive to generalization of model.\" The issue is that position values become unbounded as the sequence length increases. If a model is trained on sequences of maximum length 512 but then encounters a sequence of length 1000, positions 513-1000 would be completely out of the training distribution.\n","Additionally, as the length of sequences increases, position values grow larger and larger, potentially causing numerical instability or dominating the actual content embeddings.\n","\n","### Bounded Range Encoding\n","\n","To address the unbounded nature of integer encoding, we can normalize positions to a bounded range [0,1]:\n","$$\\text{position}_i = \\frac{i-1}{L-1}$$\n","Where L is the sequence length, mapping the first position to 0 and the last position to 1.\n","This approach ensures that regardless of sequence length, position values remain bounded between 0 and 1. For example:\n","\n","For a 3-token sequence: [0, 0.5, 1]\n","For a 4-token sequence: [0, 0.33, 0.67, 1]\n","\n","This neatly addresses the generalization problem for variable sequence lengths. However, it introduces a new issue: the relative distances between tokens now depend on sequence length. In a 3-token sequence, adjacent tokens have a positional difference of 0.5, while in a 4-token sequence, adjacent tokens have a difference of 0.33.\n","This inconsistency in relative positions violates our second requirement and can make it harder for the model to learn consistent patterns across sequences of different lengths.\n","\n","### Vector-Based Positional Encoding\n","\n","To overcome the limitations of scalar position values, we can move to vector-based representations where we use a vector with the same dimension as our token embeddings to represent position.\n","\n","Binary Vector Encoding\n","\n","One approach is to use binary vectors for positional encoding. In this method, we represent positions using binary vectors where different dimensions encode different aspects of position.\n","\n","We can see an example where d_model = 3:\n","For a token at position a₀, we might have:\n","\n","```\n","a₀ a₁ a₂ a₃ a₄ a₅ a₆ a₇\n","0  0  0  0  1  1  1  1\n","0  0  1  1  0  0  1  1\n","0  1  0  1  0  1  0  1\n","```\n","\n","This creates a unique binary signature for each position. However, this approach still has limitations in terms of generalization to unseen sequence lengths and maintaining consistent relative distances.\n","\n","## Sinusoidal Positional Encoding: The Elegant Solution\n","\n","The transformer architecture introduced an elegant solution: sinusoidal positional encodings. We need functions that are bounded and continuous, with the sine function being the simplest option.\n","The sinusoidal encoding defines each dimension of the positional encoding vector as follows:\n","$$PE_{(t,2i)} = \\sin\\left(\\frac{t}{10000^{2i/d_{model}}}\\right)$$\n","$$PE_{(t,2i+1)} = \\cos\\left(\\frac{t}{10000^{2i/d_{model}}}\\right)$$\n","Where:\n","\n","t is the position in the sequence\n","i is the dimension index (ranging from 0 to d_model/2-1)\n","d_model is the dimensionality of the model embeddings\n","\n","This creates a unique positional fingerprint for each position t, where each dimension oscillates at a different frequency. The frequencies form a geometric progression from 1 to 1/10000, providing a rich spectrum of periodic signals.\n","\n","$$PE_t = \\left[\\sin(\\omega_1 t), \\cos(\\omega_1 t), \\sin(\\omega_2 t), \\cos(\\omega_2 t), \\ldots, \\sin(\\omega_{d_{model}/2} t), \\cos(\\omega_{d_{model}/2} t)\\right]$$\n","Where:\n","$$\\omega_i = \\frac{1}{10000^{2i/(d_{model})}}$$\n","Let's explore why this formulation is particularly effective.\n","\n","## Mathematical Properties of Sinusoidal Positional Encoding\n","\n","The sinusoidal encoding possesses several mathematical properties that make it ideal for positional representation in transformers:\n","\n","### 1. Uniqueness of Position Vectors\n","\n","The vector of each token is unique (the frequency of each sin function is small enough). This ensures that each position gets a distinctive representation.\n","\n","### 2. Bounded Values\n","\n","All values in the positional encoding vector are bounded between -1 and 1, preventing numerical instability regardless of sequence length.\n","\n","### 3. Frequency Spectrum\n","\n","The use of different frequencies for different dimensions creates a rich representation. At lower values of t (positions near the beginning), high frequencies dominate, potentially creating overlap between position vectors. To avoid this, the frequencies are set to low values, achieved through the 10000 denominator term.\n","\n","Relationship b/w frequency, wavelength, and t → At lower values of t, frequency is high so there could be lots of overlap b/w position vectors. To avoid this, we try to lengthen the wavelength of function.\n","\n","### 4. Alternating Sine and Cosine\n","\n","The alternation between sine and cosine functions for consecutive dimensions serves multiple purposes:\n","\n","The vector of each token is unique\n","The value of position vector is bounded & located in a continuous space\n","The model is easier to generalize on sequence lengths that are inconsistent with the distribution of training data\n","\n","This alternating pattern also facilitates the encoding of relative positions, as we'll see next.\n","\n","## The Linear Transformation Property\n","\n","One of the most powerful properties of sinusoidal encodings is that they can represent both absolute and relative positions efficiently. Different position vectors can be obtained through linear transformation → this would help represent both absolute & relative position of tokens.\n","\n","Mathematically, we can express this as:\n","\n","$$PE_{t+\\Delta t} = [T_{\\Delta t}] \\cdot PE_t$$\n","Where $[T_{\\Delta t}]$ is a linear transformation matrix that depends only on the offset $\\Delta t$, not on the absolute position $t$.\n","\n","This linear transformation corresponds to a rotation in the 2D subspace spanned by each sine-cosine pair. It follows from a fundamental property of sinusoidal functions:\n","\n","$$\\begin{pmatrix} \\sin(t+\\Delta t) \\ \\cos(t+\\Delta t) \\end{pmatrix} = \\begin{pmatrix} \\cos \\Delta t & \\sin \\Delta t \\ -\\sin \\Delta t & \\cos \\Delta t \\end{pmatrix} \\begin{pmatrix} \\sin(t) \\ \\cos(t) \\end{pmatrix}$$\n","\n","This property means that the model can learn to \"shift\" positions through linear transformations, enabling it to understand relative positions in the sequence without explicitly computing them.\n","\n","## Inner Product and Relative Position Dependency\n","\n","Perhaps the most remarkable property of sinusoidal positional encodings is how they encode relative distances through inner products. The inner product between two position encodings depends only on the relative distance between them, not their absolute positions.\n","we see the mathematical derivation:\n","$$\\langle PE_m, PE_n \\rangle = \\text{Re}[P_m P_n^*]$$\n","Where $P_m = e^{im\\theta}$ and $P_n = e^{in\\theta}$ in the complex number representation.\n","Then:\n","$$P_m P_n^* = e^{im\\theta} \\cdot e^{-in\\theta} = e^{i(m-n)\\theta}$$\n","Taking the real part:\n","$$\\text{Re}[P_m P_n^*] = \\cos((m-n)\\theta)$$\n","\n","This elegant result shows that the inner product between two position encodings depends only on their relative offset (m-n), not their absolute positions. This is a crucial property for the self-attention mechanism, which relies heavily on inner products.\n","\n","Extending to the full d-dimensional case:\n","\n","$$\\langle PE_m, PE_n \\rangle = \\sum_{i=0}^{d_{model}-1} \\cos((m-n)\\theta_i)$$\n","\n","This means that the similarity between position vectors naturally captures their relative distance, with the similarity decreasing as the distance increases. This property elegantly addresses our requirement for consistent relative position encoding.\n","\n","## Asymptotic Analysis of Oscillatory Integrals\n","\n","Aanalysis of why the inner product between position encodings decays as the distance between positions increases. Let's explore this mathematical analysis in greater depth.\n","\n","### Why Dominant Contributions Come From Slowly Varying Phases\n","\n","For large values of |m-n|, we need to analyze integrals of the form:\n","$$I = \\int_0^1 e^{ix\\phi(t)} dt$$\n","Where x = m-n (the relative distance) and $\\phi(t)$ is the phase function.\n","For large |m-n|, integrals of form $\\int e^{ix\\phi} dt$ (where x = m-n) decay due to rapid oscillations.\n","This is a consequence of the Riemann-Lebesgue formula, which states that:\n","$$\\lim_{x \\to \\infty} \\int_a^b e^{ix\\phi(t)} dt = 0$$\n","Provided $\\phi(t)$ is smooth and not constant.\n","We specifically examine the case where:\n","$$\\phi(t) = e^{-\\lambda t}$$\n","Where $\\lambda = \\ln(10000)$ for the transformer positional encoding.\n","For large x, dominant contribution comes from regions where phase $xe^{-\\lambda t}$ (varies slowly). However, since $e^{-\\lambda t}$ decreases slowly, there is no stationary point and the integral decays as 1/x.\n","\n","\n","### Phase Analysis and Substitution\n","\n","For the integral:\n","$$I = \\int_0^1 e^{ix e^{-\\lambda t}} dt$$\n","You apply the substitution:\n","$$s = e^{-\\lambda t}$$\n","$$t = -\\frac{\\ln s}{\\lambda}$$\n","$$dt = -\\frac{1}{\\lambda s}ds$$\n","This transforms the integral to:\n","$$I = \\frac{1}{\\lambda} \\int_{e^{-\\lambda}}^1 \\frac{e^{ixs}}{s} ds$$\n","\n","### Integration by Parts\n","\n","Using $u = \\frac{1}{s}$ and $dv = e^{ixs} ds$, you apply integration by parts:\n","$$I = \\frac{1}{\\lambda} \\left[ \\frac{e^{ixs}}{ixs} \\right]{e^{-\\lambda}}^1 + \\frac{1}{ix\\lambda} \\int{e^{-\\lambda}}^1 \\frac{e^{ixs}}{s^2} ds$$\n","For large x, the boundary term dominates:\n","$$I \\approx \\frac{1}{\\lambda} \\left( \\frac{e^{ix} - e^{ixe^{-\\lambda}}}{ix} \\right) = O\\left(\\frac{1}{x}\\right)$$\n","This confirms that the integral decays as O(1/x) for large values of x, which means the correlation between positions decays as the relative distance increases.\n","The boundary term dominates for large x: $I \\approx \\frac{1}{ix\\lambda}(e^{ix} - e^{ixe^{-\\lambda}}) = O(\\frac{1}{x})$\n","\n","## Why the Frequency Parameter Choice Leads to Decaying Correlation\n","\n","The specific choice of frequency parameter $\\theta_k = 10000^{-2k/d}$ plays a critical role in how correlations decay with distance.\n","\n","### Sinusoidal Positional Encodings and Inner Product Calculation\n","\n","Starting with the definition of positional encodings:\n","$$P_{m,2i} = \\sin\\left(\\frac{m}{10000^{2i/d}}\\right)$$\n","$$P_{m,2i+1} = \\cos\\left(\\frac{m}{10000^{2i/d}}\\right)$$\n","The inner product between positions m and n is:\n","$$\\langle P_m, P_n \\rangle = \\sum_{k=0}^{d-1} \\cos((m-n)\\theta_k)$$\n","Where $\\theta_k = \\frac{1}{10000^{2k/d}}$ for the different frequency bands.\n","\n","#### Inner Product and Relative Position Dependency\n","\n","\n","$$\\langle P_m, P_n \\rangle = \\text{Re}\\left[ \\sum_{k=0}^{d-1} e^{i(m-n)\\theta_k} \\right]$$\n","Each term $e^{i(m-n)\\theta_k}$ corresponds to a cosine wave:\n","$$\\langle P_m, P_n \\rangle = \\sum_{k=0}^{d-1} \\cos((m-n)\\theta_k)$$\n","\n","### Decay of Inner Product with |m-n|\n","\n","Decay of inner product with |m-n| - Critical observation for large |m-n|: inner product decays even though individual cosine terms are periodic. This arises from the superposition of high-frequency oscillations with varying θk, leading to destructive interference.\"\n","For large d (high-dimensional embeddings), the sum can be approximated as an integral:\n","$$\\langle P_m, P_n \\rangle \\approx \\frac{d}{2} \\cdot \\text{Re}\\left[ \\int_0^1 e^{i(m-n)\\theta_t} dt \\right]$$\n","With $\\theta_t = 10000^{-t}$ for $t \\in [0,1]$.\n","This transforms the problem into analyzing the asymptotic behavior of an oscillatory integral. Setting $\\theta_t = e^{-\\lambda t}$ where $\\lambda = \\ln(10000)$:\n","$$I = \\int_0^1 e^{i(m-n)e^{-\\lambda t}} dt$$\n","\n","This integral decays as O(1/|m-n|) for large |m-n|, which proves why the correlation decreases as the relative distance increases.\n","\n","### Comparison with Power Laws\n","\n","For θ = t^-α, the integral also decays but rate depends on α:\n","\n","α = 1, I ∝ O(1/|x|)\n","α = 2, I ∝ O(1/|x|^(1/2))\n","\n","This shows that the exponential frequency spacing used in transformer positional encodings (θₖ = 10000⁻²ᵏ/ᵈ) creates a specific decay rate that balances local and global attention appropriately.\n","\n","## The Proof that φₘ - φₙ = φₘ₋ₙ Implies φₘ = mθ\n","\n","The rigorous proof of why the condition φₘ - φₙ = φₘ₋ₙ implies that φₘ must be of the form mθ for some constant θ. This is a crucial result for understanding why sinusoidal encodings work the way they do.\n","\n","### Step 1: Setting up the Equation\n","\n","The condition is:\n","$$\\phi_m - \\phi_n = \\phi_{m-n} \\quad \\forall m,n \\in \\mathbb{Z}$$\n","\n","### Step 2: Solving for Key Cases\n","\n","Case 1: If n=0\n","$$\\phi_m - \\phi_0 = \\phi_{m-0} = \\phi_m$$\n","This implies:\n","$$\\phi_0 = 0$$\n","Case 2: If n=1\n","$$\\phi_m - \\phi_1 = \\phi_{m-1}$$\n","Rearranging:\n","$$\\phi_m = \\phi_{m-1} + \\phi_1$$\n","This gives us a recurrence relation.\n","\n","### Step 3: Solving the Recurrence\n","\n","Setting $\\phi_1 = \\theta$ (a constant), we can solve the recurrence:\n","$$\\phi_2 = \\phi_1 + \\theta = 2\\theta$$\n","$$\\phi_3 = \\phi_2 + \\theta = 3\\theta$$\n","$$\\phi_4 = \\phi_3 + \\theta = 4\\theta$$\n","By induction, we get:\n","$$\\phi_m = m\\theta$$\n","\n","### Step 4: General Proof of Linearity\n","\n","Formal proof that the recurrence $\\phi_m = \\phi_{m-1} + \\theta$ has a unique solution $\\phi_m = m\\theta$.\n","This solution satisfies the original condition:\n","$$\\phi_m - \\phi_n = m\\theta - n\\theta = (m-n)\\theta = \\phi_{m-n}$$\n","\n","### Step 5: Why Non-Linear Solutions Fail\n","\n","Why non-linear functions fail to satisfy the condition:\n","If $\\phi_m$ were non-linear (for example, $\\phi_m = m^2\\theta$):\n","$$\\phi_m - \\phi_n = m^2\\theta - n^2\\theta = (m^2 - n^2)\\theta = (m+n)(m-n)\\theta$$\n","But:\n","$$\\phi_{m-n} = (m-n)^2\\theta$$\n","Since $(m+n)(m-n) \\neq (m-n)^2$ in general, non-linear functions don't satisfy the condition.\n","\n","The only solution [to] $\\phi_m - \\phi_n = \\phi_{m-n}$ is a linear function $\\phi_m = m\\theta$. This ensures positional encoding's angle differences encode relative positions.\n","\n","## Stationary Phase Approximation and Oscillatory Integrals\n","\n","Detailed examination of asymptotic analysis for oscillatory integrals. This section provides important mathematical intuition for why positional encoding works the way it does.\n","\n","### Stationary Phase Approximation\n","\n","For oscillatory integrals of the form:\n","$$\\int_a^b e^{ix\\phi(t)} dt$$\n","The stationary phase approximation states that the dominant contributions arise near points where the phase $\\phi(t)$ is stationary (i.e., where $\\phi'(t) = 0$). These regions exhibit slow oscillations leading to constructive interference.\n","\n","For oscillatory integrals of form $\\int_a^b e^{ix\\phi(t)}dt$, the dominant contributions arise near points where phase $\\phi(t)$ is stationary (i.e., where $\\phi'(t) = 0$). These regions exhibit slow oscillations leading to constructive interference.\n","\n","## Analysis for Transformer Positional Encoding\n","\n","In the case of $\\phi(t) = e^{-\\lambda t}$ with $\\lambda = \\ln(10000)$:\n","\n","### Phase Analysis:\n","  $\\phi'(t) = -\\lambda e^{-\\lambda t} < 0$ for all $t > 0$\n","### No Stationary Point:\n","  φ(t) = e^-λt with derivative: φ'(t) = -λe^-λt → No stationary point. φ'(t) ≠ 0 ∀ t ∈ [0,1]. Since e^-λt > 0\n","### Monotonic Decay:\n","  Monotonic decay: φ(t) decreases exponentially and φ'(t) never changes sign\n","\n","This absence of stationary points explains why the integral decays as O(1/|x|) rather than the faster decay rates typically seen in stationary phase approximations (which can be O(1/|x|^(1/2)) or faster).\n","\n","For large x, dominant contribution comes from region where phase xe^-λt (varies slowly). However, since e^-λt decreases slowly, there is no stationary point and the integral decays as 1/x.\n","\n","This mathematical insight explains why the transformer positional encoding creates a gradual decay in attention with distance, rather than a sharp cutoff or a perfectly uniform attention pattern.\n","\n","## Summary of Mathematical Properties\n","Bringing these mathematical analyses together:\n","\n","- The condition φₘ - φₙ = φₘ₋ₙ forces the positional encoding angles to follow a linear pattern φₘ = mθ, which is exactly what the sinusoidal encoding provides.\n","- The specific choice of frequency parameter θₖ = 10000⁻²ᵏ/ᵈ creates an inner product that decays as O(1/|m-n|) for large distances, due to the absence of stationary points in the phase function.\n","- This decay property helps the model naturally focus more on local context while still maintaining the ability to detect long-range dependencies when needed.\n","\n","\n","### Why Non-Linear Solutions Fail\n","\n","If $\\phi_m$ were non-linear (e.g., $\\phi_m = m^2\\theta$):\n","$$\\phi_m - \\phi_n = m^2\\theta - n^2\\theta = (m^2 - n^2)\\theta = (m-n)(m+n)\\theta \\neq \\phi_{m-n} = (m-n)^2\\theta$$\n","This confirms that only the linear relationship $\\phi_m = m\\theta$ correctly encodes relative positions, explaining why sinusoidal encodings with linearly scaled frequencies are used.\n","\n","The only solution $\\phi_m - \\phi_n = \\phi_{m-n}$ is a linear function $\\phi_m = m\\theta$. This ensures positional encoding's angle differences encode relative positions.\n","\n","\n","## Why This Works: A Taylor Expansion Perspective\n","\n","This analysis revolves around using Taylor expansion to explain why adding positional encodings breaks the symmetry in transformer models.\n","For a pure attention model without attention mask, the function is fully symmetric:\n","$$f(x_1, x_2, ..., x_n, ...) = f(x_n, ..., x_1, ...)$$\n","This means transformers cannot recognize position - the output would be the same regardless of token order. By adding positional encodings, we break this symmetry.\n","Using Taylor expansion:\n","$$\\tilde{f}(..., x_m, ..., x_n, ...) = f(..., x_m + p_m, ..., x_n + p_n, ...)$$\n","$$\\tilde{f} = f + p_m \\frac{\\partial f}{\\partial x_m} + p_n \\frac{\\partial f}{\\partial x_n} + \\frac{1}{2}p_m^2 \\frac{\\partial^2 f}{\\partial x_m^2} + ...$$\n","Where the terms $p_m \\frac{\\partial f}{\\partial x_m}$ contain position-dependent information. As long as encoding vector of each position is different, this breaks the symmetry and can be used to replace f.\n","\n","This Taylor expansion shows how positional information gets integrated with content information, allowing the model to distinguish between different token arrangements.\n","\n","## Practical Implications for Transformer Models\n","\n","The mathematical properties we've explored have significant practical implications for transformer models:\n","\n","Generalization to unseen sequence lengths: Since sinusoidal encodings are defined for any position value, they can naturally handle sequences longer than those seen during training.\n","Consistent relative positioning: The inner product properties ensure that relative positions are encoded consistently regardless of sequence length.\n","Natural attention decay: The asymptotic decay properties align with the intuition that distant tokens typically have weaker relationships.\n","Parameter efficiency: Unlike learned positional embeddings, sinusoidal encodings don't require additional trainable parameters.\n","Computational efficiency: The encodings can be computed on-the-fly rather than stored in a lookup table.\n","\n","\n","- Sinusoidal encodings use frequencies that decay exponentially across dimensions\n","- Inner Product Decay: Results from destructive interference in high-frequency oscillatory integrals\n","- Design choice: θₖ = 10000⁻²ᵏ/ᵈ ensures smooth frequency coverage & practical decay properties\n","\n","## Conclusion\n","\n","The sinusoidal positional encoding used in transformer models represents a beautiful intersection of mathematical elegance and practical utility. By encoding positions using sinusoidal functions at different frequencies, transformers gain the ability to understand sequence order while maintaining their parallelization advantages.\n","The key insights we've explored include:\n","\n","How sinusoidal functions provide a bounded, continuous representation of position\n","Why the inner product between position encodings naturally captures relative distances\n","How the specific frequency progression (10000⁻²ᵏ/ᵈ) creates a balanced representation\n","Why linear position encoding (φₘ = mθ) is the only solution that correctly encodes relative positions\n","How the asymptotic behavior creates a natural decay for distant token relationships\n","\n","These mathematical properties combine to create a positional encoding scheme that satisfies all our requirements: representing absolute positions, maintaining consistent relative distances, and generalizing to unseen sequence lengths.\n","Understanding these mathematical foundations not only gives us deeper insight into transformer models but also opens doors to potential improvements and adaptations for specific tasks or domains. The elegant mathematics behind sinusoidal positional encodings reveals how transformers achieve their remarkable ability to understand sequence order while maintaining their computational advantages.\n","\n","## References\n","\n","- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (Vol. 30).\n","- https://kexue.fm/archives/8231"],"metadata":{"id":"5SExIFqlGmWR"}}]}