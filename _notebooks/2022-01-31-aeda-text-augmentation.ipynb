{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2514ee",
   "metadata": {},
   "source": [
    "# AEDA ( An Easier Data Augmentation Technique for Text Classification )\n",
    "> A new data augmentation method proposed for text classification.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- sticky_rank: 1\n",
    "- author: Abhishek Sharma\n",
    "- image: images/aeda.png\n",
    "- categories: [deeplearning, math, fastai, nlp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5cb666",
   "metadata": {},
   "source": [
    "## Paper resources\n",
    "\n",
    "- [paper](https://arxiv.org/pdf/2108.13230.pdf)\n",
    "- [code](https://github.com/akkarimi/aeda_nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6027d7",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b4b86",
   "metadata": {},
   "source": [
    "- This paper proposes a new data augmentation technique for text classification task.\n",
    "- It also compares the performance of this augmentation technique with [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://arxiv.org/abs/1901.11196) and concludes that their method is simpler and produces better results.\n",
    "- In this experiment we will try to implement this data augmentation using `fastai` on a text classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367d790",
   "metadata": {},
   "source": [
    "### Why do we need augmentations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fba7e",
   "metadata": {},
   "source": [
    "- To have better generalizability, we need more and more comprehensive datasets but collection of these datasets and labelling is a laborious task so augmentation becomes an attractive method to introduce more examples for model to consume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b1df3",
   "metadata": {},
   "source": [
    "### What are the different kind of augmentations used in NLP?\n",
    "\n",
    "- For improving machine translation task, researchers have tried substituting common words with rare words thus providing more context for rare words.\n",
    "- Some researchers have tried replacing words with their synonyms for tweet classification.\n",
    "- Randomly swap two words in a sentence.\n",
    "- Randomly delete a word in the sentence and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153aa226",
   "metadata": {},
   "source": [
    "### What is the novel idea presented in the paper?\n",
    "\n",
    "`AEDA` method proposes randomly inserting some punctuation marks as an augmentation to introduce noise. The authors report improvement performance in **text classification** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66a4f5",
   "metadata": {},
   "source": [
    "### Can you share an example of how this augmentation would work?\n",
    "\n",
    "\n",
    "**Original Text**:\n",
    "\n",
    "Appropriate given recent events\n",
    "\n",
    "--- \n",
    "**Augmented Text**:\n",
    "\n",
    "Appropriate given ; recent events\n",
    "\n",
    "Appropriate ? given : recent events\n",
    "\n",
    "Appropriate given , recent events\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95cc6d",
   "metadata": {},
   "source": [
    "### How many punctuation marks are inserted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2e9f3",
   "metadata": {},
   "source": [
    "- Between 1 to `n/3` where `n` represents the length of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b7b94",
   "metadata": {},
   "source": [
    "### Why one-third of the sentence length?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7701bfcf",
   "metadata": {},
   "source": [
    "The authors mention that they want to increase the complexity of the sentence but doesn't want to add too many punctuation marks which would interfere with the semantic meaning of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fbedf",
   "metadata": {},
   "source": [
    "### At which positions should we insert these punctuation marks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580661d",
   "metadata": {},
   "source": [
    "- The authors inserted them at random positions in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f88127",
   "metadata": {},
   "source": [
    "### What are the different punctuation marks used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d8c4d",
   "metadata": {},
   "source": [
    "`. ; ? : ! ,`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90460bd9",
   "metadata": {},
   "source": [
    "### Why does AEDA work better compared to EDA?\n",
    "\n",
    "- EDA proposes synonym replacement, random replacement, random insertion and random deletion. These modifications could change the semantic meaning of the text.\n",
    "- Whereas AEDA just introduces punctuation marks which would only introduce noise and would not mess the semantic meaning or the word ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596c811",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282fe322",
   "metadata": {},
   "source": [
    "- We would be using [fastai](https://github.com/fastai) to implement this augmentation.\n",
    "- The authors have released the [code](https://github.com/akkarimi/aeda_nlp/) as well which we would using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f0761",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "- We will be using this dataset used in this challenge where the goal is to predict the subreddit of a subreddit post based on their title and their description. This is an example of `text categorization` / `text classification` task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f973c71",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm    import tqdm\n",
    "\n",
    "from fastai.text.all import *\n",
    "\n",
    "SEED = 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a2125",
   "metadata": {},
   "source": [
    "#### Define paths and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR      = Path('~/data/dl_nlp')\n",
    "RAW_DATA_PATH = BASE_DIR / 'data'\n",
    "OUTPUT_DIR    = Path('~/data/dl_nlp/outputs')\n",
    "\n",
    "PUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n",
    "PUNC_RATIO   = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35fb60",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed942570",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(RAW_DATA_PATH / 'train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d79695",
   "metadata": {},
   "source": [
    "![](images/aeda_df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449219be",
   "metadata": {},
   "source": [
    "- Text column represents `title` as well as `description`.\n",
    "- Subreddit column represents our `label`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83ca36",
   "metadata": {},
   "source": [
    "#### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2328f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.subreddit.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc0889",
   "metadata": {},
   "source": [
    "![](images/aeda_cl_dist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff7f4b",
   "metadata": {},
   "source": [
    "- We have multiple categories that our model needs to get right.\n",
    "- Most of the categories have similar percentage of data points in the dataset, with only `SubredditSimulator` category having less training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8fb84",
   "metadata": {},
   "source": [
    "#### Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce226ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter(seed=41)(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb9999",
   "metadata": {},
   "source": [
    "- Create a splitting strategy.\n",
    "- Here we plan to split our training dataframe randomly into training ( 80% ) and validation ( 20% ) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbb87c",
   "metadata": {},
   "source": [
    "#### Tokenize the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f04fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok, cnt = tokenize_df(train.iloc[splits[0]], text_cols='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66603511",
   "metadata": {},
   "source": [
    "- Fast.ai provides a method to tokenize our dataset.\n",
    "- Here we only passing our training examples as the corpus for tokenizer to create vocabulary.\n",
    "- We could pass in different types of tokenizers here but by default it works with `WordTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9f384",
   "metadata": {},
   "source": [
    "![](images/aeda_df_tok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18aaa59",
   "metadata": {},
   "source": [
    "- Here we could see that it has split our `text` string into tokens and created an additional column called `text_length` describing the length.\n",
    "- It has also added some library specific tokens like `xxbos`, `xxmaj` etc. `xxbos` represents beginning of the sentence token. For more details please refer to [fast.ai](https://docs.fast.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a537e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ccc14",
   "metadata": {},
   "source": [
    "![](images/aeda_cnt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca1869",
   "metadata": {},
   "source": [
    "- Here is a snapshot of the vocabulary constructed by the `tokenize_df` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb51ff7",
   "metadata": {},
   "source": [
    "#### Using `fast.ai` Pipeline to construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipe   = Pipeline([attrgetter('text'), Tokenizer.from_df(0), Numericalize(vocab=list(cnt.keys()))])\n",
    "\n",
    "lbl_pipe    = Pipeline([attrgetter('subreddit'), Categorize()])\n",
    "lbl_pipe.setup(train.subreddit)\n",
    "\n",
    "dsets       = Datasets(train, [text_pipe, lbl_pipe], splits=splits, dl_type=SortedDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f5780",
   "metadata": {},
   "source": [
    "- Here we use `Pipeline` provided by `fast.ai` to put together different transforms we want to run on our dataframe.\n",
    "- `text_pipe` represents the Pipeline that we would like to run on our `text` column in the dataframe.\n",
    "- `lbl_pipe` represents the Pipeline that we would like to run on our `subreddit` column in the dataframe.\n",
    "- `Numericalize` transform takes in our vocabulary and converts the tokens to ids.\n",
    "- `Categorize` transforms converts our labels to categories.\n",
    "- `Tokenizer.from_df` transform tokenizes the text stored in our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60197c38",
   "metadata": {},
   "source": [
    "#### AEDA data augmentation as `fast.ai` transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "PUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n",
    "PUNC_RATIO   = 0.3\n",
    "\n",
    "class InsertPunctuation(Transform):\n",
    "    split_idx = 0\n",
    "    def __init__(self, o2i, punc_ratio=PUNC_RATIO):\n",
    "        self.o2i        = o2i\n",
    "        self.punc_ratio = punc_ratio\n",
    "        \n",
    "    def encodes(self, words:TensorText):\n",
    "        new_line = []\n",
    "        q  = random.randint(1, int(self.punc_ratio * len(words) + 1))\n",
    "        qs = random.sample(range(0, len(words)), q)\n",
    "            \n",
    "        for j, word in enumerate(words):\n",
    "            if j in qs:\n",
    "                new_line.append(self.o2i[PUNCTUATIONS[random.randint(0, len(PUNCTUATIONS)-1)]])\n",
    "                new_line.append(int(word))\n",
    "            else:\n",
    "                new_line.append(int(word))\n",
    "        \n",
    "        return TensorText(new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49592e",
   "metadata": {},
   "source": [
    "- We have taken the implementation from the github shared by the authors and created a `fast.ai` tranform that would takes in the `PUNC_RATIO` and `o2i` as parameters and inserts punctuations at random positions in the sentence.\n",
    "- `PUNC_RATIO` by default takes a value of `0.3` which represents the `1/3rd` of the sentence length mentioned in the paper.\n",
    "- `o2i` is mapping between token to id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925988c",
   "metadata": {},
   "source": [
    "#### Construct dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ea547",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len    = 72\n",
    "dls_kwargs = {\n",
    "              'after_item'  : InsertPunctuation(dsets.o2i),\n",
    "              'before_batch': Pad_Chunk(seq_len=seq_len)\n",
    "             }\n",
    "\n",
    "dls        = dsets.dataloaders(bs=32, seq_len=seq_len, **dls_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29edca30",
   "metadata": {},
   "source": [
    "- When creating `fast.ai` dataloders we could perform operations on some of the events emitted.\n",
    "- Here we have made use of two such events, `after_item` callback is used to run our augmentation and add punctuation marks.\n",
    "- `before_batch` callback is used to make sure that we have paddded the tokens to make sure they are of same size before we collate them to form a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ec573",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62390a",
   "metadata": {},
   "source": [
    "![](images/aeda_dl_show.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27c123",
   "metadata": {},
   "source": [
    "- `dls.show_batch` gives a glimpse of the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b42bcb",
   "metadata": {},
   "source": [
    "#### Using the classic `TextCNN` model introduced by Yoon Kim, [paper](https://arxiv.org/abs/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(Module):\n",
    "    def __init__(self, n_embed, embed_dim, num_filters, filter_sizes, num_classes, dropout=0.5, pad_idx=1):\n",
    "        store_attr('n_embed,embed_dim')\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=n_embed,\n",
    "                                  embedding_dim=embed_dim,\n",
    "                                  padding_idx=1\n",
    "                                 )\n",
    "        self.convs = nn.ModuleList([\n",
    "                            nn.Conv2d(in_channels=1, \n",
    "                                      out_channels=num_filters, \n",
    "                                      kernel_size=(k, embed_dim)\n",
    "                                     ) \n",
    "                            for k in filter_sizes\n",
    "                                ])\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        self.relu     = nn.ReLU()\n",
    "        self.fc       = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "    \n",
    "    def _conv_and_pool(self, x, conv):\n",
    "        x = self.relu(conv(x)).squeeze(3)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        out = out.unsqueeze(1)\n",
    "        out = torch.cat([self._conv_and_pool(out, conv) for conv in self.convs], 1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c882cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab       = dls.train_ds.vocab\n",
    "num_classes = get_c(dls)\n",
    "model       = TextCNN(len(vocab[0]), \n",
    "                      embed_dim=300, \n",
    "                      num_filters=100, \n",
    "                      filter_sizes=[1, 2, 3],\n",
    "                      num_classes=num_classes,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2404cd5",
   "metadata": {},
   "source": [
    "#### Define learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, metrics=[accuracy, F1Score(average='weighted')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8300ec4b",
   "metadata": {},
   "source": [
    "- Using `F1 score weighted` metric for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8455f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(n_epoch=25, lr_max=3e-4, cbs=EarlyStoppingCallback(patience=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd818810",
   "metadata": {},
   "source": [
    "![](images/aeda_fit_one_cycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc16c90",
   "metadata": {},
   "source": [
    "- We are getting a `F1( weighted )` score of `0.869` without using any `pre-trained` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648b88d",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783fb8e",
   "metadata": {},
   "source": [
    "- [AEDA: An Easier Data Augmentation Technique for Text Classification](https://arxiv.org/pdf/2108.13230.pdf)\n",
    "- [AEDA code](https://github.com/akkarimi/aeda_nlp/)\n",
    "- [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://arxiv.org/abs/1901.11196)\n",
    "- [Fast.AI](https://www.fast.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
