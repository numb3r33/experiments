{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVhKIVAlGFlS"
   },
   "source": [
    "# Performer vs Softmax — From Kernel View to Fair Speed Tests\n",
    "\n",
    "> TL;DR. I reimplemented softmax attention and a Performer-style linear attention layer from first principles. My first attempt gave terrible perplexity and no speedup. After aligning the implementations (multi-head Wq/Wk/Wv, 1/√d_head scaling, causal prefix sums, unbiased random features), perplexity became close to softmax on TinyStories. Speed still didn’t beat softmax at seq_len=80—because the linear-time benefits only show up when context length N is large relative to the random-feature rank m and when kernels are well-fused. This post walks through the kernel view of softmax, the fixes that mattered, and a rigorous way to measure the crossover.\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- sticky_rank: 1\n",
    "- author: Abhishek Sharma\n",
    "- image: images/linear_attention.png\n",
    "- categories: [deeplearning, attention, llm, fastai]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IilwLGvqfai"
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Softmax attention is O(N²) in sequence length. Linear attention (e.g., Performer) replaces the exponential kernel with a low-rank feature map so the compute becomes O(N·m). In theory, that should be faster—so why did my early experiments show and no speedup?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFywpBuPR3xV"
   },
   "source": [
    "## Dataset Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBie-VZrf_hT"
   },
   "source": [
    "For a single head, classic attention uses the exponential kernel $$\\kappa(q,k) = e^{q\\cdot k}.$$ Softmax attention for a query q_i can be written as a normalized kernel smoother: $$y_i = \\frac{\\sum_j \\kappa(q_i,k_j)\\,v_j}{\\sum_j \\kappa(q_i,k_j)}.$$ If we factor the kernel as an inner product of features $$\\kappa(q,k)=\\phi(q)^\\top\\psi(k),\\quad\n",
    " y_i = \\frac{\\phi(q_i)^\\top\\Big(\\sum_j \\psi(k_j)v_j^\\top\\Big)}{\\phi(q_i)^\\top\\Big(\\sum_j \\psi(k_j)\\Big)}.$$ This moves the $$\\sum{j}$$ outside the per-token computation, enabling global sums (bidirectional) or prefix sums (causal). That algebra is what turns O(N²) into O(N·m).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbKx23-5TUNG"
   },
   "source": [
    "### Three constructive feature maps for exp(q·k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mL8MYIxTYlW"
   },
   "source": [
    "1. Random positive features (FAVOR+): draw set $$\\phi(x)=\\frac{1}{\\sqrt m}\\exp\\big(\\Omega x - \\tfrac{1}{2}\\|x\\|^2\\big),\\qquad \\Omega\\sim\\mathcal N(0,I).$$ As m↑, variance↓.\n",
    "\n",
    "2. Taylor truncation: $$e^{q\\cdot k} \\approx \\sum_{r=0}^{n} \\frac{(q\\cdot k)^r}{r!}.$$ with tensor-power features (theoretical lens; dims blow up as d^r). $$(q\\cdot k)^r = (q^{\\otimes r})\\cdot (k^{\\otimes r}).$$\n",
    "\n",
    "\n",
    "3. Limit definition: $$e^{x}=\\lim_{n\\to\\infty}\\big(1+\\tfrac{x}{n}\\big)^{n}.$$\n",
    " Finite n → polynomial kernel realized via an n-fold tensor power of an augmented vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDmH86D5Tin1"
   },
   "source": [
    "## Minimal, corrected Performer-style MHA (sketch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDgzleIesm9"
   },
   "source": [
    "Core pieces that mattered in practice:\n",
    "\n",
    "* Standard Wq/Wk/Wv + output projection; split into heads.\n",
    "\n",
    "* /√d_head scaling applied to both q and k paths.\n",
    "\n",
    "* Positive random features (FAVOR+) with unbiased Ω ~ N(0, I).\n",
    "\n",
    "* Prefix sums for causal attention; global sums for bidirectional.\n",
    "\n",
    "* Small ε in denominators.\n",
    "\n",
    "> Tip: for an orthogonal Ω variant, multiply the orthonormal rows by chi-distributed radii so the rows emulate Gaussian draws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2NDuL68TvZa"
   },
   "source": [
    "## Experimental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMMmcz0eT3KN"
   },
   "source": [
    "* Data: TinyStories subset (train 50k, 20% valid).\n",
    "* Tokenization: fastai TextBlock, seq_len=80.\n",
    "* Model: 2 layers, embed_dim=64, n_head=4, GELU FFN×4, LayerNorm, dropout≈0.\n",
    "* Training: 2 epochs, 1cycle, lr≈2e−3.\n",
    "* Performer rank: proj_dim m ∈ {64,128} (final runs used m=128)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "class RandomFeatureMap(nn.Module):\n",
    "    \"\"\"\n",
    "    FAVOR+ style positive random features for k(q,k) = exp(q·k).\n",
    "    phi(x) = exp(Ω x - ||x||^2 / 2) / sqrt(m), with Ω ~ N(0,I).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, proj_dim: int, orthogonal: bool = False, seed: int | None = None):\n",
    "        super().__init__()\n",
    "        self.proj_dim, self.dim = proj_dim, dim\n",
    "\n",
    "        g = torch.Generator()\n",
    "        if seed is not None: g.manual_seed(seed)\n",
    "\n",
    "        if not orthogonal:\n",
    "            omega = torch.randn(proj_dim, dim, generator=g)\n",
    "        else:\n",
    "            # Proper Orthogonal Random Features (blocks of dim with Gaussian radii)\n",
    "            blocks = []\n",
    "            remain = proj_dim\n",
    "            while remain > 0:\n",
    "                b = min(dim, remain)\n",
    "                Q, _ = torch.linalg.qr(torch.randn(dim, dim, generator=g))\n",
    "                # draw radii so that rows emulate N(0, I)\n",
    "                radii = torch.sqrt(torch.distributions.Chi2(dim).sample((b,)))\n",
    "                blocks.append((radii.unsqueeze(1) * Q[:b, :]))\n",
    "                remain -= b\n",
    "            omega = torch.cat(blocks, dim=0)\n",
    "\n",
    "        self.register_buffer(\"omega\", omega)\n",
    "\n",
    "    def phi(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, H, N, d)\n",
    "        returns: (B, H, N, m)\n",
    "        \"\"\"\n",
    "        proj = torch.einsum(\"bhnd,md->bhnm\", x, self.omega)              # (B,H,N,M)\n",
    "        sq   = (x.pow(2).sum(dim=-1, keepdim=True)) * 0.5                # (B,H,N,1)\n",
    "        return torch.exp(proj - sq) / math.sqrt(self.proj_dim)           # positive features\n",
    "\n",
    "\n",
    "class MultiheadPerformerAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, proj_dim: int,\n",
    "                 causal: bool = True, eps: float = 1e-6, orthogonal: bool = False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.h  = num_heads\n",
    "        self.d  = embed_dim // num_heads\n",
    "        self.m  = proj_dim\n",
    "        self.eps = eps\n",
    "        self.causal = causal\n",
    "\n",
    "        # Standard projections + output (like MHA)\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.wo = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        self.feature = RandomFeatureMap(self.d, proj_dim, orthogonal=orthogonal)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, E = x.shape\n",
    "        H, D = self.h, self.d\n",
    "\n",
    "        # [B,N,E] -> [B,H,N,D], with 1/sqrt(d_head) temperature\n",
    "        q = self.wq(x).view(B, N, H, D).transpose(1, 2) / math.sqrt(D)\n",
    "        k = self.wk(x).view(B, N, H, D).transpose(1, 2) / math.sqrt(D)\n",
    "        v = self.wv(x).view(B, N, H, D).transpose(1, 2)\n",
    "\n",
    "        phi_q = self.feature.phi(q)     # (B,H,N,M)\n",
    "        phi_k = self.feature.phi(k)     # (B,H,N,M)\n",
    "\n",
    "        if not self.causal:\n",
    "            # Global sums\n",
    "            KV = torch.einsum(\"bhnm,bhnd->bhmd\", phi_k, v)                  # (B,H,M,D)\n",
    "            z  = torch.einsum(\"bhnm,bhm->bhn\",     phi_q, phi_k.sum(2))     # (B,H,N)\n",
    "            y  = torch.einsum(\"bhnm,bhmd->bhnd\",   phi_q, KV)               # (B,H,N,D)\n",
    "            y  = y / (z.clamp_min(self.eps).unsqueeze(-1))\n",
    "        else:\n",
    "            # Causal: prefix sums over N\n",
    "            # Outer product per position: (B,H,N,M,D)\n",
    "            outer = phi_k.unsqueeze(-1) * v.unsqueeze(-2)\n",
    "            KVpref = outer.cumsum(dim=2)                                     # (B,H,N,M,D)\n",
    "            bpref  = phi_k.cumsum(dim=2)                                     # (B,H,N,M)\n",
    "            # Contract M: (B,H,N,D) and (B,H,N)\n",
    "            y  = torch.einsum(\"bhnm,bhnmd->bhnd\", phi_q, KVpref)\n",
    "            z  = torch.einsum(\"bhnm,bhnm->bhn\",   phi_q, bpref)\n",
    "            y  = y / (z.clamp_min(self.eps).unsqueeze(-1))\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, N, E)                     # [B,N,E]\n",
    "        return self.wo(y)\n",
    "\n",
    "class PerformerBlock(nn.Module):\n",
    "    def __init__(self, dim, n_head=8, proj_dim=64, causal=True, dropout=0.0, orthogonal=False):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadPerformerAttention(dim, n_head, proj_dim, causal=causal, orthogonal=orthogonal)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ffn   = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4), nn.GELU(), nn.Linear(dim*4, dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.drop(self.attn(x)))\n",
    "        x = self.norm2(x + self.drop(self.ffn(x)))\n",
    "        return x\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "hf_ds = load_dataset(\"roneneldan/TinyStories\")   # {'train': Dataset, 'validation': Dataset}\n",
    "\n",
    "TARGET_SIZE = 50_000                \n",
    "train_full  = hf_ds[\"train\"]\n",
    "\n",
    "SEED = 42\n",
    "rng = random.Random(SEED)\n",
    "indices = list(range(len(train_full)))\n",
    "rng.shuffle(indices)\n",
    "\n",
    "# Keep only the first TARGET_SIZE indices (or the whole set if it’s smaller)\n",
    "sub_idx = indices[:min(TARGET_SIZE, len(train_full))]\n",
    "train_subset = train_full.select(sub_idx)   # new Dataset with exactly TARGET_SIZE rows\n",
    "\n",
    "# Directory where the cached tokenised tensors will live\n",
    "cache_dir = Path(\"./cache_tinystories\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define a fastai TextBlock that reads from the HF column “text”\n",
    "# and automatically caches the tokenised output.\n",
    "text_block = TextBlock.from_df(\n",
    "    text_cols=\"text\",\n",
    "    is_lm=True,\n",
    "    seq_len=80,                 # keep your window size\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "# Build the DataBlock – we give it the raw HF Dataset objects directly.\n",
    "tinystories_block = DataBlock(\n",
    "    blocks=(text_block,),\n",
    "    get_x=ColReader(\"text\"),   \n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),   \n",
    ")\n",
    "\n",
    "dls = tinystories_block.dataloaders(\n",
    "    train_subset,               \n",
    "    bs=128,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "class SimpleMultiHeadAttention(Module):\n",
    "    \"\"\"\n",
    "    A lightweight multi-head attention that mimics fastai's implementation.\n",
    "    - `d_model` : hidden dimension (same as embed_dim)\n",
    "    - `n_head`  : number of attention heads\n",
    "    - `bias`    : whether to use bias in the linear projections\n",
    "    - `causal`  : if True, applies an upper-triangular mask\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 n_head: int = 8,\n",
    "                 bias: bool = True,\n",
    "                 causal: bool = False,\n",
    "                 attn_dropout: float = 0.):\n",
    "        \n",
    "        assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
    "        self.n_head   = n_head\n",
    "        self.d_head   = d_model // n_head\n",
    "        self.scale    = 1.0 / math.sqrt(self.d_head)\n",
    "\n",
    "        # Combined q,k,v projection for efficiency\n",
    "        self.W_qkv = nn.Linear(d_model, d_model * 3, bias=bias)\n",
    "\n",
    "        self.out_proj  = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout   = nn.Dropout(attn_dropout)\n",
    "        self.causal    = causal\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : (B, N, d_model)\n",
    "        returns (B, N, d_model)\n",
    "        \"\"\"\n",
    "        B, N, _ = x.shape\n",
    "        # qkv shape → (B, N, 3, n_head, d_head)\n",
    "        qkv = self.W_qkv(x).view(B, N, 3, self.n_head, self.d_head)\n",
    "        q, k, v = qkv.unbind(dim=2)                     # each -> (B, N, n_head, d_head)\n",
    "\n",
    "        # transpose for batched matmul: (B, n_head, N, d_head)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # scaled dot-product\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale   # (B, n_head, N, N)\n",
    "\n",
    "        if self.causal:\n",
    "            # Upper-triangular mask: allow only j ≤ i\n",
    "            mask = torch.triu(torch.ones(N, N, device=x.device), diagonal=1).bool()\n",
    "            attn_weights = attn_weights.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(attn_weights, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = (attn @ v)                                   # (B, n_head, N, d_head)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, N, -1)  # (B, N, d_model)\n",
    "\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class TransformerBlock(Module):\n",
    "    \"\"\"\n",
    "    Minimal fastai-style transformer block.\n",
    "    Parameters match the original fastai class so you can use it interchangeably.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 n_head: int = 8,\n",
    "                 d_ff: int = None,          # hidden size of the FFN; default = 4 * d_model\n",
    "                 bias: bool = True,\n",
    "                 dropout: float = 0.,       # dropout after attention & FFN\n",
    "                 attn_dropout: float = 0., # dropout inside attention\n",
    "                 ff_dropout: float = 0.,    # dropout inside the feed-forward\n",
    "                 causal: bool = False,\n",
    "                 act_fn: nn.Module = nn.GELU()):\n",
    "        \n",
    "        self.attn = SimpleMultiHeadAttention(d_model,\n",
    "                                            n_head=n_head,\n",
    "                                            bias=bias,\n",
    "                                            causal=causal,\n",
    "                                            attn_dropout=attn_dropout)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed-forward network (two linear layers + activation)\n",
    "        ff_dim = d_ff or 4 * d_model\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim, bias=bias),\n",
    "            act_fn,\n",
    "            nn.Dropout(ff_dropout),\n",
    "            nn.Linear(ff_dim, d_model, bias=bias),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x : (B, N, d_model)\n",
    "        Returns the same shape after applying:\n",
    "          self-attention + residual + norm\n",
    "          feed-forward + residual + norm\n",
    "        \"\"\"\n",
    "        attn_out = self.attn(x)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + ff_out\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SoftmaxLM(Module):\n",
    "    \"\"\"Standard fastai transformer with softmax attention.\"\"\"\n",
    "    def __init__(self, vocab_sz, embed_dim, n_head, n_layer):\n",
    "        self.emb  = nn.Embedding(vocab_sz, embed_dim)\n",
    "        self.pos  = nn.Parameter(torch.randn(1, 512, embed_dim))   # static positional encodings\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model=embed_dim,\n",
    "                             n_head=n_head,\n",
    "                             causal=causal) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln   = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n = x.shape\n",
    "        x = self.emb(x) + self.pos[:, :n]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class PerformerLM(Module):\n",
    "    \"\"\"Transformer that swaps the softmax block for Performer-style linear attention.\"\"\"\n",
    "    def __init__(self, vocab_sz, embed_dim, n_head, n_layer, proj_dim, causal=False):\n",
    "        self.emb  = nn.Embedding(vocab_sz, embed_dim)\n",
    "        self.pos  = nn.Parameter(torch.randn(1, 512, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            PerformerBlock(dim=embed_dim,\n",
    "                          n_head=n_head,\n",
    "                          proj_dim=proj_dim,\n",
    "                          causal=causal) for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln   = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_sz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n = x.shape\n",
    "        x = self.emb(x) + self.pos[:, :n]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class EpochTimer(Callback):\n",
    "    \"\"\"Records wall-clock time for each epoch.\"\"\"\n",
    "    def before_fit(self):\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def after_epoch(self):\n",
    "        self.epoch_times.append(time.time() - self.epoch_start)\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.epoch_start = time.time()\n",
    "\n",
    "\n",
    "def train_one(model, name, epochs=2, lr=2e-3):\n",
    "    learn = Learner(dls, model,\n",
    "                    loss_func=CrossEntropyLossFlat(),\n",
    "                    metrics=[Perplexity()],         # fastai's built-in perplexity metric\n",
    "                    cbs=EpochTimer())\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    learn.fit_one_cycle(epochs, lr)\n",
    "    final_ppl = float(learn.validate()[1])\n",
    "    return learn.cbs[-1].epoch_times, final_ppl\n",
    "\n",
    "\n",
    "softmax_model = SoftmaxLM(vocab_sz, embed_dim, n_head, n_layer)\n",
    "soft_times, soft_ppl = train_one(softmax_model, \"Softmax-Transformer\")\n",
    "\n",
    "performer_model = PerformerLM(vocab_sz, embed_dim, n_head, n_layer,\n",
    "                                proj_dim=proj_dim, causal=causal)\n",
    "perf_times, perf_ppl = train_one(performer_model, \"Performer-Transformer\")\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Model\": [\"Softmax-Transformer\", \"Performer-Transformer\"],\n",
    "    \"Final Perplexity\": [soft_ppl, perf_ppl],\n",
    "    \"Epoch times (sec)\": [soft_times, perf_times],\n",
    "    \"Avg time (sec)\": [sum(soft_times)/len(soft_times),\n",
    "                        sum(perf_times)/len(perf_times)]\n",
    "})\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_AAQ4ebCMtV"
   },
   "source": [
    "## Results (representative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb-0MGD2CPF9"
   },
   "source": [
    "| Model | Valid PPL | Epoch time (s) |\n",
    "|---|---:|---:|\n",
    "| Softmax Transformer | **~1.09** | **~127** |\n",
    "| Performer Transformer (m=128) | **~1.13** | **~128** |\n",
    "\n",
    "\n",
    "Perplexity parity achieved; no speedup at N=80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why no speedup (yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Back-of-envelope FLOPs per head (ignoring projections/FFN):\n",
    "\n",
    "Softmax:  $4𝑁^{2}𝐷$\n",
    " \n",
    "\n",
    "Performer:  𝑐⋅𝑁⋅𝑚⋅𝐷, with c≈6–10 (feature projections, exp, and cumsums). \n",
    " \n",
    "\n",
    "Break-even: $$4N^2D \\approx c\\,N\\,m\\,D\\;\\Rightarrow\\; N \\approx \\frac{c}{4}\\,m.$$\n",
    "\n",
    "\n",
    "With c≈8 and m=128, break-even  𝑁≈256\n",
    "\n",
    "At short contexts (N≪256), softmax’s O(N²) cost is still small, and modern softmax kernels (FlashAttention/Xformers) are heavily fused. A naïve einsum-based linear attention will not be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I would do next\n",
    "\n",
    "* Long-context grid: (N×m) sweep to locate the speed crossover on my GPU.\n",
    "\n",
    "* Fused kernels: compare Softmax(FlashAttention) vs Performer(fused FAVOR+) to reflect realistic deployments.\n",
    "\n",
    "* Hybrid maps: combine a small random-feature bank with a low-degree deterministic Taylor tail to reduce variance.\n",
    "\n",
    "* Layer-wise m: try larger m only in upper layers or selected heads.\n",
    "\n",
    "* Task sensitivity: evaluate on tasks where attention patterns are smoother (long-range modeling), where linear attention often shines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ltyk_fMIStAi"
   },
   "source": [
    "## References\n",
    "\n",
    "- Su Jianlin — perspective that softmax attention is linear in an infinite-dimensional space and constructive feature maps for exp kernels.\n",
    "\n",
    "- Choromanski et al., Rethinking Attention with Performers (ICLR 2021): FAVOR+ positive random features and linear-time attention.\n",
    "\n",
    "- Dao et al., FlashAttention series: fused softmax attention kernels for long contexts."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
