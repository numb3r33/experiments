{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Indic-CLIP: Multimodal Understanding for Indic Languages\n","\n","> Indic languages, despite their cultural richness and widespread usage, have long been underserved in the field of multimodal AI. Inspired by the success of OpenAI’s CLIP (Contrastive Language-Image Pre-training), I aimed to adapt this powerful architecture to bridge the multimodal understanding gap specifically for Hindi, with an outlook towards Sanskrit. This project demonstrates how vision-language models can effectively associate images with relevant textual descriptions, opening pathways for applications such as cross-modal search and zero-shot image classification tailored to the Indian cultural context.\n","\n","- toc: true\n","- badges: true\n","- comments: true\n","- sticky_rank: 1\n","- author: Abhishek Sharma\n","- image: images/indic_clip.png\n","- categories: [deeplearning, multimodal, fastai]"],"metadata":{"id":"MVhKIVAlGFlS"}},{"cell_type":"markdown","source":["The core objective was straightforward yet ambitious: develop a foundational vision-language model leveraging the fast.ai framework. I utilized the Flickr8k-Hindi dataset, which contains approximately 8,000 images each paired with multiple Hindi captions, as my primary data source due to its accessibility and structure."],"metadata":{"id":"7IilwLGvqfai"}},{"cell_type":"markdown","source":["## Dataset Acquisition and Preprocessing"],"metadata":{"id":"gFywpBuPR3xV"}},{"cell_type":"markdown","source":["I set up scripts to efficiently download and preprocess the Flickr8k-Hindi dataset from Kaggle (images: [adityajn105/flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k), captions: [dsmeena/flickr8k-hindi-captions](https://www.kaggle.com/datasets/dsmeena/flickr8k-hindi-captions/data)). The preprocessing pipeline included:\n","\n","- Associating image filenames with their respective Hindi captions.\n","- Filtering out corrupted or missing images.\n","- Ensuring images met specified resolution and aspect ratio standards.\n","- Removing captions that were either too short or excessively long.\n","- Deduplicating image-caption pairs through perceptual hashing and exact caption matching.\n","\n","The processed data was stored in JSONL format (filtered_data.jsonl) to seamlessly integrate with fast.ai’s DataBlock API.\n","\n","Challenges arose primarily in extending this pipeline to Sanskrit due to the complexity of manuscript processing and data scarcity, highlighting an area for future exploration.\n","\n"],"metadata":{"id":"vBie-VZrf_hT"}},{"cell_type":"markdown","source":["## Model Architecture"],"metadata":{"id":"vbKx23-5TUNG"}},{"cell_type":"markdown","source":["The Indic-CLIP model was constructed using fast.ai, integrating components from timm and transformers. Key architectural elements included:\n","\n","Vision Encoder: Leveraged a pre-trained ResNet50 model (timm) initialized with ImageNet weights to extract visual features.\n","\n","Text Encoder: Utilized the ai4bharat/indic-bert model from Hugging Face for generating contextual embeddings from Hindi captions, accompanied by its dedicated tokenizer.\n","\n","Projection Layers: Added linear projections to map both visual and textual embeddings into a common 512-dimensional space.\n","\n","Contrastive Learning: Implemented a learnable temperature scaling factor (logit_scale) to refine the similarity calculations during training."],"metadata":{"id":"-mL8MYIxTYlW"}},{"cell_type":"markdown","source":["## Training Approach"],"metadata":{"id":"4sPz1GDQTYYi"}},{"cell_type":"markdown","source":["Training was performed with the symmetric InfoNCE contrastive loss to encourage matching pairs while differentiating non-matching pairs within each batch. The process included:\n","\n","Orchestrating training loops through fast.ai’s Learner API.\n","\n","Employing the AdamW optimizer combined with fastai’s fit_one_cycle learning rate strategy.\n","\n","Integrating optional techniques such as Automatic Mixed Precision (AMP) and Gradient Accumulation for efficient resource use.\n","\n","Utilizing Weights & Biases (wandb) for comprehensive tracking of metrics and training progress."],"metadata":{"id":"3TurxspBd2Mo"}},{"cell_type":"markdown","source":["## Evaluation Methods"],"metadata":{"id":"SDmH86D5Tin1"}},{"cell_type":"markdown","source":["Model evaluation focused on two core tasks:\n","\n","* Cross-Modal Retrieval: Measuring performance via Recall@k (R@1, R@5, R@10) and Mean Recall (MR), assessing how accurately the model retrieves the correct text given an image (I2T) and vice versa (T2I).\n","\n","* Zero-Shot Classification: Evaluating the model’s ability to classify unseen categories using generated textual prompts and measuring performance via Top-1 accuracy."],"metadata":{"id":"EHDgzleIesm9"}},{"cell_type":"markdown","source":["## Results and Demonstration"],"metadata":{"id":"C2NDuL68TvZa"}},{"cell_type":"markdown","source":["The Indic-CLIP model successfully demonstrated effective multimodal understanding on the Flickr8k-Hindi dataset. To showcase its capabilities, I developed an interactive application using Gradio, hosted on Hugging Face Spaces. Users can now explore functionalities such as image-to-text retrieval, text-to-image retrieval, and zero-shot classification interactively.\n","\n","[Demo](https://huggingface.co/spaces/numb3r33/indic-clip) available here:\n","\n"],"metadata":{"id":"aMMmcz0eT3KN"}},{"cell_type":"markdown","source":["## Future Directions and Limitations\n","\n"],"metadata":{"id":"Z_AAQ4ebCMtV"}},{"cell_type":"markdown","source":["Significant potential exists for expanding this framework through:\n","\n","- Incorporating larger, more diverse datasets including web-scraped content and specialized synthetic datasets like IndicTTI.\n","- Improving Sanskrit data processing techniques, particularly advanced tokenization and Sandhi splitting.\n","- Scaling model complexity by experimenting with more advanced and larger model backbones.\n","- Extending the approach to support additional Indic languages.\n","\n","This project establishes a robust foundation and working pipeline for future multimodal research in Indic languages, validated through an accessible and practical demonstration."],"metadata":{"id":"Lb-0MGD2CPF9"}},{"cell_type":"markdown","source":["## References\n","\n","- Kakwani, D., Kunchukuttan, A., Golla, S., N.C., G., Bhattacharyya, A., Khapra, M. M., & Kumar, P. (2020). *IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages*. In *Findings of EMNLP*.\n","\n","- He, K., Zhang, X., Ren, S., & Sun, J. (2016). *Deep Residual Learning for Image Recognition*. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 770-778."],"metadata":{"id":"Ltyk_fMIStAi"}}]}